[
["validation.html", "Chapter 6 Validation 6.1 Exploration of General Ledger data 6.2 Examination of potential errors 6.3 Exploration of Trial Balance data 6.4 Assertion: Completeness", " Chapter 6 Validation The first thing you should do when you get a new piece of data, before you do any analysis, is to explore the data for potential data errors and to perform completeness testing. Ideally, completeness on a data set should be performed by comparing to another dataset - for example, a detailed ledger can roll up into account balances. Other ways of performing completeness is to compare data against a third party, independently managed source of information. For this section we will use the accounting database validate our Journal Entry file prior to testing: library(dplyr) # For manipulating data library(tidyr) # For making data long and wide library(DBI) # For database connections dir.create(&quot;data&quot;, showWarnings = FALSE) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) # Creates a table reference and collects the table, saving it as a gl object gl &lt;- tbl(con, &#39;gl&#39;) %&gt;% collect() 6.1 Exploration of General Ledger data General Ledger (GL) data is where all the detailed transactions against the accounting system take place. Sub-ledgers may exist for specific systems (Accounts Payable, Accounts Receivable, Inventory), and these systems will also post to the General Ledger, occasionally in a more summarized form. A rapid preview of all your columns can be done quickly with summary(). Not only does it list all the fields available in the table, but it also gives quick statistics on numeric columns as well (whether it makes sense is up to you): summary(gl) ## je_num amount gl_date gl_date_char ## Min. : 1.0 Min. :-92284 Min. :17897 Length:2000 ## 1st Qu.: 250.8 1st Qu.: -7160 1st Qu.:17998 Class :character ## Median : 500.5 Median : 0 Median :18080 Mode :character ## Mean : 500.5 Mean : 0 Mean :18084 ## 3rd Qu.: 750.2 3rd Qu.: 7160 3rd Qu.:18173 ## Max. :1000.0 Max. : 92284 Max. :18261 ## ## vendor_id account description ## Min. : 387 Length:2000 Length:2000 ## 1st Qu.:2531 Class :character Class :character ## Median :2695 Mode :character Mode :character ## Mean :2676 ## 3rd Qu.:2894 ## Max. :2894 ## NA&#39;s :1000 Some initial scanning is already useful: je_num is a number value, although it has no meaning as a number, as it is a reference. vendor_id is a number as well, but it contains NA’s. NA’s are R’s way of indicating that data does not exist. amount is both positive and negative - indicating this column has indicators of both credits and debits gl_date is numeric while gl_date_char is a character. It may be useful to look at a few data samples more closely to understand patterns. head() is useful in seeing the first few columns head(gl) ## # A tibble: 6 x 7 ## je_num amount gl_date gl_date_char vendor_id account description ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 30487. 17984 2019-03-29 2894 exp_material… Quality control t… ## 2 1 -30487. 17984 2019-03-29 NA liab_account… Quality control t… ## 3 2 5019. 18194 2019-10-25 2695 exp_material… Packaging and box… ## 4 2 -5019. 18194 2019-10-25 NA liab_account… Packaging and box… ## 5 3 264. 18051 2019-06-04 2894 exp_material… Quality control t… ## 6 3 -264. 18051 2019-06-04 NA liab_account… Quality control t… We get more understanding understanding the details of our dataset: je_num indicates a set of lines within a journal entry. vendor_id is usually associated to an expense account. amount is both positive and negative for the same journal entry - this means theoretically, it should balance to zero. gl_date is numeric while gl_date_char appears as a date (but is still a character). 6.2 Examination of potential errors 6.2.1 NA values NAs are “Not available” or Missing Values. As an auditor, its important for to understand why NA values exist in your dataset. Reasons I have heard in my career include: Data was not recorded - A field may be blank because it was intentionally or accidentally omitted. A “void date” is quite commonly NA in a GL database as most entries have not been voided. Or perhaps a journal entry is NA because it has not yet been approved. There may be business rules that indicate why a row’s value may be NA. Data was not recorded… at the time - A data source is always evolving, and new columns may be introduced as new features are rolled out or data structure changes. For example, a relatively new requirement indicating companies must identify government companies within their databases, and only applicable for new companies in the database. Historically entered vendors may have NAs. Inappropriate coercion - the column type was converted from one to another, indicating loss of values. For example, converting the letter ‘a’ using as.numeric() will give the following: as.numeric(&#39;a&#39;) ## Warning: NAs introduced by coercion ## [1] NA Its not that this value never existed, it is that it did not have a valid value when converted from a letter to a number, and therefore shows as NA. In our data set, the vendor_id has NA values. We can inspect these by isolating them to determine the nature of the pattern: # Base R equivalent: # gl[is.na(gl$vendor_id), ] # Tidyverse / dplyr gl %&gt;% filter(is.na(vendor_id)) ## # A tibble: 1,000 x 7 ## je_num amount gl_date gl_date_char vendor_id account description ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 -30487. 17984 2019-03-29 NA liab_account… Quality control … ## 2 2 -5019. 18194 2019-10-25 NA liab_account… Packaging and bo… ## 3 3 -264. 18051 2019-06-04 NA liab_account… Quality control … ## 4 4 -567. 17903 2019-01-07 NA liab_account… Medical grade fi… ## 5 5 -13623. 18261 2019-12-31 NA liab_account… Quality control … ## 6 6 -10702. 17959 2019-03-04 NA liab_account… Sewing machines ## 7 7 -16680. 18248 2019-12-18 NA liab_account… Quality control … ## 8 8 -14623 18166 2019-09-27 NA liab_account… Medical grade fi… ## 9 9 -2654. 17926 2019-01-30 NA liab_account… Face mask elasti… ## 10 10 -17685. 18039 2019-05-23 NA liab_account… Medical grade fi… ## # … with 990 more rows This indicates that several values are NA. This enables us to ask the proper questions - specifically, we should seek to understand and corroborate with the business if there is a certain pattern associated to these NAs. dplyr’s group_by() and summarize() are useful for identifying these patterns further: gl %&gt;% filter(is.na(vendor_id)) %&gt;% group_by(account) %&gt;% summarize(n = n(), .groups = &#39;drop&#39;) # Needed to suppress the ungrouping object message ## # A tibble: 2 x 2 ## account n ## &lt;chr&gt; &lt;int&gt; ## 1 liab_accountspayable_2000 990 ## 2 liab_creditcardpayable_2100 10 6.2.2 Journal Entries balance to zero A quick sanity check for the analysis of GL accounts is to do a quick summarization. In this case, you will want to group_by() and summarize() again - in this case, by the je_num will test whether all journal entries will net to zero. gl %&gt;% group_by(je_num) %&gt;% summarize(amount = sum(amount), .groups = &#39;drop&#39;) %&gt;% filter(amount != 0) ## # A tibble: 0 x 2 ## # … with 2 variables: je_num &lt;int&gt;, amount &lt;dbl&gt; 6.3 Exploration of Trial Balance data The Trial Balance (TB) is intended to track and record higher level movements of the General Ledger. It does so by maintaining an accurate balance of debits and credits made to the General Ledger, usually against accounts. Lets look at our trial balance: tb &lt;- tbl(con, &#39;tb&#39;) %&gt;% collect() head(tb) ## # A tibble: 5 x 13 ## account activity_2019_01 activity_2019_02 activity_2019_03 activity_2019_04 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_co… 37907. 18618. 0 58208. ## 2 exp_ma… 970382. 503354. 699039. 989992. ## 3 exp_me… 0 126. 285. 153. ## 4 liab_a… -1008289. -514495. -699323. -1023042. ## 5 liab_c… 0 -7603. 0 -25311. ## # … with 8 more variables: activity_2019_05 &lt;dbl&gt;, activity_2019_06 &lt;dbl&gt;, ## # activity_2019_07 &lt;dbl&gt;, activity_2019_08 &lt;dbl&gt;, activity_2019_09 &lt;dbl&gt;, ## # activity_2019_10 &lt;dbl&gt;, activity_2019_11 &lt;dbl&gt;, activity_2019_12 &lt;dbl&gt; Our TB has activity levels by month. In your day-to-day work, you may also receive a TB that has an Opening and Closing balance - to obtain the activity level for the audit period, simply deduct the Close from the Open to calculate the Change for the year. In our case, we would like to perform completeness testing by account for the entire year, which means we compare the total activity of the account in the TB to the GL. As each column is its own month, we can approach this in several ways. Watching the problem solved in multiple ways is always beneficial. 6.3.1 Sum by absolute references In traditional “Excel-esque” form, you would summing up each column for each row - simply taking the values of each column and adding them together. tb %&gt;% mutate(tb_activity = activity_2019_01 + activity_2019_02 + activity_2019_03 + activity_2019_04 + activity_2019_05 + activity_2019_06 + activity_2019_07 + activity_2019_08 + activity_2019_09 + activity_2019_10 + activity_2019_11 + activity_2019_12) %&gt;% select(account, tb_activity) ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. ## 2 exp_materials_6000 9798845. ## 3 exp_meals_7000 1531. ## 4 liab_accountspayable_2000 -10186325. ## 5 liab_creditcardpayable_2100 -61802. While the above works, it is difficult to read, is prone to errors, and also means this approach could only be used once this year, as we expect the year numbers to change as time passes. 6.3.2 Sum by numeric position To make this code more reusable, we could make some changes. Lets take the logical assumption that the position of the columns will never change, and therefore we can reference the position number in our script: names(tb) # This tells us the position number of each column header ## [1] &quot;account&quot; &quot;activity_2019_01&quot; &quot;activity_2019_02&quot; &quot;activity_2019_03&quot; ## [5] &quot;activity_2019_04&quot; &quot;activity_2019_05&quot; &quot;activity_2019_06&quot; &quot;activity_2019_07&quot; ## [9] &quot;activity_2019_08&quot; &quot;activity_2019_09&quot; &quot;activity_2019_10&quot; &quot;activity_2019_11&quot; ## [13] &quot;activity_2019_12&quot; tb_activity &lt;- tb %&gt;% select(2:13) %&gt;% # Selects just the numeric columns that we assume, by position number rowSums(na.rm = TRUE) # And sums up the row print(tb_activity) # This is saved as a numeric vector ## [1] 447751.60 9798845.29 1530.63 -10186325.19 -61802.33 data.frame(account = tb$account, tb_activity = tb_activity) # We create a new dataframe - one from the character vector in the original trial balance file, the other from the created tb_activity ## account tb_activity ## 1 exp_consulting_6500 447751.60 ## 2 exp_materials_6000 9798845.29 ## 3 exp_meals_7000 1530.63 ## 4 liab_accountspayable_2000 -10186325.19 ## 5 liab_creditcardpayable_2100 -61802.33 This technique allows us to use numeric positions to summarize each row, and then create a new data frame with the account name and trial balance activity we calculated. 6.3.3 Sum by named references In addition of referencing by position number, we can also reference by column name. We want to sum up all columns that start with “activity_”. The selecting by position and selecting by variable name are similar, so we’ll also introduce the ‘dot’ in this select statement. tb %&gt;% mutate(tb_activity = rowSums( select(., contains(&#39;activity_&#39;)) )) %&gt;% select(account, tb_activity) ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. ## 2 exp_materials_6000 9798845. ## 3 exp_meals_7000 1531. ## 4 liab_accountspayable_2000 -10186325. ## 5 liab_creditcardpayable_2100 -61802. What occurs here is that the dot will take the preceding command (technically known as the ‘left hand side’ or LHS) and feed it directly into the function. So in this case, the command can be narrated as: Using the TB table, create a column named “tb_activity”, calculated as… Identify all column names starting with the word “activity_” from the TB table (aka dot). Using these columns, add them together with rowSums. By introducing and referencing our columns by names, we’ve introduced a more specific and robust way to aggregate our information by account. 6.3.4 Pivot then summarize While the prior methods focused on summing up multiple columns, you could also approach this problem as if it was a wide data set that needed to become long. The ability to pivot data longer and wider is incredibly useful - not only for cleaning, but also for reshaping data into other formats for plotting and preparing for databases. If we look at our original TB data again, we notice there is: one unique identifier (the account name), multiple values for each month (example, activity_2019_01 represents the period with a value January 2019), and the dollar value itself for each month. When we deconstruct our data, it becomes much easier to delve into the tidyr package and the functions pivot_longer() and pivot_wider(). head(tb) # Notice how this data looks wide ## # A tibble: 5 x 13 ## account activity_2019_01 activity_2019_02 activity_2019_03 activity_2019_04 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_co… 37907. 18618. 0 58208. ## 2 exp_ma… 970382. 503354. 699039. 989992. ## 3 exp_me… 0 126. 285. 153. ## 4 liab_a… -1008289. -514495. -699323. -1023042. ## 5 liab_c… 0 -7603. 0 -25311. ## # … with 8 more variables: activity_2019_05 &lt;dbl&gt;, activity_2019_06 &lt;dbl&gt;, ## # activity_2019_07 &lt;dbl&gt;, activity_2019_08 &lt;dbl&gt;, activity_2019_09 &lt;dbl&gt;, ## # activity_2019_10 &lt;dbl&gt;, activity_2019_11 &lt;dbl&gt;, activity_2019_12 &lt;dbl&gt; tb_long &lt;- tb %&gt;% pivot_longer(cols = starts_with(&quot;activity_&quot;), # We want to aggregate the values in these columns names_to = &quot;period&quot;, # What we want to call this new column values_to = &quot;activity&quot;) # And the values we want to take from it tb_long ## # A tibble: 60 x 3 ## account period activity ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 activity_2019_01 37907. ## 2 exp_consulting_6500 activity_2019_02 18618. ## 3 exp_consulting_6500 activity_2019_03 0 ## 4 exp_consulting_6500 activity_2019_04 58208. ## 5 exp_consulting_6500 activity_2019_05 58177. ## 6 exp_consulting_6500 activity_2019_06 19160. ## 7 exp_consulting_6500 activity_2019_07 9919. ## 8 exp_consulting_6500 activity_2019_08 28261 ## 9 exp_consulting_6500 activity_2019_09 28871. ## 10 exp_consulting_6500 activity_2019_10 64969. ## # … with 50 more rows This data is now represented longer - there is now one unique value (activity) for each account and month. From here, we can now summarize(): tb_long %&gt;% group_by(account) %&gt;% summarize(tb_activity = sum(activity)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. ## 2 exp_materials_6000 9798845. ## 3 exp_meals_7000 1531. ## 4 liab_accountspayable_2000 -10186325. ## 5 liab_creditcardpayable_2100 -61802. While any of these approaches will work to calculate the trial balance of activity, there are sustainability advantages where the columns are not verbatim and explicitly mentioned - what matters is how you believe your dataset can change over time, or how easily readable you can communicate your work to a new individual. 6.4 Assertion: Completeness The practical goal before we do any further testing is to ensure we’re not wasting time with a data set that is missing information. This completeness test will help validate that we received data for twelve months of GL activity. In Journal Entry testing, this means we will compare the summarized Trial Balance file against the General Ledger entries to obtain reasonableness that our data set we received is complete. To compare both the GL and TB, we will want to aggregate the data in both datasets before joining them together. First, we will aggregate the GL: gl_summarized &lt;- gl %&gt;% group_by(account) %&gt;% summarize(gl_total = sum(amount), .groups = &#39;drop&#39;) # Needed to suppress the ungrouping object message gl_summarized ## # A tibble: 5 x 2 ## account gl_total ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. ## 2 exp_materials_6000 9798845. ## 3 exp_meals_7000 1531. ## 4 liab_accountspayable_2000 -10186325. ## 5 liab_creditcardpayable_2100 -61802. And also aggregate the TB: tb_summarized &lt;- tb %&gt;% mutate(tb_activity = rowSums( select(., contains(&#39;activity_&#39;)) )) %&gt;% select(account, tb_activity) tb_summarized ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. ## 2 exp_materials_6000 9798845. ## 3 exp_meals_7000 1531. ## 4 liab_accountspayable_2000 -10186325. ## 5 liab_creditcardpayable_2100 -61802. To perform a proper test of completeness, we should join both tables together. dplyr and the *_join() family of functions can be used to join tables, and also used as diagnosis tools to help debug information as well. Our GL and TB summarized datasets could be joined by the account column, prior to performing the calculation to identify differences: gl_summarized %&gt;% full_join(tb_summarized, by = &#39;account&#39;) ## # A tibble: 5 x 3 ## account gl_total tb_activity ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. 447752. ## 2 exp_materials_6000 9798845. 9798845. ## 3 exp_meals_7000 1531. 1531. ## 4 liab_accountspayable_2000 -10186325. -10186325. ## 5 liab_creditcardpayable_2100 -61802. -61802. Once you have joined the columns together, a simple difference calculation will let you know what the differences are (if any): gl_summarized %&gt;% full_join(tb_summarized, by = &#39;account&#39;) %&gt;% mutate(tb_diff = gl_total - tb_activity) ## # A tibble: 5 x 4 ## account gl_total tb_activity tb_diff ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. 447752. 0 ## 2 exp_materials_6000 9798845. 9798845. 0 ## 3 exp_meals_7000 1531. 1531. 0 ## 4 liab_accountspayable_2000 -10186325. -10186325. 0 ## 5 liab_creditcardpayable_2100 -61802. -61802. 0 And if you like clean working papers, simply filter to identify where the reconciliation did not work out: gl_summarized %&gt;% full_join(tb_summarized, by = &#39;account&#39;) %&gt;% mutate(tb_diff = gl_total - tb_activity) %&gt;% dplyr::filter(tb_diff != 0) ## # A tibble: 0 x 4 ## # … with 4 variables: account &lt;chr&gt;, gl_total &lt;dbl&gt;, tb_activity &lt;dbl&gt;, ## # tb_diff &lt;dbl&gt; 6.4.1 Cautionary notes for Completeness While performing completeness, you are proving you have a complete set of data. Exercise caution and ensure you are aware of the following: A completeness check is only as good as the data provided. In the above case, if November was completely excluded in both GL and TB, you would not be able to detect it at this stage if you did not look through the TB data to see that all twelve months were included. Using *_join() functions will generally return all combinations of matches. This is a strong motivation to ensure you have summarized rows by the join columns, prior to joining. Once you have summarized your data, the *_join() functions are far more predictable. full_join() will indicate if there are any accounts missing from either table. In our example, if the tb_summarized data had missing accounts that did exist in the gl_summarized table, the resulting tb_activity column values would show up as NA. anti_join() will show what columns are included on the left, but missing on the right. In our example, if the tb_summarized data had missing accounts that existed in the gl_summarized table, only this account would show up in the results. You may want to consider testing for invalid values or missing dates up front. While the primary goal is to ensure the datasets received are appropriate, you may want to consider validating this information earlier. The next chapter will show you how to further manipulate and test these columns. "]
]
