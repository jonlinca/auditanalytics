[
["index.html", "Audit Analytics with R Welcome", " Audit Analytics with R Jonathan Lin 2020-07-29 Welcome Draft Version - Please post any issues found This is the website for Audit Analytics in R. This audience of this book is for: Audit leaders who are looking to design their environment to encourage code sharing and data products. Audit data analytics practitioners, who are looking to leverage R in their data analytics tasks. You will learn what tools and technologies are well suited for a modern audit analytics toolkit, as well as learn skills with R to perform data analytics tasks. Consider this book to be your roadmap of practical items to implement and follow. If you are brand new to R, it is encouraged you to read https://rstudio-education.github.io/hopr/ and https://r4ds.had.co.nz. While some foundations will be covered in this book, this book is focused on an applied view of R to the financial auditor practice. "],
["about-the-author.html", "About the author", " About the author "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction Within the accounting and audit profession, analytics has been around for several decades, under the concept of Computer Aided Auditing Techniques (CAATs). The software of choice was led by ACL and its ACL Analytics software. ACL Analytics was a significant audit enabler at the time, as it allowed direct access to analyze mainframe data and flat files that were otherwise inaccessible by mainstream software on the market. It enabled audit teams to obtain transparency in analysis, a rigorous audit trail, and even automation of scripts. As computers, data analytic technology and accessibility of coding in the Accounting practice has become mainstream, there are far more tools that enable auditors to become far more powerful and self sufficient than ever before. Tools that are typically reserved for software engineers and statisticians have empowered financial auditors to expand their breadth and scope. A traditional internal audit team would consider themselves to be consumers of information, limited by flat files sent by emails from their stakeholders. While most internal auditors have implemented data analytics in one way or another, the realm of possibilities and challenges have outpaced audit shop capabilities. The expectation now is for auditors to be fully integrated into the business, and contribute directly to the management of the financial and IT risks the company faces on a regular basis. The most effective way to meet this new standard is to implement a current data analytics architecture and empower your team to leverage modern data analytics techniques. "],
["approach.html", "Chapter 2 Approach 2.1 Code-based development 2.2 Automate relentlessly 2.3 Share everything 2.4 Always deliver useful product 2.5 Don’t be a hero 2.6 “The opportunity cost”", " Chapter 2 Approach For many audit teams, having data analytics employed in your audit team is generally a given. The advantages are well articulated - 100% population coverage, timely response, and targeted high risk identification and results are true and real. Unfortunately, any short-burst of investment in data analytics training tends to not last more than a few months. This is due to lack of applied skills, sustained management interest, and consistency of values. While regulations for adherence to SEC and Sox requirements exist and are necessary, the means to approaching and achieving compliance has been largely static in nature since the introduction of Sox. The root cause is that audit teams culturally view analytics as a skill, and not as a cultural shift. On a micro level, while not every individual on your audit team needs to be a ‘coder’, everyone needs to share the same values and philosophy for any audit analytics program to have success. Your core analytics team will set the foundation for the rest of the team to play on, but if they don’t show up, the analytics program is limited at best. Having a combination of the right values and technologies will enable your team to propel forward, and should amplify your teams efforts. By thoughtfully choosing your tools and encouraging sustainable processes, your team will create a positive cycle of development, learning and deployment. 2.1 Code-based development Consider the creation and auditing of a spreadsheet, where every row and column has the potential to be manipulated. Following the motto, ‘trust but verify’, an auditor would need to examine each cell value, the formulas, the relationships, and keep a sharp eye out for manual adjustments. The auditor also needs to validate the source of data in the spreadsheet. With no built-in tracability to how the spreadsheet is used after it has been created, it contributes to the madness that is the spreadsheet ecosystem. While it is easier to superficially consume information from a spreadsheet, to understand the inner workings is a tedious task into itself. And unfortunately, both controls and audits are conducted without a second thought within spreadsheets, as it is easier to tackle the task without fundamentally changing the approach itself. Contrast this to a code-based environment. To achieve anything in code, you need to be explicit and specific on the mechanisms taken to reach the end state. Each line will tell the program what inputs it needs, how it is processed, and the collection of lines tells you what is achieved. The beauty of this is that it also tells the reader exactly what was executed to achieve the end result. A code based environment is inherently self documenting. Once a baseline code has been established, finding ongoing changes becomes trivial. Similar to black-line functionality in Microsoft Word or Track Changes in Google Docs, its far easier to detect how code (and the process it supports) has changed. Perhaps with some merit, code can be difficult to read at times, as it is still a completely different language. Notebooks address this problem. Notebooks are interactive renderings of code, containing not only the code, but can also include sections of commentary as to why something was done, and can include data visualizations and interactivity. It is the ultimate form of auditability. 2.2 Automate relentlessly By writing your routines in code, then the next logical step is to automate. Automation not only frees up your time from performing a task, but it also frees up your mental critical thinking and creative processing power. Every professional has a cognitive load - don’t waste it on routine tasks. A common use case for automation in audit is the creation of a data mart that is relevant to auditors. A data mart is a collection of pre-processed data that is suited for the group using it - for example, a data mart may hold a subset of accounts payable information. Imagine that instead of needing to email HR for the latest employee list, the audit team merely needs to check within its internal database, makes research a snap Instead of asking the vendor management team on how much money was spent per vendor, having this information in an audit mart already available means you can spend more time thinking about how to assess these vendors for risk. Gone are the days where you only audit a topic once a year. A byproduct of every audit is a monitoring mechanism - how do you know something has gone off the rails? Traditional remediation paths include a follow-up in the future - after that, there are few mechanisms to faithfully maintain confidence that the process is still operating reasonably. If you’ve coded your audit, then you have already done the hard work of structuring how to extract your data data, finding exceptions and distributing actionable results for your stakeholders. Automation takes that one step further, and allows you to ensure the steps are done repeatedly. The value you derived initially from your audit can now be done continuously. Once you’ve audited the identification of issues and the metrics around them, then you can focus on the automation the handling and resolution of them. With a true audit results database, you can the results of automated monitoring into it, enabling on-demand availability, visibility, transparency, and even reporting, all which can now be part of that automation process. 2.3 Share everything Tribal knowledge is information or knowledge that is known within a tribe but often unknown outside of it. A tribe, in this sense, may be a group or subgroup of people that share such a common knowledge. From a corporate perspective, “Tribal Knowledge or know-how is the collective wisdom of the organization. It is the sum of all the knowledge and capabilities of all the people”. (“Tribal Knowledge” 2020) When you code the procedures, not only are you creating letters to yourself in the future, but you’re also writing letters to everyone else on your team, even those who haven’t joined. These fully contained notebooks serve as a guide to those on your team who are learning what you have developed. The next thing to do is to put them in a central location where everyone on your team can access them. While you can opt for files and folders on a network drive, more practical technologies exist. Code repositories, such as git and subversion, serve a purpose in your data analytics environment to track changes to code and notebooks. You can encourage your team to upload notebooks to these repositories to access the latest and greatest code that your team has developed for problems already solved. New team members can go into a repository to learn how code has solved prior problems, and become inspired to solve problems of their own. As your team gains more experience and consistency, it may be more practical to write repeatable processes and functions instead of copy and pasting code between notebooks. Code packages enable you to share your code and functions to solve specific problems, templates to encourage consistency amongst team members, and are easily distributed to your team for quick deployment. 2.4 Always deliver useful product The outputs of your work are useless if they are not adopted by the business. This means you need to prioritize your customer and iterate often, to win buy-in, gain momentum and stay on course. Audit teams are always surprised when I say that they have customers. Audit department interactions are across the entire company and up-and-down the entire chain - from manufacturing to engineering, IT to accounting, analysts to the c-suite. Audit departments handle all these groups, with kid-gloves and diplomacy. Yet we tend to ignore the #1 customer - ourselves. Audit teams are usually at the mercy of what the business deems most convenient for them - whether in terms of reports, testing controls or even where files are kept. Historically, this was tolerated. Audit teams were composed of accountants that specialized in Excel, IT auditors in testing general controls, and a focus adhering to evolving accounting standards. With the advance in maturity of code-based tools and even commercial audit software, there has been never a better time to code intuition. The idea of these data products is that they promote the cognitive ease - this means, repeated experience, clear display, primed idea, and good mood.(Kahneman 2011) By helping your team feel at ease, but they should deliver both the most trivial of tasks and the most time-consuming tests of applied knowledge. By removing the need to execute these tasks, then the customer can focus on what they’re good at - in the case of auditors, its applying professional judgement. These products can be taken up in the form of data generation, data and trend visualization, or standardize reporting. 2.5 Don’t be a hero Not everything needs to be fixed by you, especially in code. In a well-functioning organization where hundreds of people are supporting different software, application or databases, sometimes the best way to solve the problems found isn’t directly in the work you do, but how you get others to fix systemic issues in the source itself. Key indicators of this are when you have to start hard-coding compensating controls in your code because the upstream data isn’t standardized. For example, in a typical company, employees are issued unique ID numbers. This is preferably at the source of truth - the Human Resources department. This identifier is generally considered reliable and stable, especially if is the source of truth. Consider now that you’re now trying to join the data to a credit card system, where employees are issued credit cards based on their first and last name. If you take an assumption to join this dataset using an individuals first and last name, that would be a reasonable first attempt. However, last names can change over time, or individuals may prefer to go with their middle names, or even simple spelling mistakes can occur. Instead of compensating by adding different ways of ‘joining’ information, its more effective to work with the application’s data owner to see if they would be willing to adopt the unique employee ID instead as a field in their system. 2.6 “The opportunity cost” The idea of opportunity costs presumes the fungibility of human experience: all our activities are equivalent or interchangeable once they are reduced to the abstract currency of clock time, and its wage correlate. (Crawford 2009) An internal auditors’ largest limitation is time. The biggest barrier to the adoption and execution of data analytic talents is the fact there is ‘more’ perceived value in doing something else. A common excuse is that a task will only be performed once (or even worse, at an irregular basis) - auditors may instead opt to perform a manual task in an unsustainable way. Instead of learning a new skill, our opportunity cost is projected against how much time can be used have to perform this task at hand. If your existence was highly limited by the lifespan of a fly, it would be hard to argue against such a position. However, your career is long (hopefully), fruitful and full of exciting and interesting work. Truly engaging work should challenging and rewarding, and applying coding to audit lends itself to a skill worth mastering. Using these skills, no matter how immature, will continue to pay off dividends in future implementations, no matter how insignificant or incremental. Implementing repeatable processes means not only does the end consumer receive their product quicker, but the data auditor is enabled to continuously refine or tackle another code-related problem. And that is worth the opportunity cost, today. References "],
["architecture.html", "Chapter 3 Architecture 3.1 R and RStudio 3.2 Code repositories 3.3 Data products 3.4 Data sources 3.5 Audit data mart", " Chapter 3 Architecture Figure 3.1: Internal Audit Data Analytics Architecture The architecture and tools you select should support and amplify your team, and not be a burden to maintain. Maintenance of databases and applications can also be delegated to application support teams, so you can focus on implementing analytics and delivering data products. Other things to consider: You should have, at a bare minimum, direct access to read-only internal databases or data warehouses. A generic service user account has its merits here when it comes to automation - tying an eventual automation to a user with password expiry every 90 days will make updating passwords feel like quarterly financial reporting. Any software you consider should be able to talk to your company’s internal and external applications. Be wary of software that locks you in or makes access to data cumbersome, as it limits your ability to integrate with your company’s tools as you gain sophistication. 3.1 R and RStudio R is a programming language with an emphasis on statistics. It is considered free software, free in the perspective that you can run, distribute, and change it as you like. What makes R so great is the large suite of packages that are available to use to help analyze information. What doesn’t exist in base R will likely have been developed by someone else: there are hundreds of packages that support data connectivity (DBI, xlsx), day-to-day manipulation and analysis (dplyr, ggplot2), and even auditing (jfa, MUS). CRAN is the central repository for all these packages that meet minimum quality standards. As R is a language, you may want to consider an application to code in, similar to how you may write memos in Microsoft Word. RStudio Desktop is an integrated development environment (IDE) that has an open source version and is free to use. There are also free versions of RStudio Server, as well as commercially supported versions of its Desktop, Server, and two unique products that we will go into more later - Package Manager and Connect. With both R and RStudio installed, you can perform the minimum requirements of your audit data analysis career: you can download data, wrangle it, visualize it, and save completed analyses. The potential is limitless though, as it enables all the other technologies to operate with it - machine learning, automation, dashboarding, and code sharing. 3.2 Code repositories Here is a typical logistical challenge faced in even the smallest of audit teams. Person A will write the first version of a script to download data (V1). Person B in the team may want to use it to download data from elsewhere, so they will get an emailed script file from person B, modify it, and start to use it. Person A may make improvements to their own file (now V2), but Person B will not benefit from the improvements. Knowledge are immediately siloed, and changes become more difficult to share between auditors. One of the most effective ways to solve this problem is to leverage a code repository (also known as version control). While version control has several software engineering advantages, the most notable advantages for audit teams are: Centralizing domain knowledge and sharing code, Tracking and tracing changes to code throughout time (including who, what was changed, and when), and Ability to test with an isolated code set before rolling out to production. Code versioning technologies resonate closely with IT General Controls and even the COBIT framework. 3.2.1 Git git is a type of version control. Another free, open source software, and the basic usage of the tool is accessible. The basics of git are: Pull code from the remote server to the local computer. Write and save code on your local computer. Commit code on your local computer. Push code from your local computer to the remote server. While this is a superficial example of how to use git, it is enough to get the most basic of audit teams started. The git hole can go very deep, so if things don’t go according to plan, just remember that you can always make a new folder and start fresh. Several different server technologies support git - GitHub, Azure Repos within Azure DevOps, gitlab are all willing to host your code. If you’re using these to hold proprietary or sensitive code, it would be wise to get your company’s support and pay for the ability to have a private repository that only your team can see. 3.2.2 Packages As you write more code and templates, you will eventually want to share these new techniques with others on your team. Packages put your best practices together, including templates, functions to solve common problems, and templates for common workflows. In short: packages contain tasks help you do your job, and save you time. Packages go beyond the tangible and provide several qualitative benefits as well. They standardize your team’s workflow, create consistency and readability standards, and get your team to speak a single language. It creates a cohesive foundation where anyone on the team can contribute, and a library for those who wish to learn more. However, you do not need to force your team to copy-and-paste a folder or even a Word document from a template folder. The most elegant way to distribute your code is via packages. These packages can be hosted on a private or public code repository, enabling your team to download best practices with a simple line of code. You can also compile your packages and leave them on a network drive. The best part? You can give your package a creative name that represents the culture of your internal audit team or organization. Just don’t call it auditR. 3.3 Data products All auditors face this problem at one point or another. An audit finding doesn’t matter until someone takes accountability for it. And if an audit finding is too vague, or does not have the buy-in from the right stakeholders, it is as good as unsolicited advice. The data analysis you deliver are vulnerable to the same expectations. Not only do they need to be objective and accurate, but they also need to be accessible, relevant, and delivered at the right time. If there are too many barriers for entry or usage, someone will revert back to their old processes and methods. Here is a sobering thought: when was the last time you made a beautiful visualization for someone, only to be asked for the detailed Excel worksheet afterwards? Or instead of an automated report that users can self-serve, someone asks for an email instead? Data products are deliverable that enhance your customer’s ability to gather, process and analyze information. They facilitate decision making and are integrated into their processes. They should be designed thoughtfully and simply, masking the complexity underneath. In short, they should make your co-auditors feel like rockstars. The reports you write and code you develop should strive for the goal of being accessible by your team, on-time and on-demand. 3.3.1 Galvanize Highbond Galvanize, the parent company of ACL Analytics and one of the most popular companies in the audit software industry, invested significant efforts into their cloud-based working paper solution, Highbond. One of their modules, Highbond Results, is an unrivaled for audit exception and remediation workflow. The idea with an audit exception workflow is that audit testing will identify an actionable transaction or outcome. This may be an exception within a process, a control requiring execution, or even a request for additional information or clarification. Once a process has been designed, Highbond Results will allow you to focus on the users who should action the workflow and the rules for setting up triggers. While Galvanize is a separate, third-party product that is not directly in the R universe, it does integrate with R through its API. The API enables you to upload findings and results directly into a Results set, allowing you to create the workflows on the website. The API also enables access to its Projects data that your audit team may already use to document audits, offering advantages to audit teams that design their workpaper environments effectively. Highbond Results also provides capabilities to do no-code based visualizations hosted on the web. Once set up, they offer a stable method of delivering storyboards and visualizations. Galvanize cloud-based tools are fully hosted, meaning audit teams pay for high availability and security maintained by a professional team. Galvanize supports the security, design and coding for hosting a tool online, allowing you to focus on designing workflows for your internal customers. 3.3.2 RStudio Connect RStudio PBC, which became a Certified B Corporation in 2020, offers the RStudio Connect solution that allows R deliverable to be hosted online. These include: RMarkdown notebooks, which are fully self-contained analytics. These notebooks perform full analyses from start to finish, including downloading data, wrangling, analysis and data visualization. These notebooks can also act at Extract, Transform, and Load (ETL) processes. These notebooks have the advantage of being automatically scheduled, with rules that can notify stakeholders if need be. The loading component can be any destination - most popular is the audit data mart, or into other web applications via API connectivity (example: Galvanize Highbond Results). The hosting of Shiny apps, which are interactive web applications, offer a way to analyze and present information in an intelligent, slick manner. The analysis performed in R can be factored into a Shiny app, which can be hooked directly into your data. For audit teams with expertise in programming, RStudio Connect offers some of the best capabilities for publishing visualizations and analysis to your teams and internal stakeholders. With its git capability, it can also receive updates from a code repository, integrating tightly with a team’s best practices of a code repository. RStudio server software will require talent and cost to stand-up and maintain, and should be considered in an environment where automation and internal hosting of data products will bring advantages to an internal audit team. Lightweight alternatives include the hosting a free Shiny Server or low-cost publishing to Shinyapps.io. 3.3.3 RStudio Package Manager As your team develops more code and functions, there becomes a greater need to distribute these best practices easily. RStudio Package Manager offers the capability to distribute code packages to your coworkers and even the broader organization. By integrating with your code repository, it can bundle new functionality added by your team and distribute it. It offers versioning of packages, for those audit environments where reproducability is paramount. An alternative is also miniCRAN, a reduced feature set yet free and open source version, or even simply hosting packages on a network drive. 3.4 Data sources The end goal of automation ETLs, and creating and hosting data products for audit customers to consume. To achieve these goals, an audit team needs direct, programmatic access to the data being audited. By programmatic, we mean that code can be used to access data, versus going through a front-end user interface. 3.4.1 Internal databases At its core, databases hold transactional information that runs the business. Databases generally can fit two separate use cases: Online Transaction Processing (OLTP), for high speed transaction writing, and Online Analytic Processing (OLAP), for analyzing to support business decisions. Typically OLTP databases act as a source of truth, and send updates to the OLAP database. The language of choice to access internal databases is Structured Query Language (SQL). As a defacto standard for accessing databases since 1970’s, all relational databases still leverage SQL. While each brand may have subtle nuances in the way SQL works, this essential language will allow your audit team to access a majority of sources within the company. Non-relational databases and NoSQL are becoming more mainstream, as well as graph databases, so your audit team will need to tool up as necessary. Audit teams historically had a bad reputation for ‘bringing down the database’. While this used to be quite common, nowadays computing power is so accessible and cheap that the fear is generally unwarranted. Great caution should still be taken, where you should avoid querying any production database that supports customers or staff directly, unless absolutely necessary. Other strategies are useful depending on the circumstances: Filter the data with WHERE clauses, Perform joins in the database, instead of downloading multiple tables individually and joining them on the desktop, Download data in chunks or segments, split by day, week or month, or Schedule queries during off-business hours. 3.4.2 External sources As more applications move to the cloud, using SQL to access data becomes more difficult. While some of this data can be brought into an internal data warehouse or used as an integration, more often than not transactional data is left online within the tool. Certain online cloud software providers will make an Application Programming Interface (API) available for customers These APIs open a window to the cloud application, where subsets of data can be downloaded from the system. Each vendor may provide API documentation, and then can be accessed via packages like httr and digested with jsonlite. While APIs enable audit teams to retrieve data, it adds a layer of difficulty in obtaining data: Each API behaves differently - authentication, calls, and the data return all may vary between systems. APIs endpoints, the part that allows a tool to query it, tend to be highly specific-use cases, and may provide a limited scope of data at one time. Data may not be in the format that you want, or even exist - you are at the mercy what the API supplies. If an API does allow for a larger data download, it may be limited by pagnation, where multiple results are spread out over multiple pages. APIs may be ‘rate-limited’, which means it may restrict the number of queries, whether in parallel or in sequence. Tips for audit teams needing to rely on cloud providers: API access should be part of the requirements before signing an agreement with a provider. Ask for, and generate, an API key for usage, and treat it like a password. If APIs are out of the skill of your team, consider asking your IT department to schedule a data pull into your audit data mart. 3.5 Audit data mart While internal databases are accessible, generally they hold the entirety of the company’s data, which is far too much information. Only a fraction of the data is considered important and significant to the audit team. An audit data mart is a key piece of infrastructure that will sit between your data products and data source. This data mart should contain highly specific, refined and cleaned data, which improves the speed and responsiveness of data products and data-on-demand. The audit data mart can also be secured to your audit team members, so confidentiality is not compromised. To take full advantage of an audit data mart, an automated ETL process should connect directly to internal databases, and perform transformations to get to a clean end product. ETLs can be created and scheduled within RStudio Connect. "],
["setup.html", "Chapter 4 Setup 4.1 R and RStudio 4.2 Common packages 4.3 Audit Package helpers 4.4 GitHub (or another git-related brand)", " Chapter 4 Setup There are countless number of guides to setting up your local R and RStudio environment. If you’re learning on your own, its easy to get some of the below applications and packages installed. 4.1 R and RStudio One of the most respected introductions to R is R for Data Science by Hadley and Garrett, and the Prerequisities section is set up for installing R. 4.2 Common packages We will use several common packages; if you haven’t installed them yet, feel free to install the packages: install.packages(c(&#39;tidyverse&#39;)) 4.3 Audit Package helpers For those customers who use the cloud-based Galvanize Highbond (and specifically the Results Module), we will use the R package galvanizer to use Results information in the applied analytics components. remotes::install_github(&#39;jonlinca/galvanizer&#39;) 4.4 GitHub (or another git-related brand) There are many different companies that provide git - Azure, Atlassian, Gitlab, etc. If you’re learning, GitHub is a fantastic free source to set up shop at. Within your own company, see what the IT and developers are using, as its far more convenient to jump onto that. Like the R tutorial, Happy Git and GitHub for the useR by Jenny Bryan et. al. is a practical guide to installing R, setting up keys and the core fundamentals. "],
["import-data.html", "Chapter 5 Import data 5.1 Delimited files 5.2 Databases 5.3 APIs", " Chapter 5 Import data In this chapter, we will download some datasets and import them. You will need the following packages to follow along. library(dplyr) # For manipulating data library(readr) # For reading flat files library(DBI) # For database connections # For API connections library(httr) library(jsonlite) 5.1 Delimited files The most common method of obtaining data is via flat files, usually in the form of comma separated files (CSV). While delimited data sources are the most convenient for data sources where direct data connections are otherwise unobtainable, they are not set up for long term sustainability and automation. The base package, installed with all instances of R, and read.table() is a convenient built-in standard function for importing CSV files. Another package, readr, includes a similar function called read_delim(), which is faster and allows for easy altering of column specifications, which directs the data types each column is imported as (for example, overriding an employee’s identification number as a character versus an numeric). Before importing the file, lets download the file from the repository that contains the Vendor Master. This dataset contains the vendor system ID number, the date it was added, and other traits. dir.create(&quot;data&quot;, showWarnings = FALSE) # Creates a directory in your project download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/vendor_master.csv&quot;, destfile = &quot;data/vendor_master.csv&quot;, mode = &quot;wb&quot;) # Downloads this csv file into the data folder When importing delimited files, there will always be a few aspects to consider. The delimiter is the character that separates each field - while commas (,) are popular, pipes (|) and tabs are also used as they tend to be less common. Using an uncommon character was a typical workaround when exporting data from legacy systems, as commas within text fields were incorrectly parsed as extra columns. If possible, qualifiers should used to enclose text with a field, typically quotes or double quotes. This will indicate to the system that everything within those quotes belongs to a specific field. In this case, if you view the file (on the Files panel on the right, go to the data folder, click on vendor_master.csv, and select View File), you will see the data is separated by commas. raw_vendors &lt;- read_delim(&#39;data/vendor_master.csv&#39;, delim = &quot;,&quot;) ## Parsed with column specification: ## cols( ## id = col_double(), ## name = col_character(), ## date_added = col_date(format = &quot;&quot;), ## spend_2015 = col_double(), ## spend_2016 = col_double(), ## spend_2017 = col_double(), ## spend_2018 = col_double() ## ) The message indicates that default column types were assigned to each field imported. If it all the fields imported as expected, this message can be ignored. However, ID numbers, while presented as a number, don’t have a real value in any calculations. As a result, you can specify a column specification via the col_types argument, copy and pasting the framework in the message and changing the fields as need be: cols &lt;- cols( id = col_character(), # Changed from col_double() name = col_character(), date_added = col_date(format = &quot;&quot;), spend_2015 = col_double(), spend_2016 = col_double(), spend_2017 = col_double(), spend_2018 = col_double() ) raw_vendors &lt;- read_delim(&#39;data/vendor_master.csv&#39;, delim = &quot;,&quot;, col_types = cols) glimpse(raw_vendors) ## Rows: 6 ## Columns: 7 ## $ id &lt;chr&gt; &quot;656&quot;, &quot;387&quot;, &quot;805&quot;, &quot;2894&quot;, &quot;2531&quot;, &quot;2695&quot; ## $ name &lt;chr&gt; &quot;Henkels, Jaylene&quot;, &quot;Kirchner, Jasmine&quot;, &quot;Ro, Kiet&quot;, &quot;el-D… ## $ date_added &lt;date&gt; 2019-05-31, 2019-11-25, 2019-03-24, 2019-04-03, 2019-09-1… ## $ spend_2015 &lt;dbl&gt; NA, NA, NA, 25818.04, 93406.11, 90002.42 ## $ spend_2016 &lt;dbl&gt; NA, NA, NA, 29988.47, 155650.27, 111821.93 ## $ spend_2017 &lt;dbl&gt; NA, NA, NA, 52853.61, 196058.76, 177148.54 ## $ spend_2018 &lt;dbl&gt; NA, NA, NA, 35027.7, 111956.2, 112272.5 5.1.1 Handling problematic delimited files While well exported delimited files can be useful, they often contain hidden surprises. Consider this rather innocuous csv file from active directory (the controller for Windows authentication), with a username and manager fields: # Active directory file, with just a username and manager field. download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/active_directory.csv&quot;, destfile = &quot;data/active_directory.csv&quot;, mode = &quot;wb&quot;) # Downloads this csv file into the data folder raw_ad &lt;- read_delim(&#39;data/active_directory.csv&#39;, delim = &quot;;&quot;) ## Parsed with column specification: ## cols( ## uid = col_character(), ## manager = col_character() ## ) ## Warning: 2 parsing failures. ## row col expected actual file ## 2 -- 2 columns 1 columns &#39;data/active_directory.csv&#39; ## 4 -- 2 columns 1 columns &#39;data/active_directory.csv&#39; These warnings indicate that there were columns expected (as dictated by the first line of column headers), but missing in one or more lines. You can inspect the csv file in the RStudio interface by clicking on the file on the navigation pane to the right, and select ‘View File’. You will notice that the location for both accounts is on a new line, but it belongs to the prior record. The raw characters can be confirmed within R, by reading the file directly as is (i.e. raw): ad_char &lt;- readChar(&#39;data/active_directory.csv&#39;, file.info(&#39;data/active_directory.csv&#39;)$size) print(ad_char) ## [1] &quot;uid;manager\\r\\njonlin; jason durrand\\nlocation: calgary\\r\\nronaldlee; reid turra\\nlocation: melbourne&quot; These special characters are hidden within the seemingly innocuous delimited file, and are typical of systems where information is extracted from, especially Windows, \\r represents a carriage return, and \\n represents a line feed. Together, \\r\\n represents a new line and new record, while \\n can appear in a file when a new line is made by pressing Shift+Enter. In this common yet inconvenient case, these can be substituted out with regular expressions. Regular expressions are a standard, cryptic yet powerful way to match text in strings. We will cover specific use cases of these in Searching Text. In this case, the below regular expression only replaces \\n when there is no \\r preceding it. The gsub() function will try to match the regular expression criteria in the in the first field, with its replacement value in the second field. gsub(&quot;(?&lt;!\\\\r)\\\\n&quot;,&quot; &quot;, ad_char, perl = TRUE) ## [1] &quot;uid;manager\\r\\njonlin; jason durrand location: calgary\\r\\nronaldlee; reid turra location: melbourne&quot; The gsub output shows that the manager’s name and location no longer has a \\n in between them. As a result, it can now be imported cleanly. ad_raw &lt;- read_delim(gsub(&quot;(?&lt;!\\\\r)\\\\n&quot;,&quot; &quot;, ad_char, perl = TRUE), delim = &quot;;&quot;) print(ad_raw) ## # A tibble: 2 x 2 ## uid manager ## &lt;chr&gt; &lt;chr&gt; ## 1 jonlin &quot; jason durrand location: calgary&quot; ## 2 ronaldlee &quot; reid turra location: melbourne&quot; 5.2 Databases It is likely that the company you are auditing will have their data stored in a database. While having skills in SQL is recommended, having R skills means you are able to perform basic queries on databases. There are many different database brands and vendors in the world, and thus there are many different subtleties on how SQL works for each vendor, but they mostly adhere to the same principles,. The Open Databases Connectivity (ODBC) standard allows different vendors to write drivers, or the technical back-end methods, to connect to their database. Generally, you will need a driver that matches the vendor and version of the database you’re using. Installing a driver is straight forward The Database Interface (DBI) is the interaction between R and the driver. Practically speaking, it enables R to send queries to the database via the driver that is defined in the ODBC. The most common way to connect to a database on your network is to install the vendor drivers, and then create a Data Source Name (DSN). To properly create this DSN, you’ll need the name of your database, as well as read-only credentials. Alternatively, you may specify the server name, database schema and credentials explicitly, which offers some advantages from a portability perspective as your other team mates will not need to create DSNs, and only need to install the drivers themselves. For this example, we will use an SQLite database, which are self sustained databases files and perfect for lightweight applications (including training!) Again, lets start by downloading the file, or in this case, a database: dir.create(&quot;data&quot;, showWarnings = FALSE) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) One thing that is different about connecting to databases is that you need to set up a database connection within R. This will usually consist of a driver (in this case, RSQLite::SQLite()), a DSN or a file location (data/rauditanalytics.sqlite). In the help file, it asks for other needed fields as well; user, password, host etc. At your company, having that information along with ports, schema names, and whether or not its a trusted connection (authenticating automatically with your own active directory credentials). If you haven’t yet already at your company, request a read-only account that can get this information. We’re going to establish a connection with the database, creating the con connection object: con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) You can confirm it works by listing the tables in the database. One important thing to remember is that you’ll be passing this connection object each time as you perform your commands. For example, if you want to see what tables exist in the database and schema, you will still need to tell the command which database you want to connect to. dbListTables(con) ## [1] &quot;gl&quot; &quot;tb&quot; &quot;vendors&quot; Typically to get data out of a database, you’ll need to communicate to it in its own language to obtain data - SQL (Structured Query Language). Here is an example of how to select all the records in a table: dbGetQuery(con, &#39;select * from gl&#39;) ## je_num amount gl_date gl_date_char vendor_id account ## 1 1 30487.47 17984 2019-03-29 2894 exp_materials_6000 ## 2 1 -30487.47 17984 2019-03-29 NA liab_accountspayable_2000 ## description ## 1 Quality control testing supplies ## 2 Quality control testing supplies ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 1998 rows ] While knowing SQL is advantageous (and eventually critical), sometimes switching between languages is a hassle, especially when performing basic tasks within a database. Using the dplyr package, you can generate several of the same queries using the dplyr syntax. All you need to do is create a reference to the table in the connection object - in this case, the con connection object contains the gl table: db_gl &lt;- tbl(con, &#39;gl&#39;) db_gl ## # Source: table&lt;gl&gt; [?? x 7] ## # Database: sqlite 3.30.1 ## # [/Users/jon/Documents/R/auditanalytics/data/rauditanalytics.sqlite] ## je_num amount gl_date gl_date_char vendor_id account description ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 30487. 17984 2019-03-29 2894 exp_material… Quality control … ## 2 1 -30487. 17984 2019-03-29 NA liab_account… Quality control … ## 3 2 5019. 18194 2019-10-25 2695 exp_material… Packaging and bo… ## 4 2 -5019. 18194 2019-10-25 NA liab_account… Packaging and bo… ## 5 3 264. 18051 2019-06-04 2894 exp_material… Quality control … ## 6 3 -264. 18051 2019-06-04 NA liab_account… Quality control … ## 7 4 567. 17903 2019-01-07 2894 exp_material… Medical grade fi… ## 8 4 -567. 17903 2019-01-07 NA liab_account… Medical grade fi… ## 9 5 13623. 18261 2019-12-31 2894 exp_material… Quality control … ## 10 5 -13623. 18261 2019-12-31 NA liab_account… Quality control … ## # … with more rows Not only is it pointing to the same table, but its also performing the same query: db_gl %&gt;% show_query() ## &lt;SQL&gt; ## SELECT * ## FROM `gl` There are a few differences though between using these approaches. dbGetQuery() will return a data.frame that is immediately usable, while creating the table connection via tbl() results in a preview of the query but hasn’t been formally downloaded in the database. In order to use the data within R with tbl(), a further collect() is needed. Instead of performing a data download every time, it is advantageous to preview the results first before collecting it. dbGetQuery requires the entire SQL query to be pre-written, and can not be further leveraged within the database. However, the tbl object can still be further manipulated. This is especially useful when creating new fields or performing joins in same database. Lets say that you wanted to filter amounts greater than $75,000 in the database. In the SQL method, you would need to type a whole new SQL query: dbGetQuery(con, &#39;select * from gl where amount &gt; 75000&#39;) ## je_num amount gl_date gl_date_char vendor_id account ## 1 991 92284.43 17912 2019-01-16 2894 exp_materials_6000 ## description ## 1 Quality control testing supplies Where in the dplyr method, you would only need to build on the same connection object already established. Identical queries, and results db_gl_threshold &lt;- db_gl %&gt;% dplyr::filter(amount &gt; 75000) db_gl_threshold ## # Source: lazy query [?? x 7] ## # Database: sqlite 3.30.1 ## # [/Users/jon/Documents/R/auditanalytics/data/rauditanalytics.sqlite] ## je_num amount gl_date gl_date_char vendor_id account description ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 991 92284. 17912 2019-01-16 2894 exp_materia… Quality control tes… show_query(db_gl_threshold) ## &lt;SQL&gt; ## SELECT * ## FROM `gl` ## WHERE (`amount` &gt; 75000.0) And when you’re happy with this, simply collect() to save the query results as a data frame. To be nice to your database administrators, you should also disconnect from the database to free up a connection. gl_threshold &lt;- db_gl_threshold %&gt;% collect() dbDisconnect(con) We will go through an advanced application of setting up database queries in Audit Package Creation, which builds upon leveraging dplyr to create the pieces that enable powerful queries. 5.3 APIs As more applications are hosted on the cloud, it is an important skill to obtain information from them without resorting to manually triggered reports. Data can be accessed from these systems via a Application Programming Interface (API), and typically it is exposed via the method Representational State Transfer (REST). An API will allow one application to talk to another. Some examples of APIs are web sites with search functions, or loading a list of comments other users have published, or determining who is friends with whom. REST advises how the endpoint is structured, and also suggests how the data is returned. While APIs may be more complicated, the end objective is to obtain data is quite straight forward. A user needs to know what they want, then: match what is desired to the ‘endpoint’, a pre-defined url or path where the data resides, send a valid request to GET the data, and receive the response. Here is an oversimplistic example. Lets say you want to see what repositories I have available to the public. You can list the repositories I have through the github api via the httr package and the GET command. For anything sensitive, it will likely be protected and require authentication as well. library(httr) library(jsonlite) response &lt;- GET(&#39;https://api.github.com/users/jonlinca/repos&#39;) response ## Response [https://api.github.com/users/jonlinca/repos] ## Date: 2020-07-30 02:55 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 28.6 kB ## [ ## { ## &quot;id&quot;: 129155075, ## &quot;node_id&quot;: &quot;MDEwOlJlcG9zaXRvcnkxMjkxNTUwNzU=&quot;, ## &quot;name&quot;: &quot;ACLandR_DeployingModels&quot;, ## &quot;full_name&quot;: &quot;jonlinca/ACLandR_DeployingModels&quot;, ## &quot;private&quot;: false, ## &quot;owner&quot;: { ## &quot;login&quot;: &quot;jonlinca&quot;, ## &quot;id&quot;: 29468012, ## ... The structure of the data in an API looks different than a data frame. Typically there is an overall response (indicating success or the nature of the failure), the type of content (in this case, ‘application/json’), and the data itself. The entire download is usually stored as a list with multiple references: names(response) ## [1] &quot;url&quot; &quot;status_code&quot; &quot;headers&quot; &quot;all_headers&quot; &quot;cookies&quot; ## [6] &quot;content&quot; &quot;date&quot; &quot;times&quot; &quot;request&quot; &quot;handle&quot; And the data you want will be in list$content: head(response$content) ## [1] 5b 0a 20 20 7b 0a This raw data is generally uninterpretable, and it is because the file is in a different format - in this case, the Content-Type indicates it is a json file. Its quite easy to convert the data in this format to something more understandable with the httr content() and jsonlite fromJSON() and toJSON() functions: content &lt;- content(response) cleancontent &lt;- jsonlite::fromJSON(jsonlite::toJSON(content), flatten = TRUE) # Print it out cleancontent[,c(&#39;id&#39;, &#39;name&#39;)] ## id name ## 1 129155075 ACLandR_DeployingModels ## 2 164948874 ACLandR_KMeans ## 3 269707027 auditanalytics ## 4 183726578 calgaryr ## 5 277917653 galvanizer Some data sources may be difficult to obtain data from, or perhaps you’re not quite ready at the technical skill level to develop your own data connectivity for APIs. One alternative for such information is the use of a third party tool - for example, CData supports an interface that allows you to interact with programs like Office 365 (including emails and file audit logs) directly. "],
["data-completeness.html", "Chapter 6 Data Completeness 6.1 Exploration of General Ledger data 6.2 Examination of potential errors 6.3 Transforming of Trial Balance data 6.4 Asserting Completeness 6.5 Cautionary notes for Completeness", " Chapter 6 Data Completeness The first thing you should do when you get a new piece of data, before you do any analysis, is to validate the data for obvious data errors and to perform completeness testing. The intent of Completeness testing is to evaluate whether you have received a full set of data - data anomalies can be explored further after the preliminary testing (for example, malformed dates, NA information, etc.). Ideally, completeness on a data set should be performed by comparing to another dataset - for example, a detailed ledger can roll up into account balances. Other ways of performing completeness is to compare data against a third party, independently managed source of information. For this section we will use the accounting database and validate our Journal Entry file prior to doing further work. We covered the importing of data in the prior chapter, so we’ll do that again along with some packages: library(dplyr) # For manipulating data library(tidyr) # For making data long and wide library(DBI) # For database connections dir.create(&quot;data&quot;, showWarnings = FALSE) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) # Creates a table reference and collects the table, saving it as a gl object gl &lt;- tbl(con, &#39;gl&#39;) %&gt;% collect() 6.1 Exploration of General Ledger data General Ledger (GL) data is where all the transactions against the accounting system are stored. Sub-ledgers may exist for specific systems (Accounts Payable, Accounts Receivable, Inventory) in a more detailed form, and these systems will also post to the General Ledger. For the data set in our example, we’ve requested and received a full year’s worth of transactions. A rapid preview of all your columns can be done quickly with summary(). Not only does it list all the fields available in the table, but it also gives quick statistics on numeric columns as well (whether it makes sense is up to you): summary(gl) ## je_num amount gl_date gl_date_char ## Min. : 1.0 Min. :-92284 Min. :17897 Length:2000 ## 1st Qu.: 250.8 1st Qu.: -7160 1st Qu.:17998 Class :character ## vendor_id account description ## Min. : 387 Length:2000 Length:2000 ## 1st Qu.:2531 Class :character Class :character ## [ reached getOption(&quot;max.print&quot;) -- omitted 5 rows ] A high level summary scan is useful for us: je_num is a number value, although it has no meaning as a number, as it is a reference. vendor_id is a number as well, but it contains NA’s. NA’s are R’s way of indicating that data does not exist. amount is both positive and negative - indicating this column has indicators of both credits and debits gl_date is numeric while gl_date_char is a character. We will talk about converting these dates to something interpretable in the next chapter. It may be useful to look at a few data samples more closely to understand patterns. head() is useful in seeing the first few columns head(gl) ## # A tibble: 6 x 7 ## je_num amount gl_date gl_date_char vendor_id account description ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 30487. 17984 2019-03-29 2894 exp_material… Quality control t… ## 2 1 -30487. 17984 2019-03-29 NA liab_account… Quality control t… ## 3 2 5019. 18194 2019-10-25 2695 exp_material… Packaging and box… ## 4 2 -5019. 18194 2019-10-25 NA liab_account… Packaging and box… ## 5 3 264. 18051 2019-06-04 2894 exp_material… Quality control t… ## 6 3 -264. 18051 2019-06-04 NA liab_account… Quality control t… We get a more detailed understanding of the fields: je_num indicates a set of lines within a journal entry. vendor_id is usually associated to an expense account. amount is both positive and negative for the same journal entry - this means theoretically, it should balance to zero. gl_date is numeric while gl_date_char appears as a date (but is still a character). 6.2 Examination of potential errors 6.2.1 NA values At this stage, you could explore obvious potential issues - in this case, NA values have surfaced themselves early through the summary() command, so we should explore it a bit. NAs are “Not available” or Missing Values. As an auditor, its important for to understand why NA values exist in your data set. Reasons I have heard in my career include: Data was not recorded - A field may be blank because it was intentionally or accidentally omitted. A “void date” is quite commonly NA in a GL database as most entries have not been voided. Or perhaps a journal entry is NA because it has not yet been approved. There may be business rules that indicate why a row’s value may be NA. Data was not recorded at the time - A data source is always evolving, and new columns may be introduced as new features are rolled out or data structure changes. For example, a relatively new requirement indicating companies must identify government companies within their databases, and only applicable for new companies in the database. Vendors entered prior to this change may be left as NA. Inappropriate coercion - the column type was converted from one to another and a loss of value occured. For example, converting the letter ‘a’ using as.numeric() will give the following: as.numeric(&#39;a&#39;) ## Warning: NAs introduced by coercion ## [1] NA Its not that this value never existed. The letter ‘a’ did exist in the original format, but it doesn’t have a numerical representation within R. As it did not have a valid value when converted to a numeric type, it shows as NA. In our data set, the vendor_id has NA values. We can inspect these by isolating them to determine the nature of the pattern: # Base R equivalent: # gl[is.na(gl$vendor_id), ] # Tidyverse / dplyr gl %&gt;% filter(is.na(vendor_id)) ## # A tibble: 1,000 x 7 ## je_num amount gl_date gl_date_char vendor_id account description ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 -30487. 17984 2019-03-29 NA liab_account… Quality control … ## 2 2 -5019. 18194 2019-10-25 NA liab_account… Packaging and bo… ## 3 3 -264. 18051 2019-06-04 NA liab_account… Quality control … ## 4 4 -567. 17903 2019-01-07 NA liab_account… Medical grade fi… ## 5 5 -13623. 18261 2019-12-31 NA liab_account… Quality control … ## 6 6 -10702. 17959 2019-03-04 NA liab_account… Sewing machines ## 7 7 -16680. 18248 2019-12-18 NA liab_account… Quality control … ## 8 8 -14623 18166 2019-09-27 NA liab_account… Medical grade fi… ## 9 9 -2654. 17926 2019-01-30 NA liab_account… Face mask elasti… ## 10 10 -17685. 18039 2019-05-23 NA liab_account… Medical grade fi… ## # … with 990 more rows This indicates that several values are NA. This enables us to ask the proper questions - specifically, we should seek to understand and corroborate with the business if there is a certain pattern associated to these NAs. dplyr’s group_by() and summarize() are useful for identifying these patterns further: gl %&gt;% filter(is.na(vendor_id)) %&gt;% group_by(account) %&gt;% summarize(n = n(), .groups = &#39;drop&#39;) # Needed to suppress the ungrouping object message ## # A tibble: 2 x 2 ## account n ## &lt;chr&gt; &lt;int&gt; ## 1 liab_accountspayable_2000 990 ## 2 liab_creditcardpayable_2100 10 6.2.2 Journal Entries balance to zero A quick sanity check for the analysis of GL accounts is to do a quick summarization. In this case, you will want to group_by() and summarize() again - in this case, by the je_num will test whether all journal entries will net to zero. gl %&gt;% group_by(je_num) %&gt;% summarize(amount = sum(amount), .groups = &#39;drop&#39;) %&gt;% filter(amount != 0) ## # A tibble: 0 x 2 ## # … with 2 variables: je_num &lt;int&gt;, amount &lt;dbl&gt; 6.3 Transforming of Trial Balance data The Trial Balance (TB) is intended to track and record higher level movements of the General Ledger. It does so by maintaining an accurate balance of debits and credits made to the accounts. Lets look at our TB: tb &lt;- tbl(con, &#39;tb&#39;) %&gt;% collect() head(tb) ## # A tibble: 5 x 13 ## account activity_2019_01 activity_2019_02 activity_2019_03 activity_2019_04 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_co… 37907. 18618. 0 58208. ## 2 exp_ma… 970382. 503354. 699039. 989992. ## 3 exp_me… 0 126. 285. 153. ## 4 liab_a… -1008289. -514495. -699323. -1023042. ## 5 liab_c… 0 -7603. 0 -25311. ## # … with 8 more variables: activity_2019_05 &lt;dbl&gt;, activity_2019_06 &lt;dbl&gt;, ## # activity_2019_07 &lt;dbl&gt;, activity_2019_08 &lt;dbl&gt;, activity_2019_09 &lt;dbl&gt;, ## # activity_2019_10 &lt;dbl&gt;, activity_2019_11 &lt;dbl&gt;, activity_2019_12 &lt;dbl&gt; The TB provided has the net change or net activity level by month. In your day-to-day work, you may also receive a TB that has an Opening and Closing balance - to obtain the net change for the audit period, simply deduct the Close from the Open to calculate the Change for the year, zeroing out the ‘Open’ component for income statement accounts. In our case, we would like to perform completeness testing by account for the entire year, which means we compare the total activity of the account in the TB to the GL. As each column is its own month, we can approach this in several ways - each of the below methods demonstrates how to solve the problem. 6.3.1 Sum by absolute references In traditional “Excel-esque” form, you would add up each column for each row - simply taking the values of each column and adding them together. tb %&gt;% mutate(tb_activity = activity_2019_01 + activity_2019_02 + activity_2019_03 + activity_2019_04 + activity_2019_05 + activity_2019_06 + activity_2019_07 + activity_2019_08 + activity_2019_09 + activity_2019_10 + activity_2019_11 + activity_2019_12) %&gt;% select(account, tb_activity) ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. ## 2 exp_materials_6000 9798845. ## 3 exp_meals_7000 1531. ## 4 liab_accountspayable_2000 -10186325. ## 5 liab_creditcardpayable_2100 -61802. While the above works, there are some risks to this code: the code is difficult to read, as it is a long string of column names, is prone to errors as you have to type out each column, and this approach could only be used once, this year, as when the next audit year rolls around, you would have to manually change the references. 6.3.2 Sum by numeric position To make this code more reusable, we could make some changes. We notice that the activity columns are in the second column through the thirteenth position, so we can assume that the ‘position’ of the columns will never change - second column will always be January, and the thirteenth will be December. names(tb) # This tells us the position number of each column names ## [1] &quot;account&quot; &quot;activity_2019_01&quot; &quot;activity_2019_02&quot; &quot;activity_2019_03&quot; ## [5] &quot;activity_2019_04&quot; &quot;activity_2019_05&quot; &quot;activity_2019_06&quot; &quot;activity_2019_07&quot; ## [9] &quot;activity_2019_08&quot; &quot;activity_2019_09&quot; &quot;activity_2019_10&quot; &quot;activity_2019_11&quot; ## [13] &quot;activity_2019_12&quot; names(tb)[2] # Returns January ## [1] &quot;activity_2019_01&quot; names(tb)[13] # Returns December ## [1] &quot;activity_2019_12&quot; Therefore, we can reference a range of column numbers in our script: tb %&gt;% select(2:13) # Selects just the numeric columns that we assume, by column number ## # A tibble: 5 x 12 ## activity_2019_01 activity_2019_02 activity_2019_03 activity_2019_04 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 37907. 18618. 0 58208. ## 2 970382. 503354. 699039. 989992. ## 3 0 126. 285. 153. ## 4 -1008289. -514495. -699323. -1023042. ## 5 0 -7603. 0 -25311. ## # … with 8 more variables: activity_2019_05 &lt;dbl&gt;, activity_2019_06 &lt;dbl&gt;, ## # activity_2019_07 &lt;dbl&gt;, activity_2019_08 &lt;dbl&gt;, activity_2019_09 &lt;dbl&gt;, ## # activity_2019_10 &lt;dbl&gt;, activity_2019_11 &lt;dbl&gt;, activity_2019_12 &lt;dbl&gt; And now we can summarize by row tb_activity &lt;- tb %&gt;% select(2:13) %&gt;% rowSums() # And sums up the row print(tb_activity) # This is saved as a numeric vector ## [1] 447751.60 9798845.29 1530.63 -10186325.19 -61802.33 The newly calculated row summarization will return the total activity as a numeric vector; each item in the vector represents which row it belonged to. As this is a vector of numbers, we can a vector of the same TB account names and join them together. print(tb$account) # Vector of account names as a character ## [1] &quot;exp_consulting_6500&quot; &quot;exp_materials_6000&quot; ## [3] &quot;exp_meals_7000&quot; &quot;liab_accountspayable_2000&quot; ## [5] &quot;liab_creditcardpayable_2100&quot; With these two vectors, we can create a new data frame with the account name and TB activity we calculated. data.frame(account = tb$account, tb_activity = tb_activity) # We create a new dataframe - one from the character vector in the original trial balance file, the other from the created tb_activity ## account tb_activity ## 1 exp_consulting_6500 447751.60 ## 2 exp_materials_6000 9798845.29 ## 3 exp_meals_7000 1530.63 ## 4 liab_accountspayable_2000 -10186325.19 ## 5 liab_creditcardpayable_2100 -61802.33 6.3.3 Sum by named references In addition of referencing by position number, we can also reference by column name. We want to sum up all columns that start with “activity_”. The selecting by position and selecting by variable name are similar, so we’ll also introduce the ‘dot’ in this select statement. tb %&gt;% mutate(tb_activity = rowSums( select(., contains(&#39;activity_&#39;)) # Only keep the column names with the word &#39;activity_&#39; )) %&gt;% select(account, tb_activity) ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. ## 2 exp_materials_6000 9798845. ## 3 exp_meals_7000 1531. ## 4 liab_accountspayable_2000 -10186325. ## 5 liab_creditcardpayable_2100 -61802. What occurs here is that the dot will take the preceding command (technically known as the ‘left hand side’ or LHS) and feed it directly into the function. So in this case, the command can be narrated as: Using the TB table (this becomes our LHS), create a column named “tb_activity”… Calculate “tb_activity” by identifying all column names containing the word “activity_” from the TB table (referenced by the dot). Using these columns, add them together with rowSums. By introducing and referencing our columns by names, we’ve introduced a more specific and robust way to aggregate our information by account. 6.3.4 Pivot then summarize While the prior methods focused on summing up multiple columns, you could also approach this problem as if it was a wide data set that needed to become long. The ability to pivot data longer and wider is incredibly useful - not only for cleaning, but also for reshaping data into other formats for plotting and preparing for databases. If we look at our original TB data again, we notice there is: one unique identifier (the account name), multiple values for each month (example, activity_2019_01 represents the period with a value January 2019), and the dollar value itself for each month. When we deconstruct our data, it becomes much easier to delve into the tidyr package and the functions pivot_longer() and pivot_wider(). head(tb) # Notice how this data looks wide ## # A tibble: 5 x 13 ## account activity_2019_01 activity_2019_02 activity_2019_03 activity_2019_04 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_co… 37907. 18618. 0 58208. ## 2 exp_ma… 970382. 503354. 699039. 989992. ## 3 exp_me… 0 126. 285. 153. ## 4 liab_a… -1008289. -514495. -699323. -1023042. ## 5 liab_c… 0 -7603. 0 -25311. ## # … with 8 more variables: activity_2019_05 &lt;dbl&gt;, activity_2019_06 &lt;dbl&gt;, ## # activity_2019_07 &lt;dbl&gt;, activity_2019_08 &lt;dbl&gt;, activity_2019_09 &lt;dbl&gt;, ## # activity_2019_10 &lt;dbl&gt;, activity_2019_11 &lt;dbl&gt;, activity_2019_12 &lt;dbl&gt; tb_long &lt;- tb %&gt;% pivot_longer(cols = starts_with(&quot;activity_&quot;), # We want to aggregate the values in these columns names_to = &quot;period&quot;, # What we want to call this new column values_to = &quot;activity&quot;) # And the values we want to take from it tb_long ## # A tibble: 60 x 3 ## account period activity ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 activity_2019_01 37907. ## 2 exp_consulting_6500 activity_2019_02 18618. ## 3 exp_consulting_6500 activity_2019_03 0 ## 4 exp_consulting_6500 activity_2019_04 58208. ## 5 exp_consulting_6500 activity_2019_05 58177. ## 6 exp_consulting_6500 activity_2019_06 19160. ## 7 exp_consulting_6500 activity_2019_07 9919. ## 8 exp_consulting_6500 activity_2019_08 28261 ## 9 exp_consulting_6500 activity_2019_09 28871. ## 10 exp_consulting_6500 activity_2019_10 64969. ## # … with 50 more rows This data is now represented longer - there is now one unique value (activity) for each account and month. From here, we can now summarize(): tb_long %&gt;% group_by(account) %&gt;% summarize(tb_activity = sum(activity)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. ## 2 exp_materials_6000 9798845. ## 3 exp_meals_7000 1531. ## 4 liab_accountspayable_2000 -10186325. ## 5 liab_creditcardpayable_2100 -61802. While any of these approaches will work to calculate the trial balance of activity, there are sustainability advantages where the columns are not verbatim and explicitly mentioned - what matters is how you believe your dataset can change over time, or how easily readable you can communicate your work to a new individual. 6.4 Asserting Completeness The practical goal before we do any further testing is to ensure we’re not wasting time with a data set that is missing information. This completeness test will help validate that we received data for twelve months of GL activity. In Journal Entry testing, this means we will compare the summarized Trial Balance file against the General Ledger entries to obtain reasonableness that our data set we received is complete. To compare both the GL and TB, we will want to aggregate the data in both datasets before joining them together. First, we will aggregate the GL: gl_summarized &lt;- gl %&gt;% group_by(account) %&gt;% summarize(gl_total = sum(amount), .groups = &#39;drop&#39;) # Needed to suppress the ungrouping object message gl_summarized ## # A tibble: 5 x 2 ## account gl_total ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. ## 2 exp_materials_6000 9798845. ## 3 exp_meals_7000 1531. ## 4 liab_accountspayable_2000 -10186325. ## 5 liab_creditcardpayable_2100 -61802. And also aggregate the TB: tb_summarized &lt;- tb %&gt;% mutate(tb_activity = rowSums( select(., contains(&#39;activity_&#39;)) )) %&gt;% select(account, tb_activity) tb_summarized ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. ## 2 exp_materials_6000 9798845. ## 3 exp_meals_7000 1531. ## 4 liab_accountspayable_2000 -10186325. ## 5 liab_creditcardpayable_2100 -61802. To perform a proper test of completeness, we should join both tables together. dplyr and the *_join() family of functions can be used to join tables, and also used as diagnosis tools to help debug information as well. Our GL and TB summarized datasets could be joined by the account column, prior to performing the calculation to identify differences: gl_summarized %&gt;% full_join(tb_summarized, by = &#39;account&#39;) ## # A tibble: 5 x 3 ## account gl_total tb_activity ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. 447752. ## 2 exp_materials_6000 9798845. 9798845. ## 3 exp_meals_7000 1531. 1531. ## 4 liab_accountspayable_2000 -10186325. -10186325. ## 5 liab_creditcardpayable_2100 -61802. -61802. Once you have joined the columns together, a simple difference calculation will let you know what the differences are (if any): gl_summarized %&gt;% full_join(tb_summarized, by = &#39;account&#39;) %&gt;% mutate(tb_diff = gl_total - tb_activity) ## # A tibble: 5 x 4 ## account gl_total tb_activity tb_diff ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 447752. 447752. 0 ## 2 exp_materials_6000 9798845. 9798845. 0 ## 3 exp_meals_7000 1531. 1531. 0 ## 4 liab_accountspayable_2000 -10186325. -10186325. 0 ## 5 liab_creditcardpayable_2100 -61802. -61802. 0 And if you like clean working papers, simply filter to identify where the reconciliation did not work out: gl_summarized %&gt;% full_join(tb_summarized, by = &#39;account&#39;) %&gt;% mutate(tb_diff = gl_total - tb_activity) %&gt;% dplyr::filter(tb_diff != 0) ## # A tibble: 0 x 4 ## # … with 4 variables: account &lt;chr&gt;, gl_total &lt;dbl&gt;, tb_activity &lt;dbl&gt;, ## # tb_diff &lt;dbl&gt; 6.5 Cautionary notes for Completeness While performing completeness, you are proving you have a complete set of data. Exercise caution and ensure you are aware of the following: A completeness check is only as good as the data provided. In the above case, if November was completely excluded in both GL and TB, you would not be able to detect it at this stage if you did not look through the TB data to see that all twelve months were included. Using *_join() functions will generally return all combinations of matches. This is a strong motivation to ensure you have summarized rows by the join columns, prior to joining. Once you have summarized your data, the *_join() functions are far more predictable. full_join() will indicate if there are any accounts missing from either table. In our example, if the tb_summarized data had missing accounts that did exist in the gl_summarized table, the resulting tb_activity column values would show up as NA. anti_join() will show what columns are included on the left, but missing on the right. In our example, if the tb_summarized data had missing accounts that existed in the gl_summarized table, only this account would show up in the results. You may want to consider testing for invalid values or missing dates up front. While the primary goal is to ensure the datasets received are appropriate, you may want to consider validating this information earlier. The next chapter will show you how to further manipulate and test these columns. "],
["clean-and-explore.html", "Chapter 7 Clean and Explore 7.1 Basic math 7.2 Dates 7.3 Graphics 7.4 Summarize 7.5 Join", " Chapter 7 Clean and Explore For this section we will still use the same accounting database, along with some more packages. library(dplyr) # For manipulating data library(tidyr) # For making data long and wide library(ggplot2) # For making visualizations library(DBI) # For database connections dir.create(&quot;data&quot;, showWarnings = FALSE) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) gl &lt;- tbl(con, &#39;gl&#39;) %&gt;% collect() 7.1 Basic math sum mean 7.2 Dates Clean dates 7.3 Graphics Summary statistics 7.4 Summarize 7.5 Join to vendor master to explore longer term trends "],
["test.html", "Chapter 8 Test 8.1 Mutate - If statements 8.2 Round zero 8.3 Rare Accounts 8.4 Sequential 8.5 Outliers 8.6 Duplicates 8.7 Age 8.8 Benford Analysis 8.9 Relative size factor testing 8.10 Search text", " Chapter 8 Test 8.1 Mutate - If statements Weekend date 8.2 Round zero 8.3 Rare Accounts 8.4 Sequential 8.5 Outliers 8.6 Duplicates 8.7 Age TODO (need to insert invoice date and paid date) 8.8 Benford Analysis 8.9 Relative size factor testing 8.10 Search text Facilitation Quebec "],
["report.html", "Chapter 9 Report 9.1 RMarkdown 9.2 Export", " Chapter 9 Report 9.1 RMarkdown 9.2 Export "],
["applied-audit-analytics.html", "Chapter 10 Applied Audit Analytics 10.1 Audit R Package creation 10.2 Continious Monitoring 10.3 Controls Testing Automation 10.4 Audit Data Mart 10.5 All-in-one toolkits", " Chapter 10 Applied Audit Analytics 10.1 Audit R Package creation 10.2 Continious Monitoring 10.3 Controls Testing Automation 10.4 Audit Data Mart 10.5 All-in-one toolkits "],
["other-practices-to-follow.html", "Chapter 11 Other practices to follow 11.1 Documentation 11.2 Passwords", " Chapter 11 Other practices to follow 11.1 Documentation As you start, you should promote basic habits instilled into you. Documenting your basic thought process in .R files is generally expected, and the ‘why’ a certain process outlined with comments (lines starting with a #). The why is important as it explains to code reviewers (external auditors and your peers) the rationale for your approach, or unusual quirks about the data you are transforming. RMarkdown files become valuable as communication mediums for reports, allowing you to embed a mix of code, graphics, and interactive tables. While most of the exploratory work can be done within a basic .R file, having the ability to readily ‘knit’ a document for sharing increases the people you can share your work with. Whether you are using R or RMarkdown files, its convenient to have these files as your primary sources of editing as you can use the keyboard shortcuts command-return or control-enter to send a command from the script file to the console. 11.2 Passwords TODO Storing secrets - .Renviron, config, keyring "],
["references.html", "References", " References "]
]
