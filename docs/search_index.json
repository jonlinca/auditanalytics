[
["index.html", "Audit Analytics with R Welcome", " Audit Analytics with R Jonathan Lin 2021-02-12 Welcome Draft Version - Please post any issues found This is the website for Audit Analytics in R. This audience of this book is for: Audit leaders who are looking to design their environment to encourage cultivate collaboration and sustainability. Audit data analytics practitioners, who are looking to leverage R in their data analytics tasks. You will learn what tools and technologies are well suited for a modern audit analytics toolkit, as well as learn skills with R to perform data analytics tasks. Consider this book to be your roadmap of practical items to implement and follow. If you are brand new to R, I encourage you to read https://rstudio-education.github.io/hopr/ and https://r4ds.had.co.nz. While some limited foundations and explanations will be provided, this book is focused on an applied view of R to the financial auditor practice. "],
["about-the-author.html", "About the author", " About the author Jonathan Lin (Jon) is a Certified Information Systems Auditor, that specializes in audit data analytics. Jonathan firmly believes that anything can be measured, and data should drive decision making. Jonathan promotes the deep integration of an audit analytics in the audit practice, and believes a well structured program can multiply the effectiveness of any audit team. Trained as an IT Auditor, Jon had also led a geographically dispersed data analytics team at a Big 4 firm, before moving to industry to implement and operationalize a continuous monitoring program, detecting fraud and control issues with modern data analytics technology. He was also a co-speaker with Sergiu Cernautan at the 2019 IIA International Conference in Anaheim, California, presenting on the “The Digitally Disruptive Internal Auditor: Future Proofing Your Internal Audit Function.” Jon volunteers with the CalgaryR R User Group, helping promote R literacy through education and speakers in a diverse set of industries. Acknowledgments Thanks to those who have collaborated and contributed their time by reviewing and suggesting fixes: Kamille Espanol, Alex Hebbert, Ryan Liu. Your contributions have been greatly appreciated, and I can not thank you enough. "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction Within the accounting and audit profession, analytics has been around for several decades, under the concept of Computer Aided Auditing Techniques (CAATs). The software of choice was led by ACL and its ACL Analytics software. ACL Analytics was a significant audit enabler at the time, as it allowed direct access to analyze mainframe data and flat files that were otherwise inaccessible by mainstream software on the market. It enabled audit teams to obtain transparency in analysis, a rigorous audit trail, and even automation of scripts. As computers, data analytic technology and accessibility of coding in the Accounting practice has become mainstream, there is an increasing number of options that enable auditors to become more powerful and self sufficient than ever before. Tools that are typically reserved for software engineers and statisticians have empowered financial auditors to expand their breadth and scope, enabling faster response and sustainability. A traditional internal audit team would consider themselves to be consumers of information, limited by flat files sent by emails from their stakeholders. While most internal auditors have implemented data analytics in one way or another, the realm of possibilities and challenges have outpaced audit shop capabilities. The expectation now is for auditors to be fully integrated into the business, and contribute directly to the management of the financial and IT risks the company faces on a regular basis. The most effective way to meet this new standard is to implement a current data analytics architecture and empower your team to leverage modern data analytics techniques. "],
["approach.html", "Chapter 2 Approach 2.1 Code-based development 2.2 Automate relentlessly 2.3 Share everything 2.4 Always deliver useful product 2.5 Don’t be a hero 2.6 “The opportunity cost”", " Chapter 2 Approach For many audit teams, having data analytics employed in your audit team is fundamental requirement. The advantages are well articulated - 100% population coverage, timely response, and targeted high risk identification and results are real and achievable. Unfortunately, any short-burst of investment in data analytics training tends to not last more than a few months. This is due to lack of applied skills, sustained management interest and consistency of values. While regulations for adherence to SEC and Sox requirements exist and are necessary, the means to approaching and achieving compliance has been largely static in nature since the introduction of Sox. The root cause of this systemic failure is that audit teams culturally view analytics as a skill, and not as a cultural shift. On a micro level, while not every individual on your audit team needs to be a ‘coder,’ everyone needs to share the same values and philosophy for any audit analytics program to have success. Your core analytics team will set the foundation for the rest of the team to play on - if the non-data members don’t show up, the analytics program is limited at best. On a macro level, audit teams can no longer live in a bubble. Audit teams need to not only develop tools for their own usage, but also products that can be adopted by the business. By providing this valuable service, the audit team becomes a business adviser, not only offering quantitative advice, but real artifacts that can be used repeatably by the business. Having a combination of the right values and technologies will enable your team to propel forward, and should amplify your teams efforts. By thoughtfully choosing your tools and encouraging sustainable processes, your team will create a positive cycle of development, learning and deployment. 2.1 Code-based development Consider the creation and auditing of a spreadsheet, where every row and column has the potential to be manipulated. Following the motto, ‘trust but verify,’ an auditor would need to examine each cell value, the formulas, the relationships, and keep a sharp eye out for manual adjustments. The auditor also needs to validate the source of data in the spreadsheet. With no built-in traceability to how the spreadsheet is used after it has been created, it contributes to the madness that is the spreadsheet ecosystem. While it is easier to superficially consume information from a spreadsheet, to understand the inner workings is a tedious task into itself. And unfortunately, both controls and audits are conducted without a second thought within spreadsheets, as it is easier to tackle the task without fundamentally changing the approach itself. Contrast this to a code-based environment. To achieve anything in code, you need to be explicit and specific on the mechanisms taken to reach the end state. Each line will tell the program what inputs it needs, how it is processed, and the collection of lines tells you what is achieved. The beauty of this is that it also tells the reader exactly what was executed to achieve the end result. A code based environment is inherently self documenting. Once a baseline code has been established, finding ongoing changes becomes trivial. Similar to Track Changes or blackline comparisons in Microsoft Word or Google Docs, its far easier to detect how code (and the process it supports) has changed. Perhaps with some merit, code can be difficult to read at times, as it is still a completely different language. Notebooks address this problem. Notebooks are interactive renderings of code, containing not only the code, but can also include sections of commentary as to why something was done, and can include data visualizations and interactivity. It is the ultimate form of auditability. 2.2 Automate relentlessly By writing your routines in code, then the next logical step is to automate. Automation not only frees up your time from performing a task, but it also frees up your mental critical thinking and creative processing power. Every professional has a cognitive load - don’t waste it on routine tasks. A common use case for automation in audit is the creation of a data mart that is relevant to auditors. A data mart is a collection of pre-processed data that is suited for the group using it - for example, a data mart may hold a daily refresh of HR information, or the prior month’s vendor invoices. Imagine that instead of needing to email HR for the latest employee list, the audit team merely needs to check within its internal database, making research and response quick and efficient. Instead of asking the vendor management team on how much money was spent per vendor, having this information in an audit mart readily available means you can spend more time thinking about how to assess these vendors for risk. Gone are the days where you only audit a topic once a year. A byproduct of every audit is a monitoring mechanism - how do you know something has gone off the rails? Traditional remediation paths include a follow-up in the future - after that, there are few mechanisms to faithfully maintain confidence that the process is still operating reasonably. If you’ve coded your audit, then you have already done the hard work of structuring how to extract your data, finding exceptions and distributing actionable results for your stakeholders. Automation takes that one step further, and allows you to ensure the steps are done repeatedly. The value you derived initially from your audit can now be done continuously. Once you’ve audited the identification of issues and the metrics around them, then you can focus on the automation of work flow and resolution of them. With an audit controls database, you can upload the results of controls testing and issues identification into it, which will enable on-demand availability, visibility, transparency, and even reporting, all which can now be part of that larger automation process. 2.3 Share everything Tribal knowledge is information or knowledge that is known within a tribe but often unknown outside of it. A tribe, in this sense, may be a group or subgroup of people that share such a common knowledge. From a corporate perspective, “Tribal Knowledge or know-how is the collective wisdom of the organization. It is the sum of all the knowledge and capabilities of all the people.” (“Tribal Knowledge” 2020) When you code the procedures, not only are you creating letters to yourself in the future, but you’re also writing letters to everyone else on your team, even those who haven’t joined. These fully contained notebooks serve as a guide to those on your team who are learning what you have developed. The next thing to do is to put them in a central location where everyone on your team can access them. While you can opt for files and folders on a network drive, more practical technologies exist. Code repositories, such as the technology ‘git,’ serve a purpose in your data analytics environment to track changes to code and notebooks. You can encourage your team to upload notebooks to these repositories to access the latest and greatest code that your team has developed for problems already solved. New team members can go into a repository to learn how code has solved prior problems, and become inspired to solve problems of their own. As your team gains more experience and consistency, it may be more practical to write repeatable processes and functions instead of copy and pasting code between notebooks. Code packages enable you to share your code and functions to solve specific problems, templates to encourage consistency amongst team members, and are easily distributed to your team for quick deployment. 2.4 Always deliver useful product The outputs of your work are useless if they are not adopted by the business. This means you need to prioritize your customer and iterate often, to win buy-in, gain momentum and stay on course. Audit teams are always surprised when I say that they have customers. Audit department interactions are across the entire company and up-and-down the entire chain - from manufacturing to engineering, IT to accounting, analysts to the c-suite. Audit departments handle all these groups, with kid-gloves and diplomacy. Yet we tend to ignore the #1 customer - ourselves. Audit teams are usually at the mercy of what the business deems most convenient for them - whether in terms of reports, testing controls or even where files are kept. Historically, this was tolerated. Audit teams were composed of accountants that specialized in Excel, IT auditors in testing general controls, and a focus adhering to evolving accounting standards. With the advance in maturity of code-based tools and even commercial audit software, there has never been a better time to code intuition. The idea of these data products is that they promote the cognitive ease - this means, repeated experience, clear display, primed idea, and good mood.(Kahneman 2011) By helping your team feel at ease, but they should deliver both the most trivial of tasks and the most time-consuming tests of applied knowledge. By removing the need to execute these tasks, then the customer can focus on what they’re good at - in the case of auditors, its applying professional judgement. These products can be taken up in the form of data generation, data and trend visualization, or standardized reporting. 2.5 Don’t be a hero Not everything needs to be fixed by you, especially in code. In a well-functioning organization where hundreds of people are supporting different software, application or databases, sometimes the best way to solve the problems found isn’t directly in the work you do, but how you get others to fix systemic issues in the source itself. Key indicators of this are when you have to start hard-coding compensating controls in your code because the upstream data isn’t standardized. For example, in a typical company, employees are issued unique ID numbers. This is preferably at the source of truth - the Human Resources department. This identifier is generally considered reliable and stable, especially if is the source of truth. Consider now that you’re now trying to join the data to a credit card system, where employees are issued credit cards based on their first and last name. If you take an assumption to join this dataset using an individuals first and last name, that would be a reasonable first attempt. However, last names can change over time, or individuals may prefer to go with their middle names, or even simple spelling mistakes can occur. Instead of compensating by adding different ways of ‘joining’ information, its more effective to work with the application’s data owner to see if they would be willing to adopt the unique employee ID instead as a field in their system. 2.6 “The opportunity cost” The idea of opportunity costs presumes the fungibility of human experience: all our activities are equivalent or interchangeable once they are reduced to the abstract currency of clock time, and its wage correlate. (Crawford 2009) An internal auditors’ largest limitation is time. The biggest barrier to the adoption and execution of data analytic talents is the fact there is ‘more’ perceived value in doing something else. A common excuse is that a task will only be performed once (or even worse, at an irregular basis) - auditors may instead opt to perform a manual task in an unsustainable way. Instead of learning a new skill, our opportunity cost is projected against how much time can be used have to perform this task at hand. If your existence was highly limited by the lifespan of a fly, it would be hard to argue against such a position. However, your career is long (hopefully), fruitful and full of exciting and interesting work. Truly engaging work should challenging and rewarding, and applying coding to audit lends itself to a skill worth mastering. Using these skills, no matter how immature, will continue to pay off dividends in future implementations, no matter how insignificant or incremental. Implementing repeatable processes means not only does the end consumer receive their product quicker, but the data auditor is enabled to continuously refine or tackle another code-related problem. And that is worth the opportunity cost, today. "],
["architecture.html", "Chapter 3 Architecture 3.1 R and RStudio 3.2 Code repositories 3.3 Data products 3.4 Data sources 3.5 Audit data mart", " Chapter 3 Architecture Figure 3.1: Internal Audit Data Analytics Architecture The architecture and tools you select should support and amplify your team, and not be a burden to maintain. Maintenance of databases and applications can also be delegated to application support teams, so you can focus on implementing analytics and delivering data products. Other things to consider: You should have, at a bare minimum, direct access to read-only internal databases or data warehouses. A generic service user account has its merits here when it comes to automation - tying an eventual automation to a user with password expiry every 90 days will make updating passwords feel like quarterly financial reporting, and also increases the risk that the entire analytics program is dependent on a single person to manage credentials. Any software you consider should be able to talk to your company’s internal and external applications. Be wary of software that locks you in or makes access to data cumbersome, as it limits your ability to integrate with your company’s tools as you gain sophistication. 3.1 R and RStudio R is a programming language with an emphasis on statistics. It is considered free software, free in the perspective that you can run, distribute, and change it as you like. What makes R so great is the large suite of packages that are available to use to help analyze information. This is distributed through CRAN, which is the central repository for R packages that meet minimum quality standards. What doesn’t exist in base R will likely have been developed by someone else: there are hundreds of packages that support data connectivity (DBI, xlsx), day-to-day manipulation and analysis (dplyr, ggplot2), and even auditing (jfa, MUS). As R is a language, you may want to consider an application to code in, similar to how you may write memos in Microsoft Word. RStudio Desktop is an integrated development environment (IDE) that has an open source version and is free to use. There are also free versions of RStudio Server, as well as commercially supported versions of its Desktop, Server, and two unique products that we will go into more later - Package Manager and Connect. With both R and RStudio installed, you can perform the minimum requirements of your audit data analysis career: you can download data, wrangle it, visualize it, and save completed analyses. The potential is limitless though, as it enables all the other technologies to operate with it - machine learning, automation, dashboarding, and code sharing. 3.2 Code repositories Here is a typical logistical challenge faced in even the smallest of audit teams. Person A will write the first version of a script to download data (version 1). Person B in the team may want to use it to download data from elsewhere, so they will get an emailed script file from person B, modify it, and start to use it. Person A may make improvements to their own file (now version 2), but Person B will not benefit from the improvements. Knowledge are immediately siloed, and changes become more difficult to share between auditors. One of the most effective ways to solve this problem is to leverage a code repository (also known as version control). While version control has several software engineering advantages, the most notable advantages for audit teams are: Centralizing domain knowledge and sharing code, Tracking and tracing changes to code throughout time (including who, what was changed, and when), and Ability to test with an isolated code set before rolling out to production. Code versioning technologies resonate closely with IT General Controls and even the COBIT framework. 3.2.1 git git is a type of version control. Another free, open source software, and the basic usage of the tool is accessible. The basics of git are: Pull code from the remote server to the local computer. Write and save code on your local computer. Commit code on your local computer. Push code from your local computer to the remote server. While this is a superficial example of how to use git, it is enough to get the most basic of audit teams started. Trying to master git may take you many months of hands-on experience and collaboration. If a git technical issue results in infinite frustration, just remember that you can always make a new folder and start fresh. Several different server technologies support git - GitHub, Azure Repos within Azure DevOps, gitlab are all willing to host your code. If you’re using these to hold proprietary or sensitive code, it would be wise to get your company’s support and pay for the ability to have a private repository that only your team can see. If you’re learning, GitHub is a fantastic free source to set up shop at. Within your own company, see what the IT and developers are using, as its far more convenient to jump onto that. Like the R tutorial, Happy Git and GitHub for the useR by Jenny Bryan et. al. is a practical guide to installing R, setting up keys and the core fundamentals. 3.2.2 Packages As you write more code and templates, you will eventually want to share these new techniques with others on your team. Packages put your best practices together, including templates, functions to solve common problems, and templates for common workflows. In short: packages contain tasks to help you do your job and save you time. Packages go beyond the tangible and provide several qualitative benefits as well. They standardize your team’s workflow, create consistency and readability standards, and get your team to speak a single language. It creates a cohesive foundation where anyone on the team can contribute, and a library for those who wish to learn more. How do you share packages? You certainly don’t not need to force your team to copy-and-paste code or even a templates from a central folder! The most elegant way to distribute your code is via hosting packages on a code or package repository. These repositories (hosted internally or externally) enable your team to download best practices with a simple line of code. You can also compile your packages and leave them on a network drive. The best part? You can give your package a creative name that represents the culture of your internal audit team or organization. Just don’t call it auditR! 3.3 Data products All auditors face this problem at one point or another. An audit finding doesn’t matter until someone takes accountability for it. And if an audit finding is too vague, or does not have the buy-in from the right stakeholders, it is as good as unsolicited advice. The data analysis you deliver are vulnerable to the same expectations. Not only do they need to be objective and accurate, but they also need to be accessible, relevant, and delivered at the right time. If there are too many barriers for entry or usage, someone will revert back to their old processes and methods. Here is a sobering thought: when was the last time you made a beautiful visualization for someone, only to be asked for the detailed Excel worksheet afterwards? Or instead of an automated report that users can self-serve, someone asked you to just forward them an email instead? Data products are deliverable that enhance your customer’s ability to gather, process and analyze information. They facilitate decision making and are integrated into their processes. They should be designed thoughtfully and simply, masking the complexity underneath. In short, they should make your co-auditors feel like rockstars. The reports you write and code you develop should strive for the goal of being accessible by your team, on-time and on-demand. 3.3.1 Galvanize Highbond Galvanize, the parent company of ACL Analytics and one of the most popular companies in the audit software industry, invested significant efforts into their cloud-based working paper solution, Highbond. Highbond Results is unrivaled for audit exception and remediation workflow. The idea with an audit exception workflow is that audit testing will identify an actionable transaction or outcome. This may be an exception within a process, a control requiring execution, or even a request for additional information or clarification. Once a process has been designed, Highbond Results will allow you to focus on the users who should action the workflow and the rules for setting up triggers. Highbond Results also provides capabilities to do no-code based visualizations hosted on the web. Once set up, they offer a stable method of delivering storyboards and visualizations. While Galvanize and its products (most notably, Projects and Results) are distinct products not directly in the R universe, it does integrate with R through its Application Programming Interface (API). An API allows you to interact with a cloud service via code, and Highbond’s API enables you to upload findings and results directly into a Results set. This enables you to handle the data on R, and then upload the analyzed results online so you can create the workflows on the website. The API also enables access to its Projects data that your audit team may already use to document audits, offering advantages to audit teams that design their workpaper environments effectively. Galvanize cloud-based tools are fully hosted, meaning audit teams pay for high availability and security maintained by a professional team. Galvanize supports the security, design and coding for hosting a tool online, allowing you to focus on designing workflows for your internal customers. 3.3.2 RStudio Connect RStudio offers commercially supported server software, including the RStudio Connect solution that allows R deliverable to be hosted and accessible via a web browser. Features include: R Markdown notebooks, which are fully self-contained analytics. These notebooks perform full analyses from start to finish, including downloading data, wrangling, analysis and data visualization, and can be scheduled on a regular basis. These notebooks can also act as Extract, Transform, and Load (ETL) processes. These notebooks have the advantage of being automatically scheduled, with rules that can notify stakeholders if need be. The loading component can be any destination - most popular is the audit data mart, or into other web applications via API connectivity (example: Galvanize Highbond Results). The hosting of Shiny apps, which are interactive web applications, offer a way to analyze and present information in an intelligent, slick manner. The analysis performed in R can be factored into a Shiny app, which can be hooked directly into your data. For audit teams with expertise in programming, RStudio Connect offers some of the best capabilities for publishing visualizations and analysis to your teams and internal stakeholders. With its git capability, it can also receive updates from a code repository, integrating tightly with a team’s best practices of a code repository. RStudio server software will require talent and cost to stand-up and maintain, and should be considered in an environment where automation and internal hosting of data products will bring advantages to an internal audit team. For those socially conscious teams, you can check off the box by going with RStudio, which is a Certified B Corporation. A company that has has the Certified B Corporation is obligated to the mission of the company, and not to its shareholders. This means that they are to balance both profit and purpose. RStudio has long contributed to free and open-source data science with its packages that are widely adopted within the R ecosystem, and is a significant driver to creating free and accessible education for individuals all over the world. Lightweight alternatives include the hosting a free Shiny Server or low-cost publishing to Shinyapps.io. 3.3.3 RStudio Package Manager As your team develops more code and functions, there becomes a greater need to distribute these best practices easily. RStudio Package Manager offers the capability to distribute code packages to your coworkers and even the broader organization. By integrating with your code repository, it can bundle new functionality added by your team and distribute it. It offers versioning of packages, for those audit environments where reproducability is paramount. An alternative is also miniCRAN, a reduced feature set yet free and open source version, or even simply hosting packages on a network drive. 3.4 Data sources To create and host a data product that is always up-to-date, it needs to be directly connected to data sources and can grab data on-demand to refresh itself. Any manual steps in this process, whether manually triggering a report on a website, or waiting for an email with a data file attachment, means the data has an intolerable source of error - the person who is tasked with manually copying the file to the folder and manually refreshing the data product. A modern audit team will need direct, programmatic access to the data being audited. By programmatic, we mean that code can be used to access data, versus going through a front-end graphical user interface or website. This can be achieved whether internal or external data sources are used. 3.4.1 Internal databases At its core, databases hold transactional information that runs the business. Databases generally can fit two separate use cases: Online Transaction Processing (OLTP), for high speed transaction writing, and Online Analytic Processing (OLAP), for analyzing to support business decisions. Typically OLTP databases act as a source of truth, and send updates to the OLAP database. The language of choice to access internal databases is Structured Query Language (SQL). As a defacto standard for accessing databases since 1970’s, all relational databases still leverage SQL. While each brand may have subtle nuances in the way SQL works, this essential language will allow your audit team to access a majority of sources within the company. Non-relational databases and NoSQL are becoming more mainstream, as well as graph databases, so your audit team will need to tool up as necessary. The preferred approach is to get data from an internal data warehouse. A data warehouse mirrors the data activity from an application’s database - the application processes data and acts as an OLTP, and it will send any processed data to the data warehouse which is an OLAP. That way, critical business functionality can continue on the application with its dedicated computing resources, and intense queries can be directed to the data warehouse with its own set of computing resources. Audit teams historically had a bad reputation for ‘bringing down the database,’ meaning that an attempt to download data crashed the database and made it unaccessible to the business. While this historically used to be a common catastrophe, nowadays computing power is so accessible and cheap that the fear is generally unwarranted. An auditor should still take precautions, and you should avoid querying any production database that supports customers or staff directly unless absolutely necessary. If needed, some other strategies are useful depending on the circumstances: Test queries in a test application database, before trying them in production, Filter the data with WHERE clauses, Perform joins in the database, instead of downloading multiple tables individually and joining them on the desktop, Download data in chunks or segments, split by day, week or month, or Schedule queries during off-business hours. 3.4.2 External sources As more applications move to the cloud, using SQL to access data becomes more difficult. While some of this data can be brought into an internal data warehouse or used as an integration, more often than not transactional data is left online within the tool. Certain online cloud software providers will make an Application Programming Interface (API) available for customers These APIs open a window to the cloud application, where subsets of data can be downloaded from the system. Each vendor may provide API documentation, and then can be accessed via packages like httr and digested with jsonlite. By having the knowledge to access APIs from cloud software, audit teams gain significant autonomy in being able to download the data they need directly from a vendor. It does, however, add a layer of difficulty in obtaining data: Each API behaves differently - authentication, calls, and the data return all may vary between systems. APIs endpoints, the part that allows a tool to query it, tend to be highly specific-use cases, and may provide a limited scope of data at one time. Data may not be in the format that you want, or even exist - you are at the mercy what the API supplies. If an API does allow for a larger data download, it may be limited by pagination, where multiple results are spread out over multiple pages. APIs may be ‘rate-limited,’ which means it may restrict the number of queries, whether in parallel or in sequence. Tips for audit teams needing to rely on cloud providers: API access should be part of the requirements before signing an agreement with a provider. Ask for, and generate, an API key for usage, and treat it like a password. If APIs are out of the skill of your team, consider asking your IT department to schedule a data pull into your audit data mart. 3.5 Audit data mart While internal databases are accessible, generally they hold the entirety of the company’s data, which is far too much information. Only a fraction of the data is considered important and significant to the audit team. An audit data mart is a key piece of infrastructure that will sit between your data products and data source. This data mart should contain highly specific, refined and cleaned data, which improves the speed and responsiveness of data products and data-on-demand. The audit data mart can also be secured to your audit team members, so confidentiality is not compromised. To take full advantage of an audit data mart, an automated ETL process should connect directly to internal databases, and perform transformations to get to a clean end product. ETLs can be created and scheduled within RStudio Connect. "],
["setup.html", "Chapter 4 Setup 4.1 R with RStudio 4.2 Common packages 4.3 Highbond R package", " Chapter 4 Setup The next set of technical chapters are intended to give you more hands-on guidance for using R in an audit environment. You should have R and RStudio installed, as well as some of the packages we intend to use. There are countless number of guides to setting up your local R and RStudio environment. If you’re learning on your own, its easy to get some of the below applications and packages installed. 4.1 R with RStudio One of the most respected introductions to R is R for Data Science by Hadley Wickham and Garrett Grolemund, and the Prerequisities section is set up for installing R. 4.2 Common packages We will use several common packages; if you haven’t installed them yet, feel free to install them into R: install.packages(c(&#39;tidyverse&#39;, &#39;lubridate&#39;, &#39;readr&#39;, &#39;DBI&#39;, &#39;devtools&#39;, &#39;gt&#39;, &#39;pryr&#39;, &#39;stringi&#39;, &#39;rmarkdown&#39;, &#39;tidymodels&#39;, &#39;tidytext&#39;, &#39;tm&#39;, &#39;rpart&#39;, &#39;rpart.plot&#39;, &#39;jsonlite&#39;, &#39;httr&#39;)) 4.3 Highbond R package For those customers who use the cloud-based Galvanize Highbond (and specifically the Results Module), we will use the R package galvanizer to use Results information in the Applied Analytics chapter. This package is maintained by Jonathan Lin during his spare time, and any issues or suggestions on its improvement would be appreciated. install.packages(&#39;galvanizer&#39;) "],
["import-data.html", "Chapter 5 Import data 5.1 Delimited files 5.2 Databases 5.3 APIs", " Chapter 5 Import data In this chapter, we will download some datasets and import them. You will need the following packages to follow along. library(dplyr) # For manipulating data library(readr) # For reading flat files library(DBI) # For database connections # For API connections library(httr) library(jsonlite) 5.1 Delimited files The most common method of obtaining data is via flat files, usually in the form of comma separated files (CSV). While delimited data sources are the most convenient for data sources where direct data connections are otherwise unobtainable, they are not set up for long term sustainability and automation. The base package, installed with all instances of R, and read.table() is a convenient built-in standard function for importing CSV files. Another package, readr, includes a similar function called read_delim(), which is faster and allows for easy altering of column specifications, which directs the data types each column is imported as (for example, overriding an employee’s identification number as a character versus a numeric). Before importing the file, lets download the file from the repository that contains the Vendor Master. This dataset contains the vendor system ID number, the date it was added, and other traits. dir.create(&quot;data&quot;, showWarnings = FALSE) # Creates a directory in your project download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/vendor_master.csv&quot;, destfile = &quot;data/vendor_master.csv&quot;, mode = &quot;wb&quot;) # Downloads this csv file into the data folder When importing delimited files, there will always be a few aspects to consider. The delimiter is the character that separates each field - most common delimiters are commas (,), pipes (|) and tab separators. Using an uncommon character was a typical workaround when exporting data from legacy systems, as commas within text fields were incorrectly parsed as extra columns. If possible, qualifiers should be used to enclose text with a field, typically quotes or double quotes. This will indicate to the system that everything within those quotes belongs to a specific field. In this case, if you view the file (on the Files panel on the right, go to the data folder, click on vendor_master.csv, and select View File), you will see the data is separated by commas. raw_vendors &lt;- read_delim(&#39;data/vendor_master.csv&#39;, delim = &quot;,&quot;) ## Parsed with column specification: ## cols( ## id = col_double(), ## type = col_character(), ## name = col_character(), ## date_added = col_date(format = &quot;&quot;), ## summary = col_character(), ## `2015` = col_double(), ## `2016` = col_double(), ## `2017` = col_double(), ## `2018` = col_double() ## ) The message indicates that default column types were assigned to each field imported. If all the fields imported as expected, this message can be ignored. However, ID numbers, while presented as a number, don’t have a real value in any calculations. As a result, you can specify a column specification via the col_types argument, copy and pasting the framework in the message and changing the fields as need be: cols &lt;- cols( id = col_character(), # Changed from col_double() name = col_character(), date_added = col_date(format = &quot;&quot;), `2015` = col_double(), `2016` = col_double(), `2017` = col_double(), `2018` = col_double() ) raw_vendors &lt;- read_delim(&#39;data/vendor_master.csv&#39;, delim = &quot;,&quot;, col_types = cols) glimpse(raw_vendors) ## Rows: 9 ## Columns: 9 ## $ id &lt;chr&gt; &quot;793&quot;, &quot;666&quot;, &quot;77&quot;, &quot;2726&quot;, &quot;2612&quot;, &quot;2211&quot;, &quot;2726&quot;, &quot;2612&quot;… ## $ type &lt;chr&gt; &quot;E&quot;, &quot;E&quot;, &quot;E&quot;, &quot;V&quot;, &quot;V&quot;, &quot;V&quot;, &quot;V&quot;, &quot;V&quot;, &quot;V&quot; ## $ name &lt;chr&gt; &quot;al-Bangura, Habeeba&quot;, &quot;Akina, Duncan&quot;, &quot;el-Kazmi, Muneefa… ## $ date_added &lt;date&gt; 2015-07-15, 2015-07-28, 2015-05-26, 2015-05-23, 2015-07-0… ## $ summary &lt;chr&gt; NA, NA, NA, &quot;spend&quot;, &quot;spend&quot;, &quot;spend&quot;, &quot;credits&quot;, &quot;credits… ## $ `2015` &lt;dbl&gt; NA, NA, NA, 65325.5287, 88270.0937, 51847.2728, 509.4308, … ## $ `2016` &lt;dbl&gt; NA, NA, NA, 83413.7426, 148569.7780, 89260.3035, 860.3997,… ## $ `2017` &lt;dbl&gt; NA, NA, NA, 150261.9517, 254178.2672, 142074.7898, 908.947… ## $ `2018` &lt;dbl&gt; NA, NA, NA, 55569.8474, 179056.0944, 76761.0587, 347.9129,… You may have noticed that while our normal columns are specified as-is, we had to wrap our years with backticks (`), because while a column can technically be a number, asking R to reference a number will interpret it as a value, and not a field name. 5.1.1 Handling problematic delimited files While well exported delimited files can be useful, they often contain hidden surprises. Consider this rather innocuous CSV file from active directory (the controller for Windows authentication), with a username and manager fields: # Active directory file, with just a username and manager field. download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/active_directory.csv&quot;, destfile = &quot;data/active_directory.csv&quot;, mode = &quot;wb&quot;) # Downloads this csv file into the data folder raw_ad &lt;- read_delim(&#39;data/active_directory.csv&#39;, delim = &quot;;&quot;) ## Parsed with column specification: ## cols( ## uid = col_character(), ## manager = col_character() ## ) ## Warning: 2 parsing failures. ## row col expected actual file ## 2 -- 2 columns 1 columns &#39;data/active_directory.csv&#39; ## 4 -- 2 columns 1 columns &#39;data/active_directory.csv&#39; These warnings indicate that there were columns expected (as dictated by the first line of column headers), but missing in one or more lines. You can inspect the CSV file in the RStudio interface by clicking on the file on the navigation pane to the right, and select ‘View File.’ You will notice that the location for both accounts is on a new line, but it belongs to the prior record. The raw characters can be confirmed within R, by reading the file directly as is (i.e. raw): ad_char &lt;- readChar(&#39;data/active_directory.csv&#39;, file.info(&#39;data/active_directory.csv&#39;)$size) print(ad_char) ## [1] &quot;uid;manager\\r\\njonlin; jason durrand\\nlocation: calgary\\r\\nronaldlee; reid turra\\nlocation: melbourne&quot; These special characters are hidden within the seemingly innocuous delimited file, and are typical of systems where information is extracted from, especially Windows, \\r represents a carriage return, and \\n represents a line feed. Together, \\r\\n represents a new line and new record, while \\n can appear in a file when a new line is made by pressing Shift+Enter. In this common yet inconvenient case, these can be substituted out with regular expressions. Regular expressions are a standard, cryptic yet powerful way to match text in strings. We will cover specific use cases of these in Searching Text. In this case, the below regular expression only replaces \\n when there is no \\r preceding it. The gsub() function will try to match the regular expression criteria in the in the first field, with its replacement value in the second field. gsub(&quot;(?&lt;!\\\\r)\\\\n&quot;,&quot; &quot;, ad_char, perl = TRUE) ## [1] &quot;uid;manager\\r\\njonlin; jason durrand location: calgary\\r\\nronaldlee; reid turra location: melbourne&quot; The gsub output shows that the manager’s name and location no longer has a \\n in between them. As a result, it can now be imported cleanly. ad_raw &lt;- read_delim(gsub(&quot;(?&lt;!\\\\r)\\\\n&quot;,&quot; &quot;, ad_char, perl = TRUE), delim = &quot;;&quot;) print(ad_raw) ## # A tibble: 2 x 2 ## uid manager ## &lt;chr&gt; &lt;chr&gt; ## 1 jonlin &quot; jason durrand location: calgary&quot; ## 2 ronaldlee &quot; reid turra location: melbourne&quot; 5.2 Databases It is likely that the company you are auditing will have their data stored in a database. While having skills in SQL is recommended, having R skills means you are able to perform basic queries on databases. There are many different database brands and vendors in the world, and thus there are many different subtleties on how SQL works for each vendor, but they all operate on the same standard. The Open Databases Connectivity (ODBC) standard allows different vendors to write drivers, or the technical back-end methods, to connect to their database. Generally, you will need a driver that matches the vendor and version of the database you’re using. The Database Interface (DBI) is the interaction between R and the driver. Practically speaking, it enables R to send queries to the database via the driver that is defined in the ODBC. The most common way to connect to a database on your network is to install the vendor drivers, and then create a Data Source Name (DSN). To properly create this DSN, you’ll need the name of your database, as well as read-only credentials. Alternatively, you may specify the server name, database schema and credentials explicitly, which offers some advantages from a portability perspective as your other team mates will not need to create DSNs, and only need to install the drivers themselves. For this example, we will use an SQLite database. Unlike commercial databases you have to install, configure and hire database administrators to manage, a SQLite database is a small self-sustaining file. SQLite files can be kept in your normal documents folders, and are perfect for lightweight applications (including training!) Again, lets start by downloading the file, or in this case, a database: dir.create(&quot;data&quot;, showWarnings = FALSE) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) One thing that is different about connecting to databases is that you need to set up a database connection within R. This will usually consist of a driver (in this case, RSQLite::SQLite()), a DSN or a file location (data/rauditanalytics.sqlite). In the help file, it asks for other needed fields as well; user, password, host etc. At your company, having that information along with ports, schema names, and whether or not its a trusted connection (authenticating automatically with your own active directory credentials). If you haven’t yet already at your company, request a read-only account that can get this information. We’re going to establish a connection with the database, creating the con connection object: con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) You can confirm the connection works correctly by listing the tables in the database. One important thing to remember is that you’ll be passing this connection object each time as you perform your commands. For example, if you want to see what tables exist in the database and schema, you will still need to tell the command which database you want to connect to. dbListTables(con) ## [1] &quot;gl&quot; &quot;industry&quot; &quot;tb&quot; &quot;vendors&quot; To get data out of a database, you’ll need to communicate to it with SQL (Structured Query Language). Here is an example of how to select all the records in a table: dbGetQuery(con, &#39;select * from gl&#39;) ## je_num amount gl_date gl_date_char vendor_id account ## 1 1 22667.46 18232 2019-12-02 2211 exp_materials_6000 ## invoice_date description paid_date ## 1 18225 Packaging and boxes 18262 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 1999 rows ] While knowing SQL is advantageous (and eventually critical), sometimes switching between languages is a hassle, especially when performing basic tasks within a database. Using the dplyr package, you can generate several of the same SQL queries using R syntax. All you need to do is create a reference to the table in the connection object - in this case, the con connection object contains the gl table: db_gl &lt;- tbl(con, &#39;gl&#39;) db_gl ## # Source: table&lt;gl&gt; [?? x 9] ## # Database: sqlite 3.30.1 ## # [/Users/jon/Documents/R/auditanalytics/data/rauditanalytics.sqlite] ## je_num amount gl_date gl_date_char vendor_id account invoice_date ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 22667. 18232 2019-12-02 2211 exp_ma… 18225 ## 2 1 -22667. 18232 2019-12-02 NA liab_a… 18225 ## 3 2 1397. 18228 2019-11-28 2612 exp_ma… 18218 ## 4 2 -1397. 18228 2019-11-28 NA liab_a… 18218 ## 5 3 2319. 17995 2019-04-09 2612 exp_ma… 17985 ## 6 3 -2319. 17995 2019-04-09 NA liab_a… 17985 ## 7 4 20037. 18199 2019-10-30 2612 exp_ma… 18189 ## 8 4 -20037. 18199 2019-10-30 NA liab_a… 18189 ## 9 5 43140. 18051 2019-06-04 2612 exp_ma… 18041 ## 10 5 -43140. 18051 2019-06-04 NA liab_a… 18041 ## # … with more rows, and 2 more variables: description &lt;chr&gt;, paid_date &lt;dbl&gt; Not only is it pointing to the same table, but its also performing the same query, even though it was made up using R and dplyr: db_gl %&gt;% show_query() ## &lt;SQL&gt; ## SELECT * ## FROM `gl` There are a few differences though between using these approaches. dbGetQuery() will return a data.frame that is immediately usable, while creating the table connection via tbl() results in a preview of the query but hasn’t been formally downloaded in the database. In order to use the data within R with tbl(), a further collect() is needed. Instead of performing a data download every time, it is advantageous to preview the results first before collecting it. dbGetQuery requires the entire SQL query to be pre-written, and can not be further leveraged within the database. However, the tbl object can still be further manipulated. This is especially useful when creating new fields or performing joins in same database. Lets say that you wanted to filter amounts greater than $75,000 in the database. In the SQL method, you would need to type a whole new SQL query: dbGetQuery(con, &#39;select * from gl where amount &gt; 75000&#39;) ## je_num amount gl_date gl_date_char vendor_id account ## 1 140 96354.61 18249 2019-12-19 2211 exp_materials_6000 ## invoice_date description paid_date ## 1 18242 Packaging and boxes 18279 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 1 rows ] Where in the dplyr method, you would only need to build on the same connection object already established. Identical queries, and results db_gl_threshold &lt;- db_gl %&gt;% dplyr::filter(amount &gt; 75000) db_gl_threshold ## # Source: lazy query [?? x 9] ## # Database: sqlite 3.30.1 ## # [/Users/jon/Documents/R/auditanalytics/data/rauditanalytics.sqlite] ## je_num amount gl_date gl_date_char vendor_id account invoice_date description ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 140 96355. 18249 2019-12-19 2211 exp_ma… 18242 Packaging … ## 2 693 88524. 18121 2019-08-13 2726 exp_ma… 18114 Quality co… ## # … with 1 more variable: paid_date &lt;dbl&gt; show_query(db_gl_threshold) ## &lt;SQL&gt; ## SELECT * ## FROM `gl` ## WHERE (`amount` &gt; 75000.0) And when you’re happy with this, simply collect() to save the query results as a data frame. To be nice to your database administrators, you should also disconnect from the database to free up a connection. gl_threshold &lt;- db_gl_threshold %&gt;% collect() dbDisconnect(con) We will go through an advanced application of setting up database queries in Audit Package Creation, which builds upon leveraging dplyr to create the pieces that enable powerful queries. 5.3 APIs As more applications are hosted on the cloud, it is an important skill to obtain information from them without resorting to manually triggered reports. Data can be accessed from these systems via an Application Programming Interface (API), and typically it is exposed via the method Representational State Transfer (REST). An API will allow one application to talk to another. Some examples of APIs are web sites with search functions, or loading a list of comments other users have published, or determining who is friends with whom. REST advises how the endpoint is structured, and also suggests how the data is returned. While APIs may appear to be daunting, retrieving data from them is not difficult at all. A user needs to know what they want, then: match what is desired to the ‘endpoint,’ a pre-defined url or path where the data resides, send a valid request to GET the data, and receive the response. Here is an example. Lets say you want to see what repositories I have available to the public on GitHub. You look up the GitHub API documentation, and discover you can list what repositories a user has. You formulate your request, and the request URL becomes https://api.github.com/users/jonlinca/repos Then you send the above URL using the httr package and the GET command. library(httr) library(jsonlite) response &lt;- GET(&#39;https://api.github.com/users/jonlinca/repos&#39;) # Endpoint response ## Response [https://api.github.com/users/jonlinca/repos] ## Date: 2021-02-13 03:55 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 33.9 kB ## [ ## { ## &quot;id&quot;: 129155075, ## &quot;node_id&quot;: &quot;MDEwOlJlcG9zaXRvcnkxMjkxNTUwNzU=&quot;, ## &quot;name&quot;: &quot;ACLandR_DeployingModels&quot;, ## &quot;full_name&quot;: &quot;jonlinca/ACLandR_DeployingModels&quot;, ## &quot;private&quot;: false, ## &quot;owner&quot;: { ## &quot;login&quot;: &quot;jonlinca&quot;, ## &quot;id&quot;: 29468012, ## ... The structure of the data in an API looks different than a data frame. Typically there is an overall response (indicating success or the nature of the failure), the type of content (in this case, ‘application/json’), and the data itself. The entire download is usually stored as a list with multiple references: names(response) ## [1] &quot;url&quot; &quot;status_code&quot; &quot;headers&quot; &quot;all_headers&quot; &quot;cookies&quot; ## [6] &quot;content&quot; &quot;date&quot; &quot;times&quot; &quot;request&quot; &quot;handle&quot; And the data you want will be in the response’s $content: head(response$content) ## [1] 5b 0a 20 20 7b 0a This raw data is generally uninterpretable, and it is because the structure of the data is in a different format - in this case, the Content-Type above indicates it is a json file. Its quite easy to convert the data in this format to something more understandable with the httr content() and jsonlite fromJSON() and toJSON() functions: content &lt;- httr::content(response) cleancontent &lt;- jsonlite::fromJSON(jsonlite::toJSON(content), flatten = TRUE) # Print it out cleancontent[,c(&#39;id&#39;, &#39;name&#39;)] ## id name ## 1 129155075 ACLandR_DeployingModels ## 2 164948874 ACLandR_KMeans ## 3 269707027 auditanalytics ## 4 183726578 calgaryr ## 5 277917653 galvanizer ## 6 298054213 onepass Some data sources may be difficult to obtain data from, or perhaps you’re not quite ready at the technical skill level to develop your own data connectivity for APIs. One alternative for such information is the use of a third party tool - for example, CData supports an interface that allows you to interact with programs like Office 365 (including emails and file audit logs) directly. Office 365 also offers APIs that expose Sharepoint and Outlook, although they take more time to understand. "],
["data-completeness.html", "Chapter 6 Data Completeness 6.1 Exploration of General Ledger data 6.2 Examination of potential errors 6.3 Transforming of Trial Balance data 6.4 Asserting Completeness 6.5 Cautionary notes for Completeness", " Chapter 6 Data Completeness The first thing you should do when you get a new piece of data, before you do any analysis, is to validate the data for obvious data errors and to perform completeness testing. The intent of Completeness testing is to evaluate whether you have received a full set of data - data anomalies can be explored further after the preliminary testing (for example, malformed dates, NA information, etc.). Ideally, completeness on a data set should be performed by comparing to another data set - for example, a detailed ledger can roll up into account balances. Other ways of performing completeness is to compare data against a third party, independently managed source of information. For this section we will use the accounting database and validate our Journal Entry file prior to doing further work. We covered the importing of data in the prior chapter, so we’ll do that again along with some packages: library(dplyr) # For manipulating data library(tidyr) # For making data long and wide library(DBI) # For database connections dir.create(&quot;data&quot;, showWarnings = FALSE) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) # Creates a table reference and collects the table, saving it as a gl object gl &lt;- tbl(con, &#39;gl&#39;) %&gt;% collect() 6.1 Exploration of General Ledger data General Ledger (GL) data is where all the transactions against the accounting system are stored. Sub-ledgers may exist for specific systems (Accounts Payable, Accounts Receivable, Inventory) in a more detailed form, and these systems will also post to the General Ledger. For the data set in our example, we’ve requested and received a full year’s worth of transactions. A rapid preview of all your columns can be done quickly with summary(). Not only does it list all the fields available in the table, but it also gives quick statistics on numeric columns as well (whether it makes sense is up to you): summary(gl) ## je_num amount gl_date gl_date_char ## Min. : 1.0 Min. :-96355 Min. :17897 Length:2000 ## 1st Qu.: 250.8 1st Qu.: -6846 1st Qu.:17986 Class :character ## Median : 500.5 Median : 0 Median :18080 Mode :character ## Mean : 500.5 Mean : 0 Mean :18078 ## 3rd Qu.: 750.2 3rd Qu.: 6846 3rd Qu.:18171 ## Max. :1000.0 Max. : 96355 Max. :18261 ## ## vendor_id account invoice_date description ## Min. : 77 Length:2000 Min. :17887 Length:2000 ## 1st Qu.:2211 Class :character 1st Qu.:17976 Class :character ## Median :2612 Mode :character Median :18072 Mode :character ## Mean :2464 Mean :18069 ## 3rd Qu.:2726 3rd Qu.:18162 ## Max. :2726 Max. :18254 ## NA&#39;s :1000 NA&#39;s :44 ## paid_date ## Min. :17900 ## 1st Qu.:18003 ## Median :18097 ## Mean :18096 ## 3rd Qu.:18188 ## Max. :18291 ## NA&#39;s :44 A high level summary scan is useful for us: je_num is a number value, although it has no meaning as a number, as it is a reference. vendor_id is a number as well, but it contains NA’s. NA’s are R’s way of indicating that data does not exist. amount is both positive and negative - indicating this column has indicators of both credits and debits The dates (gl_date, invoice_date and paid_date) are numeric while gl_date_char is a character. We will talk about converting these dates to something interpretable in the next chapter. It may be useful to look at a few data samples more closely to understand patterns. head() is useful in seeing the first few columns head(gl) ## # A tibble: 6 x 9 ## je_num amount gl_date gl_date_char vendor_id account invoice_date description ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 22667. 18232 2019-12-02 2211 exp_ma… 18225 Packaging … ## 2 1 -22667. 18232 2019-12-02 NA liab_a… 18225 Packaging … ## 3 2 1397. 18228 2019-11-28 2612 exp_ma… 18218 Paper ## 4 2 -1397. 18228 2019-11-28 NA liab_a… 18218 Paper ## 5 3 2319. 17995 2019-04-09 2612 exp_ma… 17985 Paper ## 6 3 -2319. 17995 2019-04-09 NA liab_a… 17985 Paper ## # … with 1 more variable: paid_date &lt;dbl&gt; We get a more detailed understanding of the fields: je_num indicates a set of lines within a journal entry. vendor_id is usually associated to an expense account. amount is both positive and negative for the same journal entry - this means theoretically, it should balance to zero. The dates, gl_date specifically, are numeric while gl_date_char appears as a date (but is still a character). 6.2 Examination of potential errors 6.2.1 NA values At this stage, you could explore obvious potential issues - in this case, NA values have surfaced themselves early through the summary() command, so we should explore it a bit. NAs are “Not available” or Missing Values. As an auditor, its important for to understand why NA values exist in your data set. Reasons I have heard in my career include: Data was not recorded - A field may be blank because it was intentionally or accidentally omitted. A “void date” is quite commonly NA in a GL database as most entries have not been voided. Or perhaps a journal entry is NA because it has not yet been approved. There may be business rules that indicate why a row’s value may be NA. Data was not recorded at the time - A data source is always evolving, and new columns may be introduced as new features are rolled out or data structure changes. For example, a relatively new requirement indicating companies must identify government companies within their databases, and only applicable for new companies in the database. Vendors entered prior to this change may be left as NA. Inappropriate coercion - the column type was converted from one to another and a loss of value occured. For example, converting the letter ‘a’ using as.numeric() will give the following: as.numeric(&#39;a&#39;) ## Warning: NAs introduced by coercion ## [1] NA Its not that this value never existed. The letter ‘a’ did exist in the original format, but it doesn’t have a numerical representation within R. As it did not have a valid value when converted to a numeric type, it shows as NA. In our data set, the vendor_id has NA values. We can inspect these by isolating them to determine the nature of the pattern: # Base R equivalent: # gl[is.na(gl$vendor_id), ] # Tidyverse / dplyr gl %&gt;% filter(is.na(vendor_id)) ## # A tibble: 1,000 x 9 ## je_num amount gl_date gl_date_char vendor_id account invoice_date ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 -22667. 18232 2019-12-02 NA liab_a… 18225 ## 2 2 -1397. 18228 2019-11-28 NA liab_a… 18218 ## 3 3 -2319. 17995 2019-04-09 NA liab_a… 17985 ## 4 4 -20037. 18199 2019-10-30 NA liab_a… 18189 ## 5 5 -43140. 18051 2019-06-04 NA liab_a… 18041 ## 6 6 -13603. 17927 2019-01-31 NA liab_a… 17920 ## 7 7 -7001. 18190 2019-10-21 NA liab_a… 18183 ## 8 8 -24753. 18010 2019-04-24 NA liab_a… 18003 ## 9 9 -2295. 18087 2019-07-10 NA liab_a… 18080 ## 10 10 -34263. 18060 2019-06-13 NA liab_a… 18053 ## # … with 990 more rows, and 2 more variables: description &lt;chr&gt;, ## # paid_date &lt;dbl&gt; This indicates that several values are NA. This enables us to ask the proper questions - specifically, we should seek to understand and corroborate with the business if there is a certain pattern associated to these NAs. dplyr’s group_by() and summarize() are useful for identifying these patterns further: gl %&gt;% filter(is.na(vendor_id)) %&gt;% group_by(account) %&gt;% summarize(n = n(), .groups = &#39;drop&#39;) # Needed to suppress the ungrouping object message ## # A tibble: 2 x 2 ## account n ## * &lt;chr&gt; &lt;int&gt; ## 1 liab_accountspayable_2000 990 ## 2 liab_creditcardpayable_2100 10 6.2.2 Journal Entries balance to zero A quick sanity check for the analysis of GL accounts is to do a quick summarization. In this case, you will want to group_by() and summarize() again - in this case, by the je_num will test whether all journal entries will net to zero. gl %&gt;% group_by(je_num) %&gt;% summarize(amount = sum(amount), .groups = &#39;drop&#39;) %&gt;% filter(amount != 0) ## # A tibble: 0 x 2 ## # … with 2 variables: je_num &lt;int&gt;, amount &lt;dbl&gt; 6.3 Transforming of Trial Balance data The Trial Balance (TB) is intended to track and record higher level movements of the General Ledger. It does so by maintaining an accurate balance of debits and credits made to the accounts. Lets look at our TB: tb &lt;- tbl(con, &#39;tb&#39;) %&gt;% collect() head(tb) ## # A tibble: 5 x 13 ## account activity_2019_01 activity_2019_02 activity_2019_03 activity_2019_04 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_co… 28785. 9942. 9715. 9931. ## 2 exp_ma… 1021508. 834766. 746621. 636011. ## 3 exp_me… 1.55 0 0 281. ## 4 liab_a… -1050295. -844708. -756336. -646057. ## 5 liab_c… 0 0 0 -166. ## # … with 8 more variables: activity_2019_05 &lt;dbl&gt;, activity_2019_06 &lt;dbl&gt;, ## # activity_2019_07 &lt;dbl&gt;, activity_2019_08 &lt;dbl&gt;, activity_2019_09 &lt;dbl&gt;, ## # activity_2019_10 &lt;dbl&gt;, activity_2019_11 &lt;dbl&gt;, activity_2019_12 &lt;dbl&gt; The TB provided has the net change or net activity level by month. In your day-to-day work, you may also receive a TB that has an Opening and Closing balance - to obtain the net change for the audit period, simply deduct the Close from the Open to calculate the Change for the year, zeroing out the ‘Open’ component for income statement accounts. In our case, we would like to perform completeness testing by account for the entire year, which means we compare the total activity of the account in the TB to the GL. As each column is its own month, we can approach this in several ways - each of the below methods demonstrates how to solve the problem. 6.3.1 Sum by absolute references In traditional “Excel-esque” form, you would add up each column for each row - simply taking the values of each column and adding them together. tb %&gt;% mutate(tb_activity = activity_2019_01 + activity_2019_02 + activity_2019_03 + activity_2019_04 + activity_2019_05 + activity_2019_06 + activity_2019_07 + activity_2019_08 + activity_2019_09 + activity_2019_10 + activity_2019_11 + activity_2019_12) %&gt;% select(account, tb_activity) ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 248427. ## 2 exp_materials_6000 9984507. ## 3 exp_meals_7000 1706. ## 4 liab_accountspayable_2000 -10227945. ## 5 liab_creditcardpayable_2100 -6695. While the above works, there are some risks to this code: the code is difficult to read, as it is a long string of column names, is prone to errors as you have to type out each column, and this approach could only be used once, this year, as when the next audit year rolls around, you would have to manually change the references. 6.3.2 Sum by numeric position To make this code more reusable, we could make some changes. We notice that the activity columns are in the second column through the thirteenth position, so we can assume that the ‘position’ of the columns will never change - second column will always be January, and the thirteenth will be December. names(tb) # This tells us the position number of each column names ## [1] &quot;account&quot; &quot;activity_2019_01&quot; &quot;activity_2019_02&quot; &quot;activity_2019_03&quot; ## [5] &quot;activity_2019_04&quot; &quot;activity_2019_05&quot; &quot;activity_2019_06&quot; &quot;activity_2019_07&quot; ## [9] &quot;activity_2019_08&quot; &quot;activity_2019_09&quot; &quot;activity_2019_10&quot; &quot;activity_2019_11&quot; ## [13] &quot;activity_2019_12&quot; names(tb)[2] # Returns January ## [1] &quot;activity_2019_01&quot; names(tb)[13] # Returns December ## [1] &quot;activity_2019_12&quot; Therefore, we can reference a range of column numbers in our script: tb %&gt;% select(2:13) # Selects just the numeric columns that we assume, by column number ## # A tibble: 5 x 12 ## activity_2019_01 activity_2019_02 activity_2019_03 activity_2019_04 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 28785. 9942. 9715. 9931. ## 2 1021508. 834766. 746621. 636011. ## 3 1.55 0 0 281. ## 4 -1050295. -844708. -756336. -646057. ## 5 0 0 0 -166. ## # … with 8 more variables: activity_2019_05 &lt;dbl&gt;, activity_2019_06 &lt;dbl&gt;, ## # activity_2019_07 &lt;dbl&gt;, activity_2019_08 &lt;dbl&gt;, activity_2019_09 &lt;dbl&gt;, ## # activity_2019_10 &lt;dbl&gt;, activity_2019_11 &lt;dbl&gt;, activity_2019_12 &lt;dbl&gt; And now we can summarize by row tb_activity &lt;- tb %&gt;% select(2:13) %&gt;% rowSums() # And sums up the row print(tb_activity) # This is saved as a numeric vector ## [1] 248426.92 9984507.34 1705.97 -10227945.12 -6695.11 The newly calculated row summarization will return the total activity as a numeric vector; each item in the vector represents which row it belonged to. As this is a vector of numbers, we can a vector of the same TB account names and join them together. print(tb$account) # Vector of account names as a character ## [1] &quot;exp_consulting_6500&quot; &quot;exp_materials_6000&quot; ## [3] &quot;exp_meals_7000&quot; &quot;liab_accountspayable_2000&quot; ## [5] &quot;liab_creditcardpayable_2100&quot; With these two vectors, we can create a new data frame with the account name and TB activity we calculated. data.frame(account = tb$account, tb_activity = tb_activity) # We create a new dataframe - one from the character vector in the original trial balance file, the other from the created tb_activity ## account tb_activity ## 1 exp_consulting_6500 248426.92 ## 2 exp_materials_6000 9984507.34 ## 3 exp_meals_7000 1705.97 ## 4 liab_accountspayable_2000 -10227945.12 ## 5 liab_creditcardpayable_2100 -6695.11 6.3.3 Sum by named references In addition of referencing by position number, we can also reference by column name. We want to sum up all columns that start with “activity_.” The selecting by position and selecting by variable name are similar, so we’ll also introduce the ‘dot’ in this select statement. tb %&gt;% mutate(tb_activity = rowSums( select(., contains(&#39;activity_&#39;)) # Only keep the column names with the word &#39;activity_&#39; )) %&gt;% select(account, tb_activity) ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 248427. ## 2 exp_materials_6000 9984507. ## 3 exp_meals_7000 1706. ## 4 liab_accountspayable_2000 -10227945. ## 5 liab_creditcardpayable_2100 -6695. What occurs here is that the dot will take the preceding command (technically known as the ‘left hand side’ or LHS) and feed it directly into the function. So in this case, the command can be narrated as: Using the TB table (this becomes our LHS), create a column named “tb_activity”… Calculate “tb_activity” by identifying all column names containing the word “activity_” from the TB table (referenced by the dot). Using these columns, add them together with rowSums. By introducing and referencing our columns by names, we’ve introduced a more specific and robust way to aggregate our information by account. 6.3.4 Pivot then summarize While the prior methods focused on summing up multiple columns, you could also approach this problem as if it was a wide data set that needed to become long. The ability to pivot data longer and wider is incredibly useful - not only for cleaning, but also for reshaping data into other formats for plotting and preparing for databases. If we look at our original TB data again, we notice there is: one unique identifier (the account name), multiple values for each month (example, activity_2019_01 represents the period with a value January 2019), and the dollar value itself for each month. When we deconstruct our data, it becomes much easier to delve into the tidyr package and the functions pivot_longer() and pivot_wider(). We will discuss the details of pivoting in cleaning data. head(tb) %&gt;% DT::datatable(options = list(scrollX = TRUE)) # Notice how this data looks wide tb_long &lt;- tb %&gt;% pivot_longer(cols = starts_with(&quot;activity_&quot;), # We want to aggregate the values in these columns names_to = &quot;period&quot;, # What we want to call this new column values_to = &quot;activity&quot;) # And the values we want to take from it tb_long ## # A tibble: 60 x 3 ## account period activity ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 activity_2019_01 28785. ## 2 exp_consulting_6500 activity_2019_02 9942. ## 3 exp_consulting_6500 activity_2019_03 9715. ## 4 exp_consulting_6500 activity_2019_04 9931. ## 5 exp_consulting_6500 activity_2019_05 28973. ## 6 exp_consulting_6500 activity_2019_06 37677. ## 7 exp_consulting_6500 activity_2019_07 19295. ## 8 exp_consulting_6500 activity_2019_08 9104. ## 9 exp_consulting_6500 activity_2019_09 28698. ## 10 exp_consulting_6500 activity_2019_10 28753. ## # … with 50 more rows This data is now represented longer - there is now one unique value (activity) for each account and month. From here, we can now summarize(): tb_long %&gt;% group_by(account) %&gt;% summarize(tb_activity = sum(activity)) ## # A tibble: 5 x 2 ## account tb_activity ## * &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 248427. ## 2 exp_materials_6000 9984507. ## 3 exp_meals_7000 1706. ## 4 liab_accountspayable_2000 -10227945. ## 5 liab_creditcardpayable_2100 -6695. While any of these approaches will work to calculate the trial balance of activity, there are sustainability advantages where the columns are not verbatim and explicitly mentioned - what matters is how you believe your dataset can change over time, or how easily readable you can communicate your work to a new individual. 6.4 Asserting Completeness The practical goal before we do any further testing is to ensure we’re not wasting time with a data set that is missing information. This completeness test will help validate that we received data for twelve months of GL activity. In Journal Entry testing, this means we will compare the summarized Trial Balance file against the General Ledger entries to obtain reasonableness that our data set we received is complete. To compare both the GL and TB, we will want to aggregate the data in both datasets before joining them together. First, we will aggregate the GL: gl_summarized &lt;- gl %&gt;% group_by(account) %&gt;% summarize(gl_total = sum(amount), .groups = &#39;drop&#39;) # Needed to suppress the ungrouping object message gl_summarized ## # A tibble: 5 x 2 ## account gl_total ## * &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 248427. ## 2 exp_materials_6000 9984507. ## 3 exp_meals_7000 1706. ## 4 liab_accountspayable_2000 -10227945. ## 5 liab_creditcardpayable_2100 -6695. And also aggregate the TB: tb_summarized &lt;- tb %&gt;% mutate(tb_activity = rowSums( select(., contains(&#39;activity_&#39;)) )) %&gt;% select(account, tb_activity) tb_summarized ## # A tibble: 5 x 2 ## account tb_activity ## &lt;chr&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 248427. ## 2 exp_materials_6000 9984507. ## 3 exp_meals_7000 1706. ## 4 liab_accountspayable_2000 -10227945. ## 5 liab_creditcardpayable_2100 -6695. 6.4.1 Joining the GL to the TB To perform a proper test of completeness, we should join both tables together. dplyr and the *_join() family of functions can be used to join tables, and also used as diagnosis tools to help debug information as well. Our GL and TB summarized datasets could be joined by the account column, prior to performing the calculation to identify differences: gl_summarized %&gt;% full_join(tb_summarized, by = &#39;account&#39;) ## # A tibble: 5 x 3 ## account gl_total tb_activity ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 248427. 248427. ## 2 exp_materials_6000 9984507. 9984507. ## 3 exp_meals_7000 1706. 1706. ## 4 liab_accountspayable_2000 -10227945. -10227945. ## 5 liab_creditcardpayable_2100 -6695. -6695. Once you have joined the columns together, a simple difference calculation will let you know what the differences are (if any): gl_summarized %&gt;% full_join(tb_summarized, by = &#39;account&#39;) %&gt;% mutate(tb_diff = gl_total - tb_activity) ## # A tibble: 5 x 4 ## account gl_total tb_activity tb_diff ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 exp_consulting_6500 248427. 248427. 0 ## 2 exp_materials_6000 9984507. 9984507. 0 ## 3 exp_meals_7000 1706. 1706. 0 ## 4 liab_accountspayable_2000 -10227945. -10227945. 0 ## 5 liab_creditcardpayable_2100 -6695. -6695. 0 And if you like clean working papers, simply filter to identify where the reconciliation did not work out: gl_summarized %&gt;% full_join(tb_summarized, by = &#39;account&#39;) %&gt;% mutate(tb_diff = gl_total - tb_activity) %&gt;% dplyr::filter(tb_diff != 0) ## # A tibble: 0 x 4 ## # … with 4 variables: account &lt;chr&gt;, gl_total &lt;dbl&gt;, tb_activity &lt;dbl&gt;, ## # tb_diff &lt;dbl&gt; 6.5 Cautionary notes for Completeness While performing completeness, you are proving you have a complete set of data. Exercise caution and ensure you are aware of the following: A completeness check is only as good as the data provided. In the above case, if November was completely excluded in both GL and TB, you would not be able to detect it at this stage if you did not look through the TB data to see that all twelve months were included. Using *_join() functions will generally return all combinations of matches. This is a strong motivation to ensure you have summarized rows by the join columns, prior to joining. Once you have summarized your data, the *_join() functions are far more predictable. full_join() will indicate if there are any accounts missing from either table. In our example, if the tb_summarized data had missing accounts that did exist in the gl_summarized table, the resulting tb_activity column values would show up as NA. anti_join() will show what columns are included on the left, but missing on the right. In our example, if the tb_summarized data had missing accounts that existed in the gl_summarized table, only this account would show up in the results. You may want to consider testing for invalid values or missing dates up front. While the primary goal is to ensure the datasets received are appropriate, you may want to consider validating this information earlier. The next chapter will show you how to further manipulate and test these columns. "],
["cleaning-data.html", "Chapter 7 Cleaning data 7.1 Basic math 7.2 Dates and Times 7.3 Separate 7.4 Separate_rows 7.5 Tidy data 7.6 Pivots, wide and longer 7.7 Joining data", " Chapter 7 Cleaning data Cleaning a data set, including fields and reshaping of the data, is a fundamental skill in order to get the data into an analyzable format. Real world data is messy, and will not typically be immediately analyzable immediately after downloading. For this section we will still use the company database, along with some more packages. New packages in this chapter include lubridate and tidyr. library(dplyr) # For manipulating data library(tidyr) # For making data long and wide library(lubridate) # For date handling library(DBI) # For database connections dir.create(&quot;data&quot;, showWarnings = FALSE) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) gl &lt;- tbl(con, &#39;gl&#39;) %&gt;% collect() vendors &lt;- tbl(con, &#39;vendors&#39;) %&gt;% collect() industry &lt;- dbReadTable(con, &#39;industry&#39;) %&gt;% collect() dbDisconnect(con) 7.1 Basic math We covered some of the basic operators in R in the Completeness chapter, so we’ll cover some more calculations and transformations. x &lt;- c(1, 2, 3) y &lt;- c(4, 5, 6) The basic mathematical symbols will calculate a vector with a position element against a different vector with the same position. In this case, the first number in x is added to the first number in y resulting in the first calculation, the second number in x is added to the second number in y resulting in the second calculation, and so on. y + x ## [1] 5 7 9 This distinction is important - vectorization allows R to be fast, but it changes how the user needs to understand objects. sum() is fairly straight forward - it will add all the elements given, together: sum(x) ## [1] 6 If you specify multiple vectors - it will merely combine them all together, equivalent to sum(c(1, 2, 3), c(4, 5, 6)): sum(x, y) ## [1] 21 We mentioned in the prior chapter about NA values, which were ‘Not Available’ or Missing Values. It becomes important to understand the implication of them in your calculations. For example, when a NA value exists in a vector, you will get an NA by default: z &lt;- c(1, 2, NA) sum(z) ## [1] NA While sum(), and several other functions like mean(), min() and max() all generally behave in predictable manners, they all have an argument of na.rm = FALSE by default (enter ?sum to see this argument in the help file. The reason why FALSE is by default is that it brings attention to the vector missing values. If you want to perform the calculation on this, you either need to substitute the NA out for a value (imputation), or simply ignore them. Both these decisions require professional judgment before substitution or ignoring. To substitute NA for a value (for example, 10): is.na(z) # Shows which positions in the vector have an NA value ## [1] FALSE FALSE TRUE z[is.na(z)] &lt;- 10 # Where the z value is is.na, assign 10 sum(z) ## [1] 13 Or alternatively, set the argument na.rm = TRUE to throw away these values completely: z &lt;- c(1, 2, NA) sum(z, na.rm = TRUE) ## [1] 3 7.2 Dates and Times Unlike numbers, date and times are inherently more complex. They require many additional considerations to properly wrangle: What format is the date and time in? Are the timezones as expected? This also applies for dates. Do you want the the month the transaction took place in? The year? The day-of-week? Do you care about daylight savings time? Starting with a basic example, lets understand the basic characteristics of dates. When you define a new date, the lubridate::as_date() function (or base as.Date()) works to convert character strings to dates. startString &lt;- &#39;2020-07-30&#39; # ISO 8601 standards, follow them # base::as.Date(startString) # Same thing as as_date, for now... start &lt;- lubridate::as_date(startString) class(start) ## [1] &quot;Date&quot; You can convert any date to a date, as long as you specify the mask or format. A mask specifies where the date, month, year exist (as well as for time, hours, minutes, seconds). If you check the help file for ?strptime, you will see several different formats for both specifying dates and times. For example, %Y is a four digit year while %y% is a two digit year. Here are a few examples with the same date (some are more unreasonable than others, but all can be captured): lubridate::as_date(&#39;2020-07-30&#39;, format = &quot;%Y-%m-%d&quot;) lubridate::as_date(&#39;30/07/2020&#39;, format = &quot;%d/%m/%Y&quot;) # Not ISO 8601 lubridate::as_date(&#39;July 30, 20&#39;, format = &quot;%B %d, %y&quot;) # Really not ISO 8601 lubridate::as_date(&#39;Jul3020&#39;, format = &quot;%b%d%y&quot;) # Definitely not ISO 8601 The same concept also applies to datetimes, where you can convert a given time to a ‘POSIXct’ or ‘POSIXlt’ format (the differences between ‘POSIXct’ and ‘POSIXlt’ are found in ?DateTimeClasses, under details): starttime_base &lt;- as.POSIXct(&#39;2020-07-30 12:00:00&#39;) starttime_lub &lt;- lubridate::as_datetime(&#39;2020-07-30 12:00:00&#39;) class(starttime_base); class(starttime_lub) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; And format masks apply as well to date time, just in case you get something unconventional: lubridate::as_datetime(&#39;2020-07-30 12:00:00 PM -0600&#39;, format = &quot;%Y-%m-%d %I:%M:%S %p %z&quot;) ## [1] &quot;2020-07-30 18:00:00 UTC&quot; 7.2.1 Understanding Default Time Zones Did you consider the time zone that the times were created in? This is system-specific and base POSIXct and lubridate as_datetime behave differently, so you have to be careful! print(starttime_base); attr(starttime_base,&quot;tzone&quot;) ## [1] &quot;2020-07-30 12:00:00 MDT&quot; ## [1] &quot;&quot; Notice that base treats the creation of the current timezone based off your local R machine, although its not extractable (and you will eventually find, hard to change). print(starttime_lub); tz(starttime_lub) ## [1] &quot;2020-07-30 12:00:00 UTC&quot; ## [1] &quot;UTC&quot; The lubridate version will assign the date-time automatically to UTC, unless you specify it. It is for this reason you will generally want to avoid using base R to convert to date times, and stick with lubridate for better predictability. Having an automatic conversion to UTC isn’t bad per se - having a universal ground truth to UTC is good, as it removes Daylight Saving component, and is an easy way to standardize multiple data sets together. Unfortunately, many systems will not tell you what time zone their data is in, so you will have to figure it out and ensure R is aligned accordingly. 7.2.2 Override or display timezones Hopefully you have been convinced to stick with lubridate for your date transformations. Now, lets get some confidence on these timezone transformations. The first method is force_tz(), which forces the current time to become that same hour in a new timezone. When we assigned July 30th to the date, we would have assumed this date would have been in the current timezone we exist in (generally, the system time-zone you are in). To do this, specify the date, but also the time zone you want the date or time to be harmonized in: starttime_lub # Starts as UTC ## [1] &quot;2020-07-30 12:00:00 UTC&quot; force_tz(starttime_lub, tz = &#39;Canada/Mountain&#39;) # Force the saved time to be Mountain ## [1] &quot;2020-07-30 12:00:00 MDT&quot; Alternatively, you may wish to only convert the time so its displayed in the correct timezone, but leave the base truth as is. with_tz() will allow you to change the timezone that is displayed, while honoring the original time in the data. starttime_lub ## [1] &quot;2020-07-30 12:00:00 UTC&quot; with_tz(starttime_lub, tz = &#39;Canada/Mountain&#39;) # Display the current time as Mountain ## [1] &quot;2020-07-30 06:00:00 MDT&quot; 7.2.3 Daylight Saving Implications There are also timezone implications when it comes to daylight saving time. This is especially important when making audit decisions based on duration, or expected local norms for time-of-day. In several locales across the world, daylight saving time will alter the clock for half a year locally, but has no impact on UTC. For example, your staff may start their day at 8:00 AM locally. When daylight saving kicks in, the local time stays at 8:00 AM and the UTC time shifts from 15:00 to 14:00. pre &lt;- as_datetime(&#39;2020-03-07 8:00:00&#39;, tz = &#39;Canada/Mountain&#39;) # The day before DST starts post &lt;- as_datetime(&#39;2020-03-08 8:00:00&#39;, tz = &#39;Canada/Mountain&#39;) # The day DST started dst(pre); with_tz(pre, tz = &#39;UTC&#39;) # Was DST active before? ; What is the UTC time before? ## [1] FALSE ## [1] &quot;2020-03-07 15:00:00 UTC&quot; dst(post); with_tz(post, tz = &#39;UTC&#39;) # Was DST active after? ; What is the UTC time after? ## [1] TRUE ## [1] &quot;2020-03-08 14:00:00 UTC&quot; 7.2.4 Differences between time Another common calculation is to calculate the time differences between two points of time. This seemingly simple action has several interpretations behind it. Do you want relative dates? Do you want exact elapsed time? People are not always consistent in how they expect time to behave. Sometimes the passage of time is a monotone progression of instants that should be as mathematically reliable as the number line. On other occasions time must follow complex conventions and rules so that the clock times we see reflect what we expect to observe in terms of daylight, season, and congruence with the atomic clock. (Vitalie Spinu 2016) Adding time is easy - you simply need to decide if you’re adding a period (i.e. human construct of days) or if you are following physical time (i.e. seconds). The main difference is choosing lubridate’s functions that honor periods or durations pre &lt;- as_datetime(&#39;2020-03-07 8:00:00&#39;, tz = &#39;Canada/Mountain&#39;) # The day before DST starts post &lt;- as_datetime(&#39;2020-03-08 8:00:00&#39;, tz = &#39;Canada/Mountain&#39;) # The day DST started, skipped ahead 1 hour # Periods: Human construct of days pre + days(1) ; pre + hours(24) # Equal to post, the human construct ## [1] &quot;2020-03-08 08:00:00 MDT&quot; ## [1] &quot;2020-03-08 08:00:00 MDT&quot; # Duration: Physical construct of time pre + ddays(1) ; pre + dhours(24) # NOT to post, but actually 1 hour after ## [1] &quot;2020-03-08 09:00:00 MDT&quot; ## [1] &quot;2020-03-08 09:00:00 MDT&quot; Calculating intervals and periods is a bit different. If we go back up to our example of pre-and-post daylight saving time, we can either expect this answer to be 1 day or 23 hours. Again, depending on your use case, it is up to you.1 span &lt;- interval(pre, post) # Period: To find the number of days, in human constructs span / days(1) # Period calculation by days. ## [1] 1 # Duration - To find the hours between, in physical constructs as.duration(span) ## [1] &quot;82800s (~23 hours)&quot; 7.2.5 Derived date information A common task for auditors is to find out the month a transaction took place in. With lubridate, its quite straight forward - for example, to extract the month, you don’t need to have any messy substrings (or to extract other useful times, for that matter): thedate &lt;- as_datetime(&#39;2020-07-01 12:00:00&#39;, tz = &#39;Canada/Mountain&#39;) month(thedate) ## [1] 7 year(thedate) ## [1] 2020 quarter(thedate) # Canada Day starts on the first day of the third quarter. Does your company Financial Year not start on January 1? Just change the fiscal_start argument. ## [1] 3 wday(thedate, label = TRUE) ## [1] Wed ## Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat 7.2.6 Cleaning numerical dates With dates, sometimes they may represented as a number - it is important to understand your source system date before you convert it. Some systems may calculate a date from an ‘origin’ using the number. While our concept of time may start with 0 AD (as the origin), several computer systems and applications do not behave in that manner. Excel, for example, starts from 1900-01-01, whereas Unix operating systems have their origin as 1970-01-01. If converting a date from a number, you need to research and then specify an origin, otherwise you will have challenges in specifying dates. In our vendors table, we have a numeric date but also a date in a character form for reference vendors %&gt;% select(date_added, date_added_char) ## # A tibble: 9 x 2 ## date_added date_added_char ## &lt;dbl&gt; &lt;chr&gt; ## 1 16631 2015-07-15 ## 2 16644 2015-07-28 ## 3 16581 2015-05-26 ## 4 16578 2015-05-23 ## 5 16620 2015-07-04 ## 6 16652 2015-08-05 ## 7 16578 2015-05-23 ## 8 16620 2015-07-04 ## 9 16652 2015-08-05 By specifying the origin a few different origins, we can see what the origin should have been in this case: vendors %&gt;% select(date_added, date_added_char) %&gt;% mutate(date_added_unix = as_date(date_added, origin = &#39;1970-01-01&#39;), date_added_excel = as_date(date_added, origin = &#39;1900-01-01&#39;)) ## # A tibble: 9 x 4 ## date_added date_added_char date_added_unix date_added_excel ## &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; &lt;date&gt; ## 1 16631 2015-07-15 2015-07-15 1945-07-15 ## 2 16644 2015-07-28 2015-07-28 1945-07-28 ## 3 16581 2015-05-26 2015-05-26 1945-05-26 ## 4 16578 2015-05-23 2015-05-23 1945-05-23 ## 5 16620 2015-07-04 2015-07-04 1945-07-04 ## 6 16652 2015-08-05 2015-08-05 1945-08-05 ## 7 16578 2015-05-23 2015-05-23 1945-05-23 ## 8 16620 2015-07-04 2015-07-04 1945-07-04 ## 9 16652 2015-08-05 2015-08-05 1945-08-05 Dates can also appear in a completely different format. Julian (also known as mainframe or ordinal) dates are an example, where the year is specified but the date is relative to January 1. So 212 days is either July 30 or 31, depending if it is a leap year or not. lubridate::as_date(&#39;19212&#39;, format = &quot;%y%j&quot;) ## [1] &quot;2019-07-31&quot; lubridate::as_date(&#39;20212&#39;, format = &quot;%y%j&quot;) ## [1] &quot;2020-07-30&quot; JDE E1’s dates also add an extra digit in front of it, to symbolize the century. While there is no R way to translate 0 to 19xx and 1 to 20xx, practically speaking we are safe with assuming the century as we are at least 20 years past that point (as of this writing). Amending the mask to ignore the 1 can be done in the format: lubridate::as_date(&#39;119212&#39;, format = &quot;1%y%j&quot;) ## [1] &quot;2019-07-31&quot; lubridate::as_date(&#39;120212&#39;, format = &quot;1%y%j&quot;) ## [1] &quot;2020-07-30&quot; 7.3 Separate Occasionally, distinct data will be combined in the same cell. This can be due to data entry or an unusual method of storing information. There will generally a separator within the field (similar to a delimiter), but the data is still contained within one field. For example, if we look at our vendor table and name column, we can see the company names appear to also have a Contact person. vendors %&gt;% select(type, name) ## # A tibble: 9 x 2 ## type name ## &lt;chr&gt; &lt;chr&gt; ## 1 E al-Bangura, Habeeba ## 2 E Akina, Duncan ## 3 E el-Kazmi, Muneefa ## 4 V el-Shafi Company Ltd - Contact: Rivera, Evan ## 5 V Bray Company Ltd - Contact: el-Jaffer, Shaheer ## 6 V Chrisman Company Ltd - Contact: Burch, Kryslyn ## 7 V el-Shafi Company Ltd - Contact: Rivera, Evan ## 8 V Bray Company Ltd - Contact: el-Jaffer, Shaheer ## 9 V Chrisman Company Ltd - Contact: Burch, Kryslyn While the name is accurate, the contact name shouldn’t be in there. We can isolate the contact name by using separate(), allowing us to split a column by defining a separator and also new column names. In this case, the ‘-’ (dash) character could work: vendors %&gt;% select(type, name) %&gt;% separate(name, c(&#39;name&#39;, &#39;company_contact&#39;), sep = &#39;-&#39;) ## Warning: Expected 2 pieces. Additional pieces discarded in 4 rows [4, 5, 7, 8]. ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [2]. ## # A tibble: 9 x 3 ## type name company_contact ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 E &quot;al&quot; &quot;Bangura, Habeeba&quot; ## 2 E &quot;Akina, Duncan&quot; &lt;NA&gt; ## 3 E &quot;el&quot; &quot;Kazmi, Muneefa&quot; ## 4 V &quot;el&quot; &quot;Shafi Company Ltd &quot; ## 5 V &quot;Bray Company Ltd &quot; &quot; Contact: el&quot; ## 6 V &quot;Chrisman Company Ltd &quot; &quot; Contact: Burch, Kryslyn&quot; ## 7 V &quot;el&quot; &quot;Shafi Company Ltd &quot; ## 8 V &quot;Bray Company Ltd &quot; &quot; Contact: el&quot; ## 9 V &quot;Chrisman Company Ltd &quot; &quot; Contact: Burch, Kryslyn&quot; Sometimes a character isn’t specific enough. We can actually customize the separator even further by changing its argument to be more specific. In this case, perhaps we want both the dash and the word “Contact:”: vendors %&gt;% select(id, type, name) %&gt;% separate(name, c(&#39;name&#39;, &#39;company_contact&#39;), sep = &#39;- Contact:&#39;) ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 3 rows [1, 2, 3]. ## # A tibble: 9 x 4 ## id type name company_contact ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 793 E &quot;al-Bangura, Habeeba&quot; &lt;NA&gt; ## 2 666 E &quot;Akina, Duncan&quot; &lt;NA&gt; ## 3 77 E &quot;el-Kazmi, Muneefa&quot; &lt;NA&gt; ## 4 2726 V &quot;el-Shafi Company Ltd &quot; &quot; Rivera, Evan&quot; ## 5 2612 V &quot;Bray Company Ltd &quot; &quot; el-Jaffer, Shaheer&quot; ## 6 2211 V &quot;Chrisman Company Ltd &quot; &quot; Burch, Kryslyn&quot; ## 7 2726 V &quot;el-Shafi Company Ltd &quot; &quot; Rivera, Evan&quot; ## 8 2612 V &quot;Bray Company Ltd &quot; &quot; el-Jaffer, Shaheer&quot; ## 9 2211 V &quot;Chrisman Company Ltd &quot; &quot; Burch, Kryslyn&quot; What is nice about this functionality is that if the separator isn’t found, the missing values are NA. The warning is good to see (as the employees don’t have a company contact), and can be safely ignored in this case. 7.4 Separate_rows Similarly grouped data can also be combined in the same cell. This is typically representing a one-to-many (or many-to-one) relationship. This can be due to data entry or unusual output formats. Consider our industry table, which is an industry-to-company mapping table, telling us which companies and the respective industry they work in: industry ## id industry ## 1 2726 Health Care ## 2 2612,2211 Commerical Supplies As of now, there would be no way to join the industry, and specifically the value Commercial Supplies, into our vendor table. The unique key is id number of the company, which gets joined to the vendor table. When we run into a situation where a value may belong to the same key multiple times (multiple IDs are associated with one industry), we can use separate_rows() to break up that row into several rows. For this function, we need to specify the columns that need to be separated - in this case, id needs to be broken apart. clean_industry &lt;- industry %&gt;% separate_rows(id, convert = TRUE) # Convert will try to guess the data type for each field, based on the content that was separated print(clean_industry) ## # A tibble: 3 x 2 ## id industry ## &lt;int&gt; &lt;chr&gt; ## 1 2726 Health Care ## 2 2612 Commerical Supplies ## 3 2211 Commerical Supplies separate_rows() will automatically expand the column that needs to be separated, while replicating the content for that same value into subsequent rows. 7.5 Tidy data The three key concepts in ‘tidy data’ (G. G. Hadley Wickham 2017) is that: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. While you can get away with performing basic analysis and checks, having the ability to move data between formats increases competency for cleaning, flexibility for plotting, and legibility for outputting tables in a readable format. Lets start by cleaning the vendor table clean_vendors &lt;- vendors %&gt;% separate(name, c(&#39;name&#39;, &#39;company_contact&#39;), sep = &#39;- Contact: &#39;) %&gt;% # From the separate() section select(-date_added_char) %&gt;% # Remove one of the date fields mutate(date_added = as_date(date_added, origin = &#39;1970-01-01&#39;)) %&gt;% # From the cleaning dates section filter(type == &#39;V&#39;) ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 3 rows [1, 2, 3]. print(clean_vendors) ## # A tibble: 6 x 10 ## id type name company_contact date_added summary `2015` `2016` `2017` ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2726 V &quot;el-… Rivera, Evan 2015-05-23 spend 65326. 8.34e4 1.50e5 ## 2 2612 V &quot;Bra… el-Jaffer, Sha… 2015-07-04 spend 88270. 1.49e5 2.54e5 ## 3 2211 V &quot;Chr… Burch, Kryslyn 2015-08-05 spend 51847. 8.93e4 1.42e5 ## 4 2726 V &quot;el-… Rivera, Evan 2015-05-23 credits 509. 8.60e2 9.09e2 ## 5 2612 V &quot;Bra… el-Jaffer, Sha… 2015-07-04 credits 777. 1.09e3 1.85e3 ## 6 2211 V &quot;Chr… Burch, Kryslyn 2015-08-05 credits 221. 3.69e2 6.57e2 ## # … with 1 more variable: `2018` &lt;dbl&gt; This vendor data set is a combination of both long and wide, and needs to be standardized to fit the tidy data model. Specifically: Each variable must have its own column - The vendor’s spend and credits each year is separated over multiple columns. That is, the values of the variable (the year) is in the column name. In the end output, every combination of vendor-year should be a row. Each observation must have its own row - Each vendor has both a spend and credits value, within the summary column. Even though these are one observation for each vendor-year, they are separated by two rows. Each value must have its own cell - Thankfully, this seems to be valid (although if not, you can clean it with separate() and separate_rows()). Why would you want to subscribe to this tidy data philosophy? When you have a tidy data set, you are more empowered to explore your data (especially with tidyverse related commands), but you can also have the power to reshape data as necessary to fit the visualizing output you are imagining. 7.6 Pivots, wide and longer Effectively pivoting your data into more suitable columns and rows requires you to consider the following: What columns contains the values you want to aggregate? What do you want the new name of these columns to be? Consider both the name and aggregated value. In the clean_vendors data, we see each year represented as columns, and those columns represent dollar amounts. clean_vendors ## # A tibble: 6 x 10 ## id type name company_contact date_added summary `2015` `2016` `2017` ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2726 V &quot;el-… Rivera, Evan 2015-05-23 spend 65326. 8.34e4 1.50e5 ## 2 2612 V &quot;Bra… el-Jaffer, Sha… 2015-07-04 spend 88270. 1.49e5 2.54e5 ## 3 2211 V &quot;Chr… Burch, Kryslyn 2015-08-05 spend 51847. 8.93e4 1.42e5 ## 4 2726 V &quot;el-… Rivera, Evan 2015-05-23 credits 509. 8.60e2 9.09e2 ## 5 2612 V &quot;Bra… el-Jaffer, Sha… 2015-07-04 credits 777. 1.09e3 1.85e3 ## 6 2211 V &quot;Chr… Burch, Kryslyn 2015-08-05 credits 221. 3.69e2 6.57e2 ## # … with 1 more variable: `2018` &lt;dbl&gt; We can pivot this data by specifying the years, and then aggregating the amount. In this case, we want to take the columns and assign them to rows, and making the data longer. pivot_longer() will allow us to make this transformation: clean_vendors %&gt;% pivot_longer(cols = c(`2015`, `2016`, `2017`, `2018`), # The columns we want to pivot, in this case, the years. names_to = &#39;year&#39;, # We want this collected column to be called the year values_to = &#39;amount&#39;) # the values of the aggregation ## # A tibble: 24 x 8 ## id type name company_contact date_added summary year amount ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2726 V &quot;el-Shafi Comp… Rivera, Evan 2015-05-23 spend 2015 6.53e4 ## 2 2726 V &quot;el-Shafi Comp… Rivera, Evan 2015-05-23 spend 2016 8.34e4 ## 3 2726 V &quot;el-Shafi Comp… Rivera, Evan 2015-05-23 spend 2017 1.50e5 ## 4 2726 V &quot;el-Shafi Comp… Rivera, Evan 2015-05-23 spend 2018 5.56e4 ## 5 2612 V &quot;Bray Company … el-Jaffer, Shahe… 2015-07-04 spend 2015 8.83e4 ## 6 2612 V &quot;Bray Company … el-Jaffer, Shahe… 2015-07-04 spend 2016 1.49e5 ## 7 2612 V &quot;Bray Company … el-Jaffer, Shahe… 2015-07-04 spend 2017 2.54e5 ## 8 2612 V &quot;Bray Company … el-Jaffer, Shahe… 2015-07-04 spend 2018 1.79e5 ## 9 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 spend 2015 5.18e4 ## 10 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 spend 2016 8.93e4 ## # … with 14 more rows This data is more readable - each year is on its own row. However, we still have the issue where the same company will have two rows for each year - for the summary field, both a spend and credits exist for a vendor: clean_vendors %&gt;% pivot_longer(cols = c(`2015`, `2016`, `2017`, `2018`), # The columns we want to pivot, in this case, the years. names_to = &#39;year&#39;, # We want this collected column to be called the year values_to = &#39;amount&#39;) %&gt;% # the values of the aggregation arrange(id) %&gt;% head(10) ## # A tibble: 10 x 8 ## id type name company_contact date_added summary year amount ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 spend 2015 5.18e4 ## 2 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 spend 2016 8.93e4 ## 3 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 spend 2017 1.42e5 ## 4 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 spend 2018 7.68e4 ## 5 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 credits 2015 2.21e2 ## 6 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 credits 2016 3.69e2 ## 7 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 credits 2017 6.57e2 ## 8 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 credits 2018 4.58e2 ## 9 2612 V &quot;Bray Company … el-Jaffer, Shahe… 2015-07-04 spend 2015 8.83e4 ## 10 2612 V &quot;Bray Company … el-Jaffer, Shahe… 2015-07-04 spend 2016 1.49e5 Since spend and credits belong to a single vendor-year pairing, it may be worthwhile to to put both summary classifications into their own column. In this case, we want to make the table wider by adding columns, so lets use pivot_wider(). This function is similar to pivot_longer in the same approach: What columns contains the values you want to aggregate spread out? These values becomes the new column name. Where is the value located? These will get moved under the new column. Building from our last example, lets make this data bit wider clean_vendors %&gt;% pivot_longer(cols = c(`2015`, `2016`, `2017`, `2018`), # The columns we want to pivot, in this case, the years. names_to = &#39;year&#39;, # We want this collected column to be called the year values_to = &#39;amount&#39;) %&gt;% # the values of the aggregation pivot_wider(names_from = &#39;summary&#39;, values_from = &#39;amount&#39;) ## # A tibble: 12 x 8 ## id type name company_contact date_added year spend credits ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2726 V &quot;el-Shafi Comp… Rivera, Evan 2015-05-23 2015 6.53e4 509. ## 2 2726 V &quot;el-Shafi Comp… Rivera, Evan 2015-05-23 2016 8.34e4 860. ## 3 2726 V &quot;el-Shafi Comp… Rivera, Evan 2015-05-23 2017 1.50e5 909. ## 4 2726 V &quot;el-Shafi Comp… Rivera, Evan 2015-05-23 2018 5.56e4 348. ## 5 2612 V &quot;Bray Company … el-Jaffer, Shahe… 2015-07-04 2015 8.83e4 777. ## 6 2612 V &quot;Bray Company … el-Jaffer, Shahe… 2015-07-04 2016 1.49e5 1094. ## 7 2612 V &quot;Bray Company … el-Jaffer, Shahe… 2015-07-04 2017 2.54e5 1854. ## 8 2612 V &quot;Bray Company … el-Jaffer, Shahe… 2015-07-04 2018 1.79e5 1395. ## 9 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 2015 5.18e4 221. ## 10 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 2016 8.93e4 369. ## 11 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 2017 1.42e5 657. ## 12 2211 V &quot;Chrisman Comp… Burch, Kryslyn 2015-08-05 2018 7.68e4 458. Each company now has its spend and credits in separate columns, but is now digestible by year. 7.7 Joining data Often our data will come from different data sets or different tables. Having the ability to combine data from different systems is a powerful way to add more analyzable information from your dataset. In the Completeness chapter, we joined accounts from both the GL and TB to calculate whether there were any differences. The Relational Data chapter in R for Data Science goes into depth how the different join operations work. Division is the intended mechanism - see https://github.com/tidyverse/lubridate/issues/105 and https://stackoverflow.com/questions/8765621/length-of-lubridate-interval↩︎ "],
["exploring-data.html", "Chapter 8 Exploring data 8.1 Basic Statistics 8.2 Summarizations 8.3 Data Visualization - Why 8.4 Data Visualization - How", " Chapter 8 Exploring data As for exploring data, there is no quick, one-click method to explore your data, whether at a high level or in more granular detail. Realistically, you will oscillate between cleaning and exploration before you are able to better understand the data you are looking testing. There are many primarily two methods in exploring data, both quantitatively and visually. Both are necessary methods for understanding data, at a quantitative and also a human level. For this section we will still use the same company database, and also another unique dataset, along with some more packages - mainly, ggplot2. library(dplyr) library(tidyr) library(lubridate) library(DBI) library(ggplot2) # For graphics dir.create(&quot;data&quot;, showWarnings = FALSE) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) gl &lt;- tbl(con, &#39;gl&#39;) %&gt;% collect() dbDisconnect(con) 8.1 Basic Statistics Performing summaries, or profiling the data, is a great way to begin analyzing data. It allows you to understand general numeric performance of a group. More importantly, understanding the characteristics of a datset, including mean, median, and percentiles are useful, as they help you determine what is both normal and abnormal in your data. In our GL account, we have a materials category - lets isolate by that and begin our analysis: materials &lt;- gl %&gt;% filter(account == &#39;exp_materials_6000&#39;) %&gt;% mutate(gl_date = as_date(gl_date_char)) %&gt;% # We demonstrated how to handle dates in the Cleaning chapter select(-gl_date_char) summary() gives a high level preview of each field, including the type and the statistical profile of the data table: summary(materials) ## je_num amount gl_date vendor_id ## Min. : 1.0 Min. :-18873 Min. :2019-01-01 Min. :2211 ## account invoice_date description paid_date ## Length:952 Min. :17887 Length:952 Min. :17900 ## [ reached getOption(&quot;max.print&quot;) -- omitted 5 rows ] If we focus on amount, we can reproduce the same summaries as well, useful if we want to extract items relative to the criteria: min(materials$amount) ## [1] -18872.85 max(materials$amount) ## [1] 96354.61 mean(materials$amount) # The average ## [1] 10487.93 median(materials$amount) # The value in the middle between upper half and lower half. If these are different than the average, then you may expect some skew in the data set. ## [1] 6816.1 Quartiles are useful because they help you generally understand where the majority of the data lies. These also also known as quantiles (or percentiles), but they cover the same area of a normal curve. quantile(materials$amount) # Look familiar? ## 0% 25% 50% 75% 100% ## -18872.85 2771.07 6816.10 15050.09 96354.61 quantile(materials$amount)[2] # This is the 1st quartile, or the 25% quantile ## 25% ## 2771.07 quantile(materials$amount)[4] # This is the 3st quartile, or the 75% quantile ## 75% ## 15050.09 IQR(materials$amount) # The difference between 3rd and 1st quartile ## [1] 12279.02 You may have missing values in your data set that show up as NA. Its important to recognize how you will diagnose and treat NAs. If a column has NAs, you may choose to remove them by using the argument na.rm = TRUE, which simply ignores these values when performing the calculation. some_nums &lt;- c(1, 2, NA, 3, 4, 5) mean(some_nums) # Returns NA ## [1] NA mean(some_nums, na.rm = TRUE) # Removes the NA value prior to caluclating the mean ## [1] 3 In accounting, it may be important to know the absolute value of a number. They’re useful in audit situations as they help express total activity of the subject, as positive and negative values may either zero out the total or mask credit activity and reversals. They’re also useful for calculating errors, as you may be not concerned about the direction of the error (debit or credit) but the total magnitude instead. sum(materials$amount) ## [1] 9984507 abs(sum(materials$amount)) ## [1] 9984507 We can even do basic correlation plots in R of two numerical data sets: x &lt;- c(1, 2, 3, 4) y &lt;- c(2, 4, 6, 8) cor(x, y) # The correlation of X and Y. 1 means positively highly correlated, -1 means negatively highly coorelated, and 0 means no discerable pattern. ## [1] 1 8.2 Summarizations While calculating one-off summary statistics is useful, you may want to perform summation or summary statistics, whether by an entity (a vendor) or by time frame (month, year). R provides methods for summarizing this information, with a combination of two verbs. group_by() enables you to indicate the levels you want to aggregate (for example, by vendor or by month), summarize() requires you to create new columns to contain the calculation or aggregation you want to occur (for example, sum or mean). The materials table has a vendor_id number. We can try to perform a group_by()… materials %&gt;% group_by(vendor_id) ## # A tibble: 952 x 8 ## # Groups: vendor_id [3] ## je_num amount gl_date vendor_id account invoice_date description paid_date ## &lt;int&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 22667. 2019-12-02 2211 exp_ma… 18225 Packaging … 18262 ## 2 2 1397. 2019-11-28 2612 exp_ma… 18218 Paper 18251 ## 3 3 2319. 2019-04-09 2612 exp_ma… 17985 Paper 18018 ## 4 4 20037. 2019-10-30 2612 exp_ma… 18189 Paper 18222 ## 5 5 43140. 2019-06-04 2612 exp_ma… 18041 Blue dye 18074 ## 6 6 13603. 2019-01-31 2726 exp_ma… 17920 Medical gr… 17930 ## 7 7 7001. 2019-10-21 2726 exp_ma… 18183 Quality co… 18193 ## 8 8 24753. 2019-04-24 2211 exp_ma… 18003 Packaging … 18040 ## 9 9 2295. 2019-07-10 2726 exp_ma… 18080 Quality co… 18090 ## 10 10 34263. 2019-06-13 2211 exp_ma… 18053 Packaging … 18090 ## # … with 942 more rows But it will return nothing. The group_by() will tell R what to group by, but not perform any additional calculations. Hence, the summarize(): materials %&gt;% group_by(vendor_id) %&gt;% summarize(total_amount = sum(amount)) ## # A tibble: 3 x 2 ## vendor_id total_amount ## * &lt;int&gt; &lt;dbl&gt; ## 1 2211 3650465. ## 2 2612 2951279. ## 3 2726 3382764. Summarize will isolate the data set first by the groups specified, and then perform the calculation and put the results into a new field. This works for all summary statistics as well. For example, if you want to find the number of transactions, and the highest and lowest amount charged per description: materials %&gt;% group_by(description) %&gt;% summarize(number_of_trans = n(), lowest_amount = min(amount), highest_amount = max(amount)) ## # A tibble: 11 x 4 ## description number_of_trans lowest_amount highest_amount ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Blue dye 40 1071. 43140. ## 2 Fabric cutting machines 34 432. 44950. ## 3 Face mask elastics 37 526. 42785. ## 4 Fancy cloth prints 46 294. 37269. ## 5 Meals for launch celebration 1 1630. 1630. ## 6 Medical grade filters 156 258. 57567. ## 7 Packaging and boxes 316 304. 96355. ## 8 Paper 134 284. 47748. ## 9 Partial refund for medical grad… 1 -18873. -18873. ## 10 Quality control testing supplies 169 273. 88524. ## 11 Sewing machines 18 268. 36332. 8.3 Data Visualization - Why With such useful statistical and summary functions, you may wonder why you ever need to delve into graphics. While summary statistics are fun (and perhaps, highly accurate and even ‘authoritative’), they don’t help bring to light full patterns of your data. The power of visual deduction can not be understated, especially in situations where superficially summary statistics have no issues. Consider this data set, which are merely 4 pairs of x-y values (i.e. x1 goes with y1 etc): anscombe ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 10 rows ] If we did statistical testing on these pairs, we will notice they have nearly identical summary statistics: mean(anscombe$x1); mean(anscombe$x2); mean(anscombe$x3); mean(anscombe$x4) ## [1] 9 ## [1] 9 ## [1] 9 ## [1] 9 mean(anscombe$y1); mean(anscombe$y2); mean(anscombe$y3); mean(anscombe$y4) ## [1] 7.500909 ## [1] 7.500909 ## [1] 7.5 ## [1] 7.500909 And even their correlatons are similar: cor(anscombe$x1, anscombe$y1); cor(anscombe$x2, anscombe$y2); cor(anscombe$x3, anscombe$y3); cor(anscombe$x4, anscombe$y4) ## [1] 0.8164205 ## [1] 0.8162365 ## [1] 0.8162867 ## [1] 0.8165214 With the mean and correlation statistics, you may jump to the conclusion that these datasets may be shaped identically. However, we should seriously consider plotting the data to validate our initial assumption that the data is similar. We will use the ggplot2 to chart these x-y pairs and try to understand the data we are looking at. We’ll explain ggplot2 syntax later, so for now, focus on the graph produced. If we produce the four graphs together, what do you notice? ggplot(anscombe, aes(x = x1, y = y1)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se = FALSE) + labs(title=&quot;Pair 1: Y1 over X1&quot;) ggplot(anscombe, aes(x = x2, y = y2)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se = FALSE) + labs(title=&quot;Pair 2: Y2 over X2&quot;) ggplot(anscombe, aes(x = x3, y = y3)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se = FALSE) + labs(title=&quot;Pair 3: Y3 over X3&quot;) ggplot(anscombe, aes(x = x4, y = y4)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se = FALSE) + labs(title=&quot;Pair 4: Y4 over Y4&quot;) These four charts and scatterplots themselves are quite different! While the blue correlation line and the prior summary statistics are similar, the points all follow a different general path. Independently looking at the graphics for each one will give you different insight, and may influence your audit approach. Graphics matter a lot! 8.4 Data Visualization - How ggplot2 is the package that many consider to be one of the differentiating strengths of the R universe. It is one of the most elegant methods of visualizing data, and will help you understand your audit subject. Back to our materials analysis. materials %&gt;% arrange(gl_date) %&gt;% head() ## # A tibble: 6 x 8 ## je_num amount gl_date vendor_id account invoice_date description paid_date ## &lt;int&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 373 13520. 2019-01-01 2612 exp_mat… 17887 Paper 17920 ## 2 951 13486. 2019-01-01 2612 exp_mat… 17887 Paper 17920 ## 3 961 16026. 2019-01-01 2726 exp_mat… 17890 Quality co… 17900 ## 4 90 3777. 2019-01-02 2612 exp_mat… 17888 Face mask … 17921 ## 5 115 13402. 2019-01-02 2211 exp_mat… 17891 Packaging … 17928 ## 6 198 2431. 2019-01-02 2612 exp_mat… 17888 Face mask … 17921 Perhaps we want to start analyzing the change over time for the amount spent. The gl_date is useful here, although there are many transactions that occur on the same date. We also have a vendor_id as well, identifying the vendor that the transaction the invoice belongs to. Generally we will want our time to be the x variable (the dependent variable), and the amount spent to be the y variable (the independent variable). We can insert the x and y into a ggplot function and generate the visualization we think may reveal useful infromation. ggplot(data = materials, mapping = aes(x = gl_date, y = amount)) + geom_point() The foundations of any ggplot are essentially: data - The data frame you want to visualize, mapping - The aesthetics, which dictate the x and y (generally for the rest of the ggplot), and geom_* - The visualization mechanism you want to apply to represent the data points. When you specify a single mapping up front, it will be used for all the subsequent geoms called. You can also specify a mapping for each geom as well, which is useful for isolating points. This foundations layer of data, mapping and geom_* allows us to call different geoms and perspectives of looking at the data. For example, histograms are useful to understand the shape of a dataset: ggplot(materials, aes(x = amount)) + # data is the first default argument, and mapping is the second geom_histogram() And boxplots are useful to understand the distribution of the data: ggplot(data = materials, aes(x = as.factor(vendor_id), y = amount)) + geom_boxplot() 8.4.1 Time-series As the gl table (and materials subset) is a time-series data, containing both the transaction date and amount, we may want to try plotting a line chart to see the movement of data over time. ggplot(data = materials, aes(x = gl_date, y = amount)) + geom_line() Since our time-series data has a relatively high resolution with detailed data, we may sometimes gain insight by aggregating data (by week, month, or even year). As of now, the data needs to be aggregated into a higher level time period. monthly_mats &lt;- materials %&gt;% mutate(gl_month = floor_date(gl_date, &#39;month&#39;)) %&gt;% # Rounds the date down to the first of the month group_by(gl_month) %&gt;% summarize(amount = sum(amount)) head(monthly_mats) ## # A tibble: 6 x 2 ## gl_month amount ## &lt;date&gt; &lt;dbl&gt; ## 1 2019-01-01 1021508. ## 2 2019-02-01 834766. ## 3 2019-03-01 746621. ## 4 2019-04-01 636011. ## 5 2019-05-01 953909. ## 6 2019-06-01 951240. Aggregating this same information by month may be more illustrative: ggplot(data = monthly_mats, aes(x = gl_month, y = amount)) + geom_line() 8.4.2 Facets Another powerful feature of ggplot2 is facets - that is, the ability to show how different categories of the same data may look in the same dataset. In our GL dataset, we actually had multiple types of expenses: unique(gl$account) ## [1] &quot;exp_materials_6000&quot; &quot;liab_accountspayable_2000&quot; ## [3] &quot;exp_meals_7000&quot; &quot;liab_creditcardpayable_2100&quot; ## [5] &quot;exp_consulting_6500&quot; We can pull these specific expense accounts (starting with ‘exp_’), and can choose how we want to plot them: expense &lt;- gl %&gt;% filter(account %in% c(&#39;exp_materials_6000&#39;, &#39;exp_consulting_6500&#39;, &#39;exp_meals_7000&#39;)) %&gt;% mutate(gl_date = as_date(gl_date_char), gl_month = floor_date(gl_date, &#39;month&#39;)) %&gt;% group_by(account, gl_month) %&gt;% # Need to add a new layer of grouping to preserve the account summarize(amount = sum(amount)) ## `summarise()` has grouped output by &#39;account&#39;. You can override using the `.groups` argument. ggplot(data = expense, aes(x = gl_month, y = amount, colour = account)) + # We can specify the colour now too, depending on the geom_* geom_line() Instead of plotting them onto the same chart, we can facet_* them: ggplot(data = expense, aes(x = gl_month, y = amount, colour = account)) + geom_line() + facet_grid(. ~ account) # Syntax for variable, either in the (row ~ column)... or both! "],
["test.html", "Chapter 9 Test 9.1 Amount-based tests 9.2 Date-based tests 9.3 Basic Statistical tests 9.4 Unusual pattern and behaviours 9.5 Search text", " Chapter 9 Test Testing data is the act of isolating high-risk records that meet criteria. While generally the intent of data analytics within auditing is to gain 100% population coverage and associated assurance, what will inevitably happen is that process deviations are revealed as items are detected. No process is bullet-proof and exception free, and therefore the auditor should have a founded idea of their level of risk tolerance prior to testing. Testing data will then help auditors articulate the nature of the exceptions and the overall risk level. Auditors must recognize that not all tests (and lines that meet these tests) immediately ascertain that the control has failed or that something suspicious has occurred. Rather, it points to items that require further inspection and judgment. By iterating your workflow, you will be able to reduce the incidence of false positives and increase your detection rate, allowing you to focus on follow-up of higher-risk activity. For this section we will use the company GL database. library(dplyr) library(tidyr) library(lubridate) library(DBI) library(ggplot2) library(gt) # Used for formatting outputs library(pryr) # Used to analyze encoding in text chapter library(stringi) # specialized string processing dir.create(&quot;data&quot;, showWarnings = FALSE) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) gl &lt;- tbl(con, &#39;gl&#39;) %&gt;% collect() %&gt;% mutate(gl_date = as_date(gl_date, origin = &#39;1970-01-01&#39;), paid_date = as_date(paid_date, origin = &#39;1970-01-01&#39;), invoice_date = as_date(invoice_date, origin = &#39;1970-01-01&#39;)) %&gt;% select(-gl_date_char) dbDisconnect(con) Our primary commands for this chapter will be mutate() and filter(). The mutate() command is useful for testing data, as it allows the auditor to add features and/or criteria to their testing. After records have been identified, filter() will extract the rows matching criteria. While you may able to directly filter() rows based on criteria, you may find it more effective to instead create the new fields with mutate() that match the filter criteria. My style for documenting tests is to create any additional features needed, and then create a corresponding test column, with t_ as the prefix, and isolate for these invoices to perform inspection and follow-up. This has benefits in the long run, including: Keeping all the tests on the same record allows for an easier way to detect any row that met multiple criteria, It is straightforward to extract the rows that meets a test, and Each new column related to a specific a criteria is akin to feature engineering, enabling the reuse of the tests for machine learning applications. There are practical reasons why you may need to create separate data frame objects, so do not prescribe exclusively to this method for extremely long or sophisticated steps. In cases when you have to perform multiple steps to get to your answer, it is advisable to bring the final result back into an overall reference table. 9.1 Amount-based tests 9.1.1 Above threshold The most classical test is to test items that meet or exceed a material value. gl %&gt;% mutate(t_over_te = amount &gt;= 60000) %&gt;% filter(t_over_te) %&gt;% select(je_num, amount, everything()) ## # A tibble: 2 x 9 ## je_num amount gl_date vendor_id account invoice_date description paid_date ## &lt;int&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;date&gt; ## 1 140 96355. 2019-12-19 2211 exp_ma… 2019-12-12 Packaging … 2020-01-18 ## 2 693 88524. 2019-08-13 2726 exp_ma… 2019-08-06 Quality co… 2019-08-16 ## # … with 1 more variable: t_over_te &lt;lgl&gt; When performing amount or threshold testing, you may be more interested in the actual magnitude, and not necessarily the direction. This is especially true within accounting matters, where the implication of negative and positive amounts change depending on the transaction - i.e., a debit or a credit transaction, or affecting a balance sheet or income statement account can tell a very different story. In these cases, you may want to consider using absolute values as the comparison. This allows you to capture the magnitude of the change: gl %&gt;% mutate(t_over_te = abs(amount) &gt;= 60000) %&gt;% filter(t_over_te) ## # A tibble: 4 x 9 ## je_num amount gl_date vendor_id account invoice_date description ## &lt;int&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 140 96355. 2019-12-19 2211 exp_ma… 2019-12-12 Packaging … ## 2 140 -96355. 2019-12-19 NA liab_a… 2019-12-12 Packaging … ## 3 693 88524. 2019-08-13 2726 exp_ma… 2019-08-06 Quality co… ## 4 693 -88524. 2019-08-13 NA liab_a… 2019-08-06 Quality co… ## # … with 2 more variables: paid_date &lt;date&gt;, t_over_te &lt;lgl&gt; 9.1.2 Round numbers In forensic accounting, auditors tend to take the position that transactions rarely end in whole dollars, especially if they end in multiples of 5, or even in multiples of tens. Detection of these is contingent on the use of %%, better known as the modulo operator. It will calculate the remainder of a division calculation; dividing 1.50 by 1 will give you a remainder of 0.5. 1.50 %% 1 ## [1] 0.5 This same approach can be used to detect if a transaction is round, i.e. has no ‘cents’ in it. How you decide to apply this can be useful in many cases. For example, finding a transaction that happens to be cleanly in the ‘thousands,’ perform the modulo by the corresponding amount and look for results where the remainder is 0. 15000 %% 1000 ## [1] 0 Or even a round five number, which has its use cases when it comes to things like gift cards or tipping at restaurants: 25 %% 5 ## [1] 0 When implementing as a test, I’d encourage creating multiple tests to see which test it matched, and then detect these variations together: gl %&gt;% mutate(t_round_5 = amount %% 5 == 0, t_round_10 = amount %% 10 == 0, t_round_100 = amount %% 100 == 0, t_round_1000 = amount %% 1000 == 0) %&gt;% filter_at(vars(starts_with(&quot;t_round&quot;)), any_vars(.)) %&gt;% select(je_num, amount, starts_with(&quot;t_round&quot;)) ## # A tibble: 2 x 6 ## je_num amount t_round_5 t_round_10 t_round_100 t_round_1000 ## &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 621 12800 TRUE TRUE TRUE FALSE ## 2 621 -12800 TRUE TRUE TRUE FALSE 9.2 Date-based tests 9.2.1 Weekend testing As a typical office business (along with office activities) generally happen on the weekday - that is, Monday through Friday. We may be interested in selecting journal entries from a sample of activities that occurred on the weekend. To test this, we can use the lubridate package to determine what day of week our entry falls upon: gl &lt;- gl %&gt;% mutate(day_of_week = wday(gl_date, label = TRUE), # Label is useful if you tend to forget what each number means t_weekend = day_of_week %in% c(&#39;Sat&#39;, &#39;Sun&#39;)) gl %&gt;% filter(t_weekend) %&gt;% head() ## # A tibble: 6 x 10 ## je_num amount gl_date vendor_id account invoice_date description ## &lt;int&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 54 35758. 2019-01-12 2612 exp_ma… 2019-01-02 Face mask … ## 2 54 -35758. 2019-01-12 NA liab_a… 2019-01-02 Face mask … ## 3 102 50135. 2019-01-12 2726 exp_ma… 2019-01-05 Medical gr… ## 4 102 -50135. 2019-01-12 NA liab_a… 2019-01-05 Medical gr… ## 5 223 50128. 2019-01-12 2211 exp_ma… 2019-01-05 Packaging … ## 6 223 -50128. 2019-01-12 NA liab_a… 2019-01-05 Packaging … ## # … with 3 more variables: paid_date &lt;date&gt;, day_of_week &lt;ord&gt;, t_weekend &lt;lgl&gt; 9.2.2 Cutoff testing When transactions are near the end of the month (or the end of the year), cutoff testing helps establish that the transactions were recorded within the correct time period. Testing these may require selecting invoices close to month end. Similar to how we calculate the period using floor_date(), we can take any date and transform it with ceiling_date() which will calculate the last date of the month. Then we can test to see if a date falls within the range as needed. gl %&gt;% mutate(end_of_month = ceiling_date(gl_date, unit = &#39;month&#39;)) %&gt;% select(gl_date, end_of_month) %&gt;% head() ## # A tibble: 6 x 2 ## gl_date end_of_month ## &lt;date&gt; &lt;date&gt; ## 1 2019-12-02 2020-01-01 ## 2 2019-12-02 2020-01-01 ## 3 2019-11-28 2019-12-01 ## 4 2019-11-28 2019-12-01 ## 5 2019-04-09 2019-05-01 ## 6 2019-04-09 2019-05-01 This is close to our desired result, but all these dates are representative of the beginning of the following month. To transform dates, simply use the unit as desired and treat it like a math calculation. As all these dates are one day ahead, a days difference calculation will suffice: gl &lt;- gl %&gt;% mutate(end_of_month = ceiling_date(gl_date, unit = &#39;month&#39;) - days(1)) gl %&gt;% select(gl_date, end_of_month) %&gt;% head() ## # A tibble: 6 x 2 ## gl_date end_of_month ## &lt;date&gt; &lt;date&gt; ## 1 2019-12-02 2019-12-31 ## 2 2019-12-02 2019-12-31 ## 3 2019-11-28 2019-11-30 ## 4 2019-11-28 2019-11-30 ## 5 2019-04-09 2019-04-30 ## 6 2019-04-09 2019-04-30 And to detect which lines are close to month end, simply calculate the difference between the end of month and the original date, keeping only dates very close to the end of the month: gl %&gt;% mutate(t_cutoff = (end_of_month - gl_date) &lt;= 1) %&gt;% # Find entries on or one day prior to month end filter(t_cutoff) %&gt;% select(je_num, gl_date, t_cutoff, everything()) ## # A tibble: 136 x 12 ## je_num gl_date t_cutoff amount vendor_id account invoice_date description ## &lt;int&gt; &lt;date&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 4 2019-10-30 TRUE 20037. 2612 exp_ma… 2019-10-20 Paper ## 2 4 2019-10-30 TRUE -20037. NA liab_a… 2019-10-20 Paper ## 3 6 2019-01-31 TRUE 13603. 2726 exp_ma… 2019-01-24 Medical gr… ## 4 6 2019-01-31 TRUE -13603. NA liab_a… 2019-01-24 Medical gr… ## 5 11 2019-07-30 TRUE 3000. 2612 exp_ma… 2019-07-20 Paper ## 6 11 2019-07-30 TRUE -3000. NA liab_a… 2019-07-20 Paper ## 7 31 2019-04-29 TRUE 9931. 2211 exp_co… 2019-04-14 Advisory s… ## 8 31 2019-04-29 TRUE -9931. NA liab_a… 2019-04-14 Advisory s… ## 9 44 2019-02-28 TRUE 3838. 2211 exp_ma… 2019-02-21 Packaging … ## 10 44 2019-02-28 TRUE -3838. NA liab_a… 2019-02-21 Packaging … ## # … with 126 more rows, and 4 more variables: paid_date &lt;date&gt;, ## # day_of_week &lt;ord&gt;, t_weekend &lt;lgl&gt;, end_of_month &lt;date&gt; 9.2.3 Age Aging invoices is generally a system-ran report out of the accounting system. Independently testing these reports is one of the strongest forms of assurance, which requires an understanding of the underlying data, including calculation accuracy and completeness. At its core, an aging calculation is the comparison of one date to another certain date, and then aggregating the total amount by buckets indicating a range of days. However, it gets complicated rather quick because of how different systems implement aging: Which dates are being compared - invoice dates, entry dates, paid dates and/or due dates? Speaking of due dates, are you able to calculate a due date on a per-vendor (or even, per invoice) basis? Are you able to run a report ‘as-of?’ The as-of date is intended to help understand the outstanding balances at a specific point of time. Some accounting systems have insufficient information captured, meaning a backdated entered invoice (perhaps to a prior period) may affect the accuracy of a previous report. We will initially filter our data and only keep valid invoices, by filtering on records that are related to materials: gl %&gt;% filter(account == &#39;exp_materials_6000&#39;) %&gt;% # In scope invoices for aging select(je_num, vendor_id, gl_date, invoice_date, paid_date) ## # A tibble: 952 x 5 ## je_num vendor_id gl_date invoice_date paid_date ## &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; ## 1 1 2211 2019-12-02 2019-11-25 2020-01-01 ## 2 2 2612 2019-11-28 2019-11-18 2019-12-21 ## 3 3 2612 2019-04-09 2019-03-30 2019-05-02 ## 4 4 2612 2019-10-30 2019-10-20 2019-11-22 ## 5 5 2612 2019-06-04 2019-05-25 2019-06-27 ## 6 6 2726 2019-01-31 2019-01-24 2019-02-03 ## 7 7 2726 2019-10-21 2019-10-14 2019-10-24 ## 8 8 2211 2019-04-24 2019-04-17 2019-05-24 ## 9 9 2726 2019-07-10 2019-07-03 2019-07-13 ## 10 10 2211 2019-06-13 2019-06-06 2019-07-13 ## # … with 942 more rows In our GL data set, we have captured invoices (denoted as transactions against expense accounts), as well as the GL date (the day the company received the invoice), the invoice date as written by the supplier, and the date the invoice was paid. With a better understanding of the dates, we can now focus on reproducing an accurate report: An invoice was recognized in our system as per the gl_date, An invoice was considered fully paid as of the paid_date, and The definition of age in our system is based off the invoice_date and an user-chosen ‘as-of’ date. We will use August 1, 2019 as our as-of date, and only include invoices that were recognized on or prior to then. as_of_date &lt;- as.Date(&#39;2019-08-01&#39;) gl %&gt;% filter(account == &#39;exp_materials_6000&#39;) %&gt;% # In scope invoices for aging select(je_num, vendor_id, gl_date, invoice_date, paid_date) %&gt;% filter(gl_date &lt;= as_of_date) # Only choose recognized invoices as of a date ## # A tibble: 571 x 5 ## je_num vendor_id gl_date invoice_date paid_date ## &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; ## 1 3 2612 2019-04-09 2019-03-30 2019-05-02 ## 2 5 2612 2019-06-04 2019-05-25 2019-06-27 ## 3 6 2726 2019-01-31 2019-01-24 2019-02-03 ## 4 8 2211 2019-04-24 2019-04-17 2019-05-24 ## 5 9 2726 2019-07-10 2019-07-03 2019-07-13 ## 6 10 2211 2019-06-13 2019-06-06 2019-07-13 ## 7 11 2612 2019-07-30 2019-07-20 2019-08-22 ## 8 12 2726 2019-06-19 2019-06-12 2019-06-22 ## 9 16 2211 2019-01-18 2019-01-11 2019-02-17 ## 10 17 2612 2019-02-05 2019-01-26 2019-02-28 ## # … with 561 more rows We can also remove any paid invoices too, as they’re no longer aged as-of that date. Depending when you are running your report, an invoice may could either be paid or not paid yet. gl %&gt;% filter(account == &#39;exp_materials_6000&#39;) %&gt;% # In scope invoices for aging select(je_num, vendor_id, gl_date, invoice_date, paid_date) %&gt;% filter(gl_date &lt;= as_of_date) %&gt;% # Only choose recognized invoices as of a date filter(paid_date &gt;= as_of_date | is.na(paid_date)) # Keep invoices that were not paid by the as-of date, or not paid at all yet ## # A tibble: 71 x 5 ## je_num vendor_id gl_date invoice_date paid_date ## &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; ## 1 11 2612 2019-07-30 2019-07-20 2019-08-22 ## 2 18 2612 2019-07-23 2019-07-13 2019-08-15 ## 3 20 2211 2019-07-23 2019-07-16 2019-08-22 ## 4 22 2612 2019-07-18 2019-07-08 2019-08-10 ## 5 38 2211 2019-07-25 2019-07-18 2019-08-24 ## 6 57 2211 2019-07-12 2019-07-05 2019-08-11 ## 7 75 2211 2019-07-08 2019-07-01 2019-08-07 ## 8 85 2612 2019-07-15 2019-07-05 2019-08-07 ## 9 88 2612 2019-07-31 2019-07-21 2019-08-23 ## 10 97 2612 2019-07-30 2019-07-20 2019-08-22 ## # … with 61 more rows Having isolated our invoices, we can now calculate the number of aged days: gl %&gt;% filter(account == &#39;exp_materials_6000&#39;) %&gt;% # In scope invoices for aging filter(gl_date &lt;= as_of_date) %&gt;% # Only choose recognized invoices as of a date filter(paid_date &gt;= as_of_date | is.na(paid_date)) %&gt;% # Keep invoices that are not paid by the as-of date, or not paid at all yet mutate(aged_days = as.numeric(as_of_date - invoice_date)) %&gt;% select(invoice_date, paid_date, aged_days) ## # A tibble: 71 x 3 ## invoice_date paid_date aged_days ## &lt;date&gt; &lt;date&gt; &lt;dbl&gt; ## 1 2019-07-20 2019-08-22 12 ## 2 2019-07-13 2019-08-15 19 ## 3 2019-07-16 2019-08-22 16 ## 4 2019-07-08 2019-08-10 24 ## 5 2019-07-18 2019-08-24 14 ## 6 2019-07-05 2019-08-11 27 ## 7 2019-07-01 2019-08-07 31 ## 8 2019-07-05 2019-08-07 27 ## 9 2019-07-21 2019-08-23 11 ## 10 2019-07-20 2019-08-22 12 ## # … with 61 more rows Now the most complex part is making buckets for these ages. A rather brute force way is to create a field with the range it falls in, and then eventually pivoting on it: aged_wip &lt;- gl %&gt;% filter(account == &#39;exp_materials_6000&#39;) %&gt;% # In scope invoices for aging select(je_num, account, vendor_id, gl_date, invoice_date, paid_date) %&gt;% filter(gl_date &lt;= as_of_date) %&gt;% # Only choose recognized invoices as of a date filter(paid_date &gt;= as_of_date | is.na(paid_date)) %&gt;% # Keep invoices that are not paid by the as-of date, or not paid at all yet mutate(aged_days = as.numeric(as_of_date - invoice_date)) %&gt;% select(aged_days) aged_wip %&gt;% mutate(aging_bucket = case_when( aged_days &lt; 15 ~ &quot;Current&quot;, aged_days &lt; 30 ~ &quot;15 - 30&quot;, aged_days &lt; 90 ~ &quot;30 - 59&quot;, TRUE ~ &quot;60+&quot; )) ## # A tibble: 71 x 2 ## aged_days aging_bucket ## &lt;dbl&gt; &lt;chr&gt; ## 1 12 Current ## 2 19 15 - 30 ## 3 16 15 - 30 ## 4 24 15 - 30 ## 5 14 Current ## 6 27 15 - 30 ## 7 31 30 - 59 ## 8 27 15 - 30 ## 9 11 Current ## 10 12 Current ## # … with 61 more rows Another useful method is to use the cut() function, which will take the intervals you specify (aged days) and place each row into the category. The labels look a bit different - inclusive boundaries are ( while exclusive boundaries are ]: aged_wip %&gt;% mutate(aging_bucket = cut(aged_wip$aged_days, breaks = c(0, 15, 30, 60, Inf))) ## # A tibble: 71 x 2 ## aged_days aging_bucket ## &lt;dbl&gt; &lt;fct&gt; ## 1 12 (0,15] ## 2 19 (15,30] ## 3 16 (15,30] ## 4 24 (15,30] ## 5 14 (0,15] ## 6 27 (15,30] ## 7 31 (30,60] ## 8 27 (15,30] ## 9 11 (0,15] ## 10 12 (0,15] ## # … with 61 more rows You can choose which approach best fits your style. With all the pieces, we can now create the aging report, aggregated by vendor. First, lets only keep the variables we want to pivot: aged &lt;- gl %&gt;% filter(account == &#39;exp_materials_6000&#39;) %&gt;% # In scope invoices for aging filter(gl_date &lt;= as_of_date) %&gt;% # Only choose recognized invoices as of a date filter(paid_date &gt;= as_of_date | is.na(paid_date)) %&gt;% # Keep invoices that are not paid by the as-of date, or not paid at all yet mutate(aged_days = as.numeric(as_of_date - invoice_date)) %&gt;% mutate(aging_bucket = case_when( aged_days &lt; 15 ~ &quot;Current&quot;, aged_days &lt; 30 ~ &quot;15 - 30&quot;, aged_days &lt; 90 ~ &quot;30 - 59&quot;, TRUE ~ &quot;60+&quot; )) %&gt;% select(vendor_id, amount, aging_bucket) print(aged) ## # A tibble: 71 x 3 ## vendor_id amount aging_bucket ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2612 3000. Current ## 2 2612 8436. 15 - 30 ## 3 2211 725. 15 - 30 ## 4 2612 5037. 15 - 30 ## 5 2211 7412. Current ## 6 2211 11461. 15 - 30 ## 7 2211 2206. 30 - 59 ## 8 2612 2491. 15 - 30 ## 9 2612 294. Current ## 10 2612 13593. Current ## # … with 61 more rows Since our typical aging report has vendors along the left (the unique identifier for id_cols argument), aging buckets along the top (the names we want to spread across with names_from) and the amount is a summation of the valid values (values_from and also values_fn), we can use pivot_wider() to recreate our report: aged %&gt;% pivot_wider(id_cols = vendor_id, names_from = aging_bucket, values_from = amount, values_fn = sum) ## # A tibble: 3 x 4 ## vendor_id Current `15 - 30` `30 - 59` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2612 58163. 57468. 41684. ## 2 2211 120974. 114429. 136315. ## 3 2726 105586. NA NA 9.3 Basic Statistical tests In the exploration of data, we highlighted that exploring your data using various statistical functions are useful for helping understand the shape of your data. After doing so, you can then apply statistical methods directly in your testing to help select samples. 9.3.1 Outliers based on amount An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal. Before abnormal observations can be singled out, it is necessary to characterize normal observations. (NIST/SEMATECH e-Handbook of Statistical Methods, n.d.) Our general approach to using statistical tests is to bring attention to “abnormal” entries in our data. Abnormal may be different depending on the situation, but items that stand apart from the rest are of audit interest. Outliers are very specific to the dataset we are analyzing, and are highly dependent on the nature and structure of the work we have. The most basic detection of extreme outliers can be done on an amount, looking for material transactions, by using the quantiles and IQRs. Quantiles indicate the thresholds where amounts may be in the bottom 25% or above the top 75% of data points. The IQR is defined as the difference between the upper 75% quantile and lower 25% quantile (i.e. the 3rd quartile and 1st quartile). The extreme outlier test is defined as points less than the 25% quantile minus 1.5 * inter quartile range (IQR), or above the 75% quantile plus 1.5 * IQR. In this case, we want to detect items greater than the extreme upper outlier: # Detect amounts in the top 25% of transactions top_75 &lt;- quantile(abs(gl$amount))[4] # This is the 3st quartile, or the 75% quantile iqr &lt;- IQR(abs(gl$amount)) upper_extreme_outlier &lt;- top_75 + iqr * 1.5 print(paste0(&quot;The upper_extreme_outlier is $&quot;, upper_extreme_outlier)) ## [1] &quot;The upper_extreme_outlier is $31929.19125&quot; gl %&gt;% dplyr::filter(amount &gt; upper_extreme_outlier) %&gt;% select(je_num, amount, account, description) %&gt;% arrange(description) ## # A tibble: 46 x 4 ## je_num amount account description ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 5 43140. exp_materials_6000 Blue dye ## 2 430 37341. exp_materials_6000 Blue dye ## 3 825 36918. exp_materials_6000 Blue dye ## 4 70 32584. exp_materials_6000 Fabric cutting machines ## 5 427 44950. exp_materials_6000 Fabric cutting machines ## 6 54 35758. exp_materials_6000 Face mask elastics ## 7 531 42785. exp_materials_6000 Face mask elastics ## 8 680 36860. exp_materials_6000 Fancy cloth prints ## 9 832 37269. exp_materials_6000 Fancy cloth prints ## 10 66 51347. exp_materials_6000 Medical grade filters ## # … with 36 more rows 9.3.2 Outliers based on amounts and other factors By performing a test based solely on amounts alone, other meaningful data is ignored. The nuances of specific subsets of data are lost due to an overarching amount analysis, which could be too crude. There are other useful features in our dataset we can use to potentially segment the data. There is the account information, which can be interpreted as consistent categories of information. There are also descriptions, which help us understand more granular detail about the nature of the transaction. A combination of any identifying categories or factors could be used to identify outliers. If we focus our analysis on both the description and amount, we can identify amount extreme outliers on a more unique basis. First, we need to group the descriptions and independently calculate the upper_extreme_outlier on a per group basis: outliers_per_desc &lt;- gl %&gt;% group_by(description) %&gt;% summarize(count = n(), upper_limit = quantile(abs(amount))[4], iqr = IQR(abs(amount))) %&gt;% mutate(upper_extreme_outlier = upper_limit + 1.5 * iqr) print(outliers_per_desc) ## # A tibble: 25 x 5 ## description count upper_limit iqr upper_extreme_out… ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Advisory fee for face mask manuf… 10 9277. 1.84e2 9552. ## 2 Advisory services for tax expats 24 9897. 3.53e2 10427. ## 3 Audit fees for big four audit fi… 10 9788. 4.73e2 10497. ## 4 Blue dye 80 14581. 1.20e4 32509. ## 5 Buffet 4 156. 3.76e1 212. ## 6 Catering 8 120. 7.26e1 229. ## 7 Consulting fee on how to adhere … 6 9784. 5.14e2 10556. ## 8 Fabric cutting machines 68 15591. 1.31e4 35204. ## 9 Face mask elastics 74 13461. 1.10e4 30007. ## 10 Facilitation payment to agent fi… 2 9715. 0. 9715. ## # … with 15 more rows Using this, we can quickly look up the upper_extreme_outlier in our gl table, and reduce our scope to only filter amounts greater than the limit per description entry. gl %&gt;% left_join(outliers_per_desc, by = &#39;description&#39;) %&gt;% filter(amount &gt;= upper_extreme_outlier) %&gt;% select(je_num, amount, account, description, upper_extreme_outlier) %&gt;% group_by(description) %&gt;% slice_max(order_by = amount, n = 5) ## # A tibble: 33 x 5 ## # Groups: description [14] ## je_num amount account description upper_extreme_out… ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 975 9834. exp_consultin… Advisory fee for face mask … 9552. ## 2 5 43140. exp_materials… Blue dye 32509. ## 3 430 37341. exp_materials… Blue dye 32509. ## 4 825 36918. exp_materials… Blue dye 32509. ## 5 427 44950. exp_materials… Fabric cutting machines 35204. ## 6 531 42785. exp_materials… Face mask elastics 30007. ## 7 54 35758. exp_materials… Face mask elastics 30007. ## 8 755 9715. exp_consultin… Facilitation payment to age… 9715. ## 9 335 52.2 exp_meals_7000 Italian food restaurant 52.2 ## 10 309 1630. exp_materials… Meals for launch celebration 1630. ## # … with 23 more rows The results are different than the outliers based solely on the amount, and are a good evaluation on a per-group basis. 9.4 Unusual pattern and behaviours 9.4.1 Benford’s Law analysis The general principle of fraud detection is that individuals behave differently when they attempt to conceal, manipulate or make up information, as the fictional creation of numbers by people is different than how the accounting world behaves. As a result, these behaviours tend to stand out when certain tests are applied to them. Benford’s Law is a common application in the accounting and auditing, as it is a quick litmus test to see if there are symptoms of anomalies in a dataset. The law generally states that in a set of values, the leading digit of 1 will occur more often than the leading digit of 2, the leading digit of 2 will naturally occur more often than the leading digit of 3, etc.. With a sufficiently large population, you can reasonably estimate the number of times a leading digit should appear a certain number of times. Think about the general working population and hourly wages. You could reasonably assume that its relatively accessible to get a job that pays $10-$19/hr. Its more difficult to get a job with $20-$29/hr wage, and even more so to get a job with $90-$99/hr wage. What is fascinating is that once you reach the next digit, the wages all start with 1’s again, and it takes a lot of effort to get out of the $100-$199/hr band. This type of scaling means its increasingly harder for individuals to get a higher hourly salary. While there are some lucky folks that earn $200/hr, those individuals are more rare than those who earn $20/hr. With Benford’s Law and applying the concept to a population of natural numbers, then the concept of leading digits makes intuitive sense. There will be more salaries that start with 1, than with 2, etc. You could estimate that a reasonable proportion of hourly wages could look like this: If we assume the first dataset is an example of a generally real world dataset, then lets look at a completely fictional dataset. For example, lets say in a range of 1 to 999, we randomly sample 1000 numbers and apply the Benford Law, we will notice the following: benford_sample &lt;- data.frame(seq = sample(1:999, 1000, replace = TRUE)) benford_sample$leading &lt;- as.numeric(substr(benford_sample$seq, 1, 1)) benford_sample %&gt;% group_by(leading) %&gt;% summarize(times_occured = n()) %&gt;% ggplot(aes(x = as.factor(leading), y = times_occured)) + geom_col() Notice how uniform the bars appear. The sample we chose was essentially made-up numbers between 1 and 999, the computer would choose these with equal probability, unlike the population of hourly wages, leading to a graph that does not follow the curve of Benford’s Law. 9.4.1.1 Applying Benford’s Law Several statistical tests exist to determine whether or not a population conforms to Benford’s Law. While chi-square tests are numerically ‘authoritative,’ remember that the individuals samples will indicate whether or not fraud truly exists. In application, Benford’s Law requires professional judgment, and there can be any number of reasons why a population doesn’t follow it. My successful experiences as an auditor have been to chart the actual proportion of leading digits against a curve of the expected outcome, and determine if there are any business reasons that may rationalize a pattern in the data before I perform further sampling. To calculate the curve, you need to determine the likelihood that each digit should appear. The expected proportion for each digit can be calculated as follows: \\[log_{10}\\left( 1 + \\frac{1}{d}\\right)\\] In R, this is represented as: d &lt;- 1 # Leading digit log10(1 + 1 / d) ## [1] 0.30103 We can create a quick reference curve of the leading digits for Benford’s Law: benford_curve &lt;- data.frame(digit = 1:9) %&gt;% mutate(prob = log10(1 + 1 / digit), curve = &#39;benford&#39;) ggplot(benford_curve, aes(x = as.factor(digit), y = prob, group = curve)) + geom_line(color = &#39;red&#39;) + geom_point(color = &#39;red&#39;) We can then take any of our populations, create the bar chart, and then apply the expected proportions of Benford’s Law to it. Lets take our GL database and create the proportion of how often each leading digit occurs. Note we will try to follow the same variable structure as benford_curve, as we will combine these on the same chart. gl %&gt;% mutate(digit = as.numeric(substr(amount, 1, 1))) %&gt;% group_by(digit) %&gt;% summarize(times_occured = n()) %&gt;% mutate(prop = times_occured / sum(times_occured), curve = &#39;actual&#39;) %&gt;% ggplot(aes(x = as.factor(digit), y = prop)) + geom_col() Notice in our chart that there are several NAs. In our dataset, we have several negative numbers (as journal entry credits as represented with negatives), and extracting the first character using substr() will retrieve the negative symbol. You may choose to perform Benford’s Law on the positive and negative numbers, separately or together, transforming the negative numbers with abs(). You can also choose to exclude them, as I do below: actuals &lt;- gl %&gt;% filter(amount &gt; 0) %&gt;% mutate(digit = as.numeric(substr(amount, 1, 1))) %&gt;% group_by(digit) %&gt;% summarize(times_occured = n()) %&gt;% mutate(prop = times_occured / sum(times_occured), curve = &#39;actual&#39;) %&gt;% ggplot(aes(x = as.factor(digit), y = prop)) + geom_col() actuals Now that we have the actual proportions, lets layer on the benford_curve we created: actuals + geom_line(data = benford_curve, aes(x = as.factor(digit), y = prob, group = curve), color = &#39;red&#39;) + geom_point(data = benford_curve, aes(x = as.factor(digit), y = prob), color = &#39;red&#39;) When you reach this stage, take a moment to pause and understand the business, and if there may be any potential behavours to cause potential curve differences. For example, if there is an approval ‘limit’ that exists at $1000, you may see a spike of entries at the 9 leading digit. As with any statistical test, Benford’s Law will not give you an answer as to why there may be fraud. There may be both legitimate reasons for why populations do not follow Benford’s Law. A higher than expected proportion of leading digits starting with 9 does not mean those numbers starting with 9 are fraudulent. 9.4.1.2 More leading digits? You can get any number of leading digits to apply Benford’s Law - to do so, you need to start on the lowest possible value for the number of digits desired, to remove leading zeros, and extend the sequence to the highest number before a new digit must be added. For example, two leading digits would have a sequence of numbers created from 10:99, and three leading digits would be 100:999: benford_curve_leading2 &lt;- data.frame(digit = 10:99) %&gt;% mutate(prob = log10(1 + 1 / digit), curve = &#39;benford&#39;) head(benford_curve_leading2) ## digit prob curve ## 1 10 0.04139269 benford ## 2 11 0.03778856 benford ## 3 12 0.03476211 benford ## 4 13 0.03218468 benford ## 5 14 0.02996322 benford ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 1 rows ] tail(benford_curve_leading2) ## digit prob curve ## 85 94 0.004595752 benford ## 86 95 0.004547628 benford ## 87 96 0.004500501 benford ## 88 97 0.004454341 benford ## 89 98 0.004409119 benford ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 1 rows ] 9.4.2 Relative size factor (RSF) The Relative Size Factor test is the comparison of “large amounts to a benchmark to see how large they are relative to some norm.” (Identifying Anomalies Using the Relative Size Factor Test 2012). The calculation is done by taking the largest value divided by the second largest value within a group. Examples of groups include a listing of payables for a vendor, or the expenses completed by an employee. A larger RSF indicates that there was a larger proportional between the largest and second largest invoice. One test is to determining the vendors top two invoice amounts and their RSF ratio. Recall the group_by() will perform the calculation at the level requested - adding slice_max() will select the highest row of values by the group. top_two_per_vendor &lt;- gl %&gt;% filter(account == &#39;exp_materials_6000&#39;) %&gt;% select(vendor_id, je_num, amount) %&gt;% group_by(vendor_id) %&gt;% slice_max(amount, n = 2) top_two_per_vendor ## # A tibble: 6 x 3 ## # Groups: vendor_id [3] ## vendor_id je_num amount ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2211 140 96355. ## 2 2211 555 57258. ## 3 2612 583 47748. ## 4 2612 337 46066. ## 5 2726 693 88524. ## 6 2726 756 57567. Since we’ve identified the top two invoices per vendor_id, we’ve artifically also created new max and mins for the group, which we can use to calculate our RSF: top_two_per_vendor %&gt;% summarize(rsf = max(amount) / min(amount)) ## # A tibble: 3 x 2 ## vendor_id rsf ## &lt;int&gt; &lt;dbl&gt; ## 1 2211 1.68 ## 2 2612 1.04 ## 3 2726 1.54 This test may reveal invoices that may have been miskeyed with extra zeros, or conversely, identify large one-time orders. 9.4.3 Duplicate Invoices Detection of duplicates are a seemingly simple concept. For invoices, find an invoice with the same invoice date and same amount. Experience will tell you that detecting suspected duplicates and generating a list is easy, and unfortunately there will be a significant number of false positives to wade through. We will go through the basics of duplicate detection with invoices, and then determine additional strategies you can apply to rule out unlikely samples. These same strategies can be applied across any type of payable. The initial approach is built off the Same-Same-Same or Same-Same-Different duplicate detection approach (Identifying Fraud Using Abnormal Duplications Within Subsets 2012). Essentially, you examine for factors where the may be similarities in the key fields, and potentially differences in one of them. Here is a fictional dataset to illustrate how data may be represented. Note that despite the name, there can be as many ‘Same’ key fields as needed. invoices &lt;- data.frame(system_id = 1:7, invoice_id = c(&#39;156&#39;, &#39;156&#39;, &#39;157A&#39;, &#39;157B&#39;, &#39;158&#39;, &#39;159&#39;, &#39;160&#39;), date = c(as.Date(&#39;2019-01-01&#39;), as.Date(&#39;2019-01-01&#39;), as.Date(&#39;2019-01-02&#39;), as.Date(&#39;2019-01-02&#39;), as.Date(&#39;2019-01-03&#39;), as.Date(&#39;2019-01-03&#39;), as.Date(&#39;2019-01-04&#39;)), amount = c(1000, 1000, 2500, 2500, 3000, 3000, 4000), acctcode = c(6100, 6100, 6200, 6200, 6100, 6200, 6100)) invoices %&gt;% gt() # We&#39;ll hide this table-making code in future chunks. For now, its going to make our table prettier html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gvbjqeheyl .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gvbjqeheyl .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gvbjqeheyl .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gvbjqeheyl .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #gvbjqeheyl .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gvbjqeheyl .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gvbjqeheyl .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gvbjqeheyl .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gvbjqeheyl .gt_column_spanner_outer:first-child { padding-left: 0; } #gvbjqeheyl .gt_column_spanner_outer:last-child { padding-right: 0; } #gvbjqeheyl .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #gvbjqeheyl .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #gvbjqeheyl .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gvbjqeheyl .gt_from_md > :first-child { margin-top: 0; } #gvbjqeheyl .gt_from_md > :last-child { margin-bottom: 0; } #gvbjqeheyl .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gvbjqeheyl .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #gvbjqeheyl .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gvbjqeheyl .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #gvbjqeheyl .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gvbjqeheyl .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gvbjqeheyl .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gvbjqeheyl .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gvbjqeheyl .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gvbjqeheyl .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #gvbjqeheyl .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gvbjqeheyl .gt_sourcenote { font-size: 90%; padding: 4px; } #gvbjqeheyl .gt_left { text-align: left; } #gvbjqeheyl .gt_center { text-align: center; } #gvbjqeheyl .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gvbjqeheyl .gt_font_normal { font-weight: normal; } #gvbjqeheyl .gt_font_bold { font-weight: bold; } #gvbjqeheyl .gt_font_italic { font-style: italic; } #gvbjqeheyl .gt_super { font-size: 65%; } #gvbjqeheyl .gt_footnote_marks { font-style: italic; font-size: 65%; } system_id invoice_id date amount acctcode 1 156 2019-01-01 1000 6100 2 156 2019-01-01 1000 6100 3 157A 2019-01-02 2500 6200 4 157B 2019-01-02 2500 6200 5 158 2019-01-03 3000 6100 6 159 2019-01-03 3000 6200 7 160 2019-01-04 4000 6100 9.4.3.1 Same-Same-Same To run our Same-Same-Same test, we would collect the key fields we would like to use. In this case, the invoice, date and amount could qualify as key fields - Same Invoice ID, Same Date, Same Amount. You would summarize by how often that combination appears, only keeping items that match more than once: dupes_set1 &lt;- invoices %&gt;% group_by(invoice_id, date, amount) %&gt;% summarize(times_occured = n()) %&gt;% filter(times_occured &gt; 1) %&gt;% ungroup() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #xiabbqluft .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #xiabbqluft .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xiabbqluft .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xiabbqluft .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #xiabbqluft .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xiabbqluft .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xiabbqluft .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #xiabbqluft .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #xiabbqluft .gt_column_spanner_outer:first-child { padding-left: 0; } #xiabbqluft .gt_column_spanner_outer:last-child { padding-right: 0; } #xiabbqluft .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #xiabbqluft .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #xiabbqluft .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #xiabbqluft .gt_from_md > :first-child { margin-top: 0; } #xiabbqluft .gt_from_md > :last-child { margin-bottom: 0; } #xiabbqluft .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xiabbqluft .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #xiabbqluft .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xiabbqluft .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #xiabbqluft .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xiabbqluft .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xiabbqluft .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #xiabbqluft .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xiabbqluft .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xiabbqluft .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #xiabbqluft .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xiabbqluft .gt_sourcenote { font-size: 90%; padding: 4px; } #xiabbqluft .gt_left { text-align: left; } #xiabbqluft .gt_center { text-align: center; } #xiabbqluft .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xiabbqluft .gt_font_normal { font-weight: normal; } #xiabbqluft .gt_font_bold { font-weight: bold; } #xiabbqluft .gt_font_italic { font-style: italic; } #xiabbqluft .gt_super { font-size: 65%; } #xiabbqluft .gt_footnote_marks { font-style: italic; font-size: 65%; } invoice_id date amount times_occured 156 2019-01-01 1000 2 To keep track of which invoices meet specific criteria, we will then join this information to the main dataset. This way, you’ll maintain a library of invoices that meet multiple criteria. First off, lets keep keep all the duplicate keys we summarized on, plus a unique identifier so we can match the pairs: dupes_set1 &lt;- dupes_set1 %&gt;% ungroup() %&gt;% # Remove the grouping aspect, which affects the ranking by row_number() mutate(t_dupes1 = row_number()) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #mlfhsueuok .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #mlfhsueuok .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mlfhsueuok .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #mlfhsueuok .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #mlfhsueuok .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mlfhsueuok .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mlfhsueuok .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #mlfhsueuok .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #mlfhsueuok .gt_column_spanner_outer:first-child { padding-left: 0; } #mlfhsueuok .gt_column_spanner_outer:last-child { padding-right: 0; } #mlfhsueuok .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #mlfhsueuok .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #mlfhsueuok .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #mlfhsueuok .gt_from_md > :first-child { margin-top: 0; } #mlfhsueuok .gt_from_md > :last-child { margin-bottom: 0; } #mlfhsueuok .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #mlfhsueuok .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #mlfhsueuok .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mlfhsueuok .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #mlfhsueuok .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mlfhsueuok .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #mlfhsueuok .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #mlfhsueuok .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mlfhsueuok .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mlfhsueuok .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #mlfhsueuok .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mlfhsueuok .gt_sourcenote { font-size: 90%; padding: 4px; } #mlfhsueuok .gt_left { text-align: left; } #mlfhsueuok .gt_center { text-align: center; } #mlfhsueuok .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #mlfhsueuok .gt_font_normal { font-weight: normal; } #mlfhsueuok .gt_font_bold { font-weight: bold; } #mlfhsueuok .gt_font_italic { font-style: italic; } #mlfhsueuok .gt_super { font-size: 65%; } #mlfhsueuok .gt_footnote_marks { font-style: italic; font-size: 65%; } invoice_id date amount times_occured t_dupes1 156 2019-01-01 1000 2 1 Then bring it into our invoices dataset: dupes_set1 &lt;- dupes_set1 %&gt;% select(-times_occured) # Unneeded for final result invoices &lt;- invoices %&gt;% left_join(dupes_set1, by = c(&#39;invoice_id&#39;, &#39;date&#39;, &#39;amount&#39;)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #mbrqxazrwp .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #mbrqxazrwp .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mbrqxazrwp .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #mbrqxazrwp .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #mbrqxazrwp .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mbrqxazrwp .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mbrqxazrwp .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #mbrqxazrwp .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #mbrqxazrwp .gt_column_spanner_outer:first-child { padding-left: 0; } #mbrqxazrwp .gt_column_spanner_outer:last-child { padding-right: 0; } #mbrqxazrwp .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #mbrqxazrwp .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #mbrqxazrwp .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #mbrqxazrwp .gt_from_md > :first-child { margin-top: 0; } #mbrqxazrwp .gt_from_md > :last-child { margin-bottom: 0; } #mbrqxazrwp .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #mbrqxazrwp .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #mbrqxazrwp .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mbrqxazrwp .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #mbrqxazrwp .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mbrqxazrwp .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #mbrqxazrwp .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #mbrqxazrwp .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mbrqxazrwp .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mbrqxazrwp .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #mbrqxazrwp .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mbrqxazrwp .gt_sourcenote { font-size: 90%; padding: 4px; } #mbrqxazrwp .gt_left { text-align: left; } #mbrqxazrwp .gt_center { text-align: center; } #mbrqxazrwp .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #mbrqxazrwp .gt_font_normal { font-weight: normal; } #mbrqxazrwp .gt_font_bold { font-weight: bold; } #mbrqxazrwp .gt_font_italic { font-style: italic; } #mbrqxazrwp .gt_super { font-size: 65%; } #mbrqxazrwp .gt_footnote_marks { font-style: italic; font-size: 65%; } system_id invoice_id date amount acctcode t_dupes1 1 156 2019-01-01 1000 6100 1 2 156 2019-01-01 1000 6100 1 3 157A 2019-01-02 2500 6200 NA 4 157B 2019-01-02 2500 6200 NA 5 158 2019-01-03 3000 6100 NA 6 159 2019-01-03 3000 6200 NA 7 160 2019-01-04 4000 6100 NA 9.4.3.2 Same-Same-Different For the Same-Same-Different test, we have to add an extra step. We still collect the key fields we would like to use. In this case, perhaps we would like to check for duplicates on the same date and same amount, but now we’re looking for different invoice_ids. Therefore, Same Date, Same Amount, Different Invoice_id. First, lets detect the duplicates on our key fields: dupes_set2 &lt;- invoices %&gt;% group_by(invoice_id, date, amount) %&gt;% summarize(times_occured = n()) %&gt;% ungroup() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gyezbhybfp .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gyezbhybfp .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gyezbhybfp .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gyezbhybfp .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #gyezbhybfp .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gyezbhybfp .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gyezbhybfp .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gyezbhybfp .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gyezbhybfp .gt_column_spanner_outer:first-child { padding-left: 0; } #gyezbhybfp .gt_column_spanner_outer:last-child { padding-right: 0; } #gyezbhybfp .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #gyezbhybfp .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #gyezbhybfp .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gyezbhybfp .gt_from_md > :first-child { margin-top: 0; } #gyezbhybfp .gt_from_md > :last-child { margin-bottom: 0; } #gyezbhybfp .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gyezbhybfp .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #gyezbhybfp .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gyezbhybfp .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #gyezbhybfp .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gyezbhybfp .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gyezbhybfp .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gyezbhybfp .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gyezbhybfp .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gyezbhybfp .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #gyezbhybfp .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gyezbhybfp .gt_sourcenote { font-size: 90%; padding: 4px; } #gyezbhybfp .gt_left { text-align: left; } #gyezbhybfp .gt_center { text-align: center; } #gyezbhybfp .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gyezbhybfp .gt_font_normal { font-weight: normal; } #gyezbhybfp .gt_font_bold { font-weight: bold; } #gyezbhybfp .gt_font_italic { font-style: italic; } #gyezbhybfp .gt_super { font-size: 65%; } #gyezbhybfp .gt_footnote_marks { font-style: italic; font-size: 65%; } invoice_id date amount times_occured 156 2019-01-01 1000 2 157A 2019-01-02 2500 1 157B 2019-01-02 2500 1 158 2019-01-03 3000 1 159 2019-01-03 3000 1 160 2019-01-04 4000 1 Then, you perform another group_by, but this time just on the ones that are the same. In this case, that would be Date and Amount. dupes_set2 &lt;- dupes_set2 %&gt;% # Remove the grouping aspect, which is kept by default in dplyr group_by(date, amount) %&gt;% # Second set of reductions, but this time, find how many unique invoice ids there are per date-amount summarize(times_occured = n()) %&gt;% filter(times_occured &gt; 1) %&gt;% ungroup() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #kxizmgbrzs .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #kxizmgbrzs .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kxizmgbrzs .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #kxizmgbrzs .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #kxizmgbrzs .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kxizmgbrzs .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kxizmgbrzs .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #kxizmgbrzs .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #kxizmgbrzs .gt_column_spanner_outer:first-child { padding-left: 0; } #kxizmgbrzs .gt_column_spanner_outer:last-child { padding-right: 0; } #kxizmgbrzs .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #kxizmgbrzs .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #kxizmgbrzs .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #kxizmgbrzs .gt_from_md > :first-child { margin-top: 0; } #kxizmgbrzs .gt_from_md > :last-child { margin-bottom: 0; } #kxizmgbrzs .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #kxizmgbrzs .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #kxizmgbrzs .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kxizmgbrzs .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #kxizmgbrzs .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kxizmgbrzs .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #kxizmgbrzs .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #kxizmgbrzs .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kxizmgbrzs .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kxizmgbrzs .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #kxizmgbrzs .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kxizmgbrzs .gt_sourcenote { font-size: 90%; padding: 4px; } #kxizmgbrzs .gt_left { text-align: left; } #kxizmgbrzs .gt_center { text-align: center; } #kxizmgbrzs .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #kxizmgbrzs .gt_font_normal { font-weight: normal; } #kxizmgbrzs .gt_font_bold { font-weight: bold; } #kxizmgbrzs .gt_font_italic { font-style: italic; } #kxizmgbrzs .gt_super { font-size: 65%; } #kxizmgbrzs .gt_footnote_marks { font-style: italic; font-size: 65%; } date amount times_occured 2019-01-02 2500 2 2019-01-03 3000 2 This approach is unique but intuitive. By taking all possible ‘same’ combinations, we’ve effectively grouped any sort of duplicate invoice_id we may have already had. We then ungroup and resummarize without the invoice_id, which tells us how many times the invoice_id is different within the new group. Lets save this information into our invoice table again: dupes_set2 &lt;- dupes_set2 %&gt;% mutate(t_dupes2 = row_number()) %&gt;% select(-times_occured) invoices &lt;- invoices %&gt;% left_join(dupes_set2, by = c(&#39;date&#39;, &#39;amount&#39;)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #etrcjbymsb .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #etrcjbymsb .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #etrcjbymsb .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #etrcjbymsb .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #etrcjbymsb .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #etrcjbymsb .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #etrcjbymsb .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #etrcjbymsb .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #etrcjbymsb .gt_column_spanner_outer:first-child { padding-left: 0; } #etrcjbymsb .gt_column_spanner_outer:last-child { padding-right: 0; } #etrcjbymsb .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #etrcjbymsb .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #etrcjbymsb .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #etrcjbymsb .gt_from_md > :first-child { margin-top: 0; } #etrcjbymsb .gt_from_md > :last-child { margin-bottom: 0; } #etrcjbymsb .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #etrcjbymsb .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #etrcjbymsb .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #etrcjbymsb .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #etrcjbymsb .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #etrcjbymsb .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #etrcjbymsb .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #etrcjbymsb .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #etrcjbymsb .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #etrcjbymsb .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #etrcjbymsb .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #etrcjbymsb .gt_sourcenote { font-size: 90%; padding: 4px; } #etrcjbymsb .gt_left { text-align: left; } #etrcjbymsb .gt_center { text-align: center; } #etrcjbymsb .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #etrcjbymsb .gt_font_normal { font-weight: normal; } #etrcjbymsb .gt_font_bold { font-weight: bold; } #etrcjbymsb .gt_font_italic { font-style: italic; } #etrcjbymsb .gt_super { font-size: 65%; } #etrcjbymsb .gt_footnote_marks { font-style: italic; font-size: 65%; } system_id invoice_id date amount acctcode t_dupes1 t_dupes2 1 156 2019-01-01 1000 6100 1 NA 2 156 2019-01-01 1000 6100 1 NA 3 157A 2019-01-02 2500 6200 NA 1 4 157B 2019-01-02 2500 6200 NA 1 5 158 2019-01-03 3000 6100 NA 2 6 159 2019-01-03 3000 6200 NA 2 7 160 2019-01-04 4000 6100 NA NA The beauty of this method is that, after you’ve run all the scenarios you prefer, you can rapidly assess each invoice and see what duplicate criteria they match. Ideally, you would balance having tests that are very general (same date, same amount) with tests of high precision (same date, same amount, and different account codes). 9.4.3.3 Aggregating results To derive the final list, we can filter the results based on the t_dupes columns. Starting with the invoices we’ve identified, we’ll first group by unique system_id we’ve created, allowing us to perform calculations on unique rows. To determine if any test did return something, we will take a sum of any number of columns that return a TRUE (with a value of 1 when coerced from logical to numeric). Then, we want to calculate this on multiple columns, specifically with the name starting with dupes, so we will use across() with starts_with(). invoices %&gt;% rowwise(system_id) %&gt;% mutate(dupes = sum(across(starts_with(&#39;t_dupes&#39;)) &gt;= 1, na.rm = TRUE)) %&gt;% ungroup() %&gt;% filter(dupes &gt; 0) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #utvczdvuub .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #utvczdvuub .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #utvczdvuub .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #utvczdvuub .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #utvczdvuub .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #utvczdvuub .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #utvczdvuub .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #utvczdvuub .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #utvczdvuub .gt_column_spanner_outer:first-child { padding-left: 0; } #utvczdvuub .gt_column_spanner_outer:last-child { padding-right: 0; } #utvczdvuub .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #utvczdvuub .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #utvczdvuub .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #utvczdvuub .gt_from_md > :first-child { margin-top: 0; } #utvczdvuub .gt_from_md > :last-child { margin-bottom: 0; } #utvczdvuub .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #utvczdvuub .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #utvczdvuub .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #utvczdvuub .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #utvczdvuub .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #utvczdvuub .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #utvczdvuub .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #utvczdvuub .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #utvczdvuub .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #utvczdvuub .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #utvczdvuub .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #utvczdvuub .gt_sourcenote { font-size: 90%; padding: 4px; } #utvczdvuub .gt_left { text-align: left; } #utvczdvuub .gt_center { text-align: center; } #utvczdvuub .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #utvczdvuub .gt_font_normal { font-weight: normal; } #utvczdvuub .gt_font_bold { font-weight: bold; } #utvczdvuub .gt_font_italic { font-style: italic; } #utvczdvuub .gt_super { font-size: 65%; } #utvczdvuub .gt_footnote_marks { font-style: italic; font-size: 65%; } system_id invoice_id date amount acctcode t_dupes1 t_dupes2 dupes 1 156 2019-01-01 1000 6100 1 NA 1 2 156 2019-01-01 1000 6100 1 NA 1 3 157A 2019-01-02 2500 6200 NA 1 1 4 157B 2019-01-02 2500 6200 NA 1 1 5 158 2019-01-03 3000 6100 NA 2 1 6 159 2019-01-03 3000 6200 NA 2 1 This approach is powerful, yet flexible. By maintaining our approach of having a main invoice dataset with all the matches, any new tests we add starting with the column name dupes will be included. For example, if we inserted a higher precision test of same invoice date, amount, and account code, we’ll see our counts go up: dupes_set3 &lt;- invoices %&gt;% group_by(date, amount, acctcode) %&gt;% summarize(times_occured = n()) %&gt;% filter(times_occured &gt; 1) %&gt;% ungroup() %&gt;% # Remove the grouping aspect, which affects the ranking by row_number() mutate(t_dupes3 = row_number()) %&gt;% select(-times_occured) invoices &lt;- invoices %&gt;% left_join(dupes_set3, by = c(&#39;date&#39;, &#39;amount&#39;, &#39;acctcode&#39;)) invoices %&gt;% rowwise(system_id) %&gt;% mutate(t_dupesmatched = sum(across(starts_with(&#39;t_dupes&#39;)) &gt;= 1, na.rm = TRUE)) %&gt;% ungroup() %&gt;% filter(t_dupesmatched &gt; 0) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #rdpuxiwzps .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #rdpuxiwzps .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rdpuxiwzps .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rdpuxiwzps .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #rdpuxiwzps .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rdpuxiwzps .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rdpuxiwzps .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #rdpuxiwzps .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #rdpuxiwzps .gt_column_spanner_outer:first-child { padding-left: 0; } #rdpuxiwzps .gt_column_spanner_outer:last-child { padding-right: 0; } #rdpuxiwzps .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #rdpuxiwzps .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #rdpuxiwzps .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #rdpuxiwzps .gt_from_md > :first-child { margin-top: 0; } #rdpuxiwzps .gt_from_md > :last-child { margin-bottom: 0; } #rdpuxiwzps .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rdpuxiwzps .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #rdpuxiwzps .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rdpuxiwzps .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #rdpuxiwzps .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rdpuxiwzps .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rdpuxiwzps .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #rdpuxiwzps .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rdpuxiwzps .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rdpuxiwzps .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #rdpuxiwzps .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rdpuxiwzps .gt_sourcenote { font-size: 90%; padding: 4px; } #rdpuxiwzps .gt_left { text-align: left; } #rdpuxiwzps .gt_center { text-align: center; } #rdpuxiwzps .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rdpuxiwzps .gt_font_normal { font-weight: normal; } #rdpuxiwzps .gt_font_bold { font-weight: bold; } #rdpuxiwzps .gt_font_italic { font-style: italic; } #rdpuxiwzps .gt_super { font-size: 65%; } #rdpuxiwzps .gt_footnote_marks { font-style: italic; font-size: 65%; } system_id invoice_id date amount acctcode t_dupes1 t_dupes2 t_dupes3 t_dupesmatched 1 156 2019-01-01 1000 6100 1 NA 1 2 2 156 2019-01-01 1000 6100 1 NA 1 2 3 157A 2019-01-02 2500 6200 NA 1 2 2 4 157B 2019-01-02 2500 6200 NA 1 2 2 5 158 2019-01-03 3000 6100 NA 2 NA 1 6 159 2019-01-03 3000 6200 NA 2 NA 1 9.4.3.4 Ruling out false positives Up to now, you’ve identified duplicates via the Same-Same-Same and Same-Same-Different methods. Perhaps in your own work, you’ve gone ahead and tested potential duplicates, and then realized there is some business logic you didn’t capture that may have nullified the potential duplicate, saving you time and effort. A common reason for a false positive is that the company may have already received a refund for the invoice. In this case, its more practical to identify and remove invoices that have had refunds in the original invoice population prior to processing, using an anti_join(). For example, if invoice 156 with a system_id of 2 was already removed, you could refine the invoice population before executing additional tests. refunds &lt;- data.frame(system_id = 2) # A table of credits or refunds new_invoices &lt;- invoices %&gt;% anti_join(refunds, by = &#39;system_id&#39;) %&gt;% # Remove refunds select(system_id, invoice_id, date, amount, acctcode) new_invoices %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ubckvllqkn .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ubckvllqkn .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ubckvllqkn .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ubckvllqkn .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #ubckvllqkn .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ubckvllqkn .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ubckvllqkn .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ubckvllqkn .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ubckvllqkn .gt_column_spanner_outer:first-child { padding-left: 0; } #ubckvllqkn .gt_column_spanner_outer:last-child { padding-right: 0; } #ubckvllqkn .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #ubckvllqkn .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #ubckvllqkn .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ubckvllqkn .gt_from_md > :first-child { margin-top: 0; } #ubckvllqkn .gt_from_md > :last-child { margin-bottom: 0; } #ubckvllqkn .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ubckvllqkn .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ubckvllqkn .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ubckvllqkn .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ubckvllqkn .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ubckvllqkn .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ubckvllqkn .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ubckvllqkn .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ubckvllqkn .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ubckvllqkn .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #ubckvllqkn .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ubckvllqkn .gt_sourcenote { font-size: 90%; padding: 4px; } #ubckvllqkn .gt_left { text-align: left; } #ubckvllqkn .gt_center { text-align: center; } #ubckvllqkn .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ubckvllqkn .gt_font_normal { font-weight: normal; } #ubckvllqkn .gt_font_bold { font-weight: bold; } #ubckvllqkn .gt_font_italic { font-style: italic; } #ubckvllqkn .gt_super { font-size: 65%; } #ubckvllqkn .gt_footnote_marks { font-style: italic; font-size: 65%; } system_id invoice_id date amount acctcode 1 156 2019-01-01 1000 6100 3 157A 2019-01-02 2500 6200 4 157B 2019-01-02 2500 6200 5 158 2019-01-03 3000 6100 6 159 2019-01-03 3000 6200 7 160 2019-01-04 4000 6100 A duplicate may also be ruled out because because of a minor field that wasn’t used. If you examine lines 5 and 6, you’ll see that that this invoice was picked up because of the same date &amp; amount, with a different invoice_id. You may have noticed that the acctcode is different, representing that the invoice was being allocated to a different cost center. After inquiring with the business, you may determine this behavior is normal. Its easier add another test to accommodate for this scenario, following the Same-Same-Different approach, to help increase the accuracy of test: dupes_set4 &lt;- invoices %&gt;% group_by(invoice_id, date, amount, acctcode) %&gt;% summarize(times_occured = n()) %&gt;% ungroup() %&gt;% # Remove the grouping aspect, which is kept by default in dplyr group_by(date, amount, acctcode) %&gt;% # Second set of reductions, but this time, find how many unique invoice ids there are per date-amount summarize(times_occured = n()) %&gt;% filter(times_occured &gt; 1) %&gt;% ungroup() %&gt;% # Remove the grouping aspect, which affects the ranking by row_number() mutate(t_dupes4 = row_number()) %&gt;% select(-times_occured) invoices &lt;- invoices %&gt;% left_join(dupes_set4, by = c(&#39;date&#39;, &#39;amount&#39;, &#39;acctcode&#39;)) invoices %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #wkuunzwjjz .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #wkuunzwjjz .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wkuunzwjjz .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #wkuunzwjjz .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #wkuunzwjjz .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wkuunzwjjz .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wkuunzwjjz .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #wkuunzwjjz .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #wkuunzwjjz .gt_column_spanner_outer:first-child { padding-left: 0; } #wkuunzwjjz .gt_column_spanner_outer:last-child { padding-right: 0; } #wkuunzwjjz .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #wkuunzwjjz .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #wkuunzwjjz .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #wkuunzwjjz .gt_from_md > :first-child { margin-top: 0; } #wkuunzwjjz .gt_from_md > :last-child { margin-bottom: 0; } #wkuunzwjjz .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #wkuunzwjjz .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #wkuunzwjjz .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wkuunzwjjz .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #wkuunzwjjz .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wkuunzwjjz .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #wkuunzwjjz .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #wkuunzwjjz .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wkuunzwjjz .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wkuunzwjjz .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #wkuunzwjjz .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wkuunzwjjz .gt_sourcenote { font-size: 90%; padding: 4px; } #wkuunzwjjz .gt_left { text-align: left; } #wkuunzwjjz .gt_center { text-align: center; } #wkuunzwjjz .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #wkuunzwjjz .gt_font_normal { font-weight: normal; } #wkuunzwjjz .gt_font_bold { font-weight: bold; } #wkuunzwjjz .gt_font_italic { font-style: italic; } #wkuunzwjjz .gt_super { font-size: 65%; } #wkuunzwjjz .gt_footnote_marks { font-style: italic; font-size: 65%; } system_id invoice_id date amount acctcode t_dupes1 t_dupes2 t_dupes3 t_dupes4 1 156 2019-01-01 1000 6100 1 NA 1 NA 2 156 2019-01-01 1000 6100 1 NA 1 NA 3 157A 2019-01-02 2500 6200 NA 1 2 1 4 157B 2019-01-02 2500 6200 NA 1 2 1 5 158 2019-01-03 3000 6100 NA 2 NA NA 6 159 2019-01-03 3000 6200 NA 2 NA NA 7 160 2019-01-04 4000 6100 NA NA NA NA 9.4.4 Detecting gaps in sequences A sequential test is commonly used for cheques, specifically to determine which ones are missing from a cheque run. They are also useful for measuring a level of dependency that a vendor relies upon a company, assuming the vendor issue invoices sequentially as well. As an example, lets say we create a register with 1000 cheques. You can start with any minimum and maximum range: register &lt;- seq(1020, 2019) length(register) ## [1] 1000 range(register) ## [1] 1020 2019 And if we were to create a listing of all cheques we have written, we could simulate perhaps 50 cheques that were taken out: set.seed(123) cheques_issued &lt;- sample(register, size = 975) To detect which cheques have been taken out, we can check the difference between our sets using setdiff() - that is, if we compare our original cheque register to the cheques issued, we can see which ones were missing: missing &lt;- setdiff(register, cheques_issued) missing ## [1] 1026 1062 1079 1166 1201 1302 1375 1378 1385 1387 1455 1493 1503 1504 1510 ## [ reached getOption(&quot;max.print&quot;) -- omitted 10 entries ] If you wanted to know which ones were sequential, you could check the differences between each missing cheque. In this case, it would be a lag() calculation first, to find the difference between each row: data.frame(missing) %&gt;% mutate(gap_between = missing - lag(missing)) ## missing gap_between ## 1 1026 NA ## 2 1062 36 ## 3 1079 17 ## 4 1166 87 ## 5 1201 35 ## 6 1302 101 ## 7 1375 73 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 18 rows ] And finally, since we want to know which sequential cheques were missing, we could filter and look for any rows that had only a difference of one, implying one missing cheque. Since we want chains, we also want to look ahead with lead() since data.frame(missing) %&gt;% mutate(gap_between = missing - lag(missing)) %&gt;% filter(lead(gap_between) == 1 | # Chooses the first row in the chain, by looking at the next row to see if its equal to one gap_between ==1 ) # Any subsequent numbers will always have a difference of 1 ## missing gap_between ## 1 1503 10 ## 2 1504 1 ## 3 1661 86 ## 4 1662 1 9.5 Search text For typical fields in a structured dataset, it is straight forward to filter() on a field to isolate specific traits, like usernames or accounts. However, a great deal of information kept within a database is also unstructured, where the user is allowed to input their own information as they see fit, like description or comment fields. While this information is messy, free-form and generally user-generated, it usually contains a valuable trove of information To scan these unstructured datasets, you need an effective, powerful and rapid method for analysis. Key word searches are an effective method of isolating high-value entries, especially when examining invoices, journal entries, or other datasets where the data may be inconsistent. We may also be interested in words that imply an action like override, or simply looking for mentions of a name in a comment field. 9.5.1 Regex basics We will need to expand our skill set and dabble in regular expression, also known as regex. Regular expression is another language that specializes in the searching of patterns, specifically within text fields. To use regex effectively, you need to consider the pattern of the text to look for, and the field you want to search up. Lets start off by listing of Canadian capitial cities and their provinces: canada &lt;- data.frame(province = c(&quot;Newfoundland and Labrador&quot;, &quot;Prince Edward Island&quot;, &quot;Nova Scotia&quot;, &quot;New Brunswick&quot;, &quot;Quebec&quot;, &quot;Ontario&quot;, &quot;Manitoba&quot;, &quot;Saskatchewan&quot;, &quot;Alberta&quot;, &quot;British Columbia&quot;, &quot;Nunavut&quot;, &quot;Northwest Territories&quot;, &quot;Yukon Territory&quot;), capital = c(&quot;St. John&#39;s&quot;, &quot;Charlottetown&quot;, &quot;Halifax&quot;, &quot;Fredericton&quot;, &quot;Québec City&quot;, &quot;Toronto&quot;, &quot;Winnipeg&quot;, &quot;Regina&quot;, &quot;Edmonton&quot;, &quot;Victoria&quot;, &quot;Iqaluit&quot;, &quot;Yellowknife&quot;, &quot;Whitehorse&quot;)) print(canada) ## province capital ## 1 Newfoundland and Labrador St. John&#39;s ## 2 Prince Edward Island Charlottetown ## 3 Nova Scotia Halifax ## 4 New Brunswick Fredericton ## 5 Quebec Québec City ## 6 Ontario Toronto ## 7 Manitoba Winnipeg ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 6 rows ] Lets say we wanted to find the city of Edmonton in the column of capitals. We’ll use R base function grepl(), which needs a string to search up and a pattern to look into. It returns a logical vector, telling us where matches occur. grepl(&#39;Edmonton&#39;, canada$capital) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [13] FALSE There is only one row that matches our test. We can use this to further isolate our data: canada %&gt;% filter(grepl(&#39;Edmonton&#39;, capital)) ## province capital ## 1 Alberta Edmonton What if you wanted to find two words? Separate words with the | (pipe) character, which stands for ‘or,’ and you chain as many words as you like: canada %&gt;% filter(grepl(&#39;Charlottetown|Regina&#39;, capital)) ## province capital ## 1 Prince Edward Island Charlottetown ## 2 Saskatchewan Regina Regular expressions are powerful because you can customize them to the nth degree. I’ll highlight a few of the common use cases in testing. 9.5.2 Starts or Ends with (Anchors) Lets say you wanted to find any provinces starting with ‘o.’ Lets start with this: canada %&gt;% filter(grepl(&#39;o&#39;, province)) ## province capital ## 1 Newfoundland and Labrador St. John&#39;s ## 2 Nova Scotia Halifax ## 3 Ontario Toronto ## 4 Manitoba Winnipeg ## 5 British Columbia Victoria ## 6 Northwest Territories Yellowknife ## 7 Yukon Territory Whitehorse Notice that grepl() is returning any rows where where ‘o’ is found in the province. We can be more precise and use the prefix ^, which is an anchor. This directs regex to only match where it finds a match at the beginning of the string. As regex is case sensitive, we can tell regex to ignore case sensitivity by using the ignore.case = TRUE argument, which allows us to include both upper and lower case letters in our search (you can try grepl('^o', province) and see what happens): canada %&gt;% filter(grepl(&#39;^o&#39;, province, ignore.case = TRUE)) ## province capital ## 1 Ontario Toronto What if we wanted to find all capitals ending in ‘ton?’ There is another anchor we can use, $, which tells us that we should match the regex only at the end of a string: canada %&gt;% filter(grepl(&#39;ton$&#39;, capital, ignore.case = TRUE)) ## province capital ## 1 New Brunswick Fredericton ## 2 Alberta Edmonton 9.5.3 Crash course in Encoding Simple question. What if we wanted to find Quebec City? canada %&gt;% dplyr::filter(grepl(&#39;Quebec&#39;, capital)) ## [1] province capital ## &lt;0 rows&gt; (or 0-length row.names) There are no hits! But why is that? If you look closely at Quebec, you’ll notice the accented é in Quêbec City. Unfortunately, regex is quite strict on character matching, and you will need to prepare on how to deal with these situations. (Did you notice that the accented é is in Québec City but not the province of Quebec? Why? The province is the English constitutional name and therefore has no diacritics.(“Wikipedia, Name of Quebec City” 2020)) Text can be ‘encoded’ in multiple ways. Encoding is the way a computer saves the representation of a letter. If you look at your standard QWERTY keyboard (which is probably in front of you), you’ll see letters and numbers. Each letter on your keyboard can be represented by a sequence of 8 bits (aka a byte), or a combination of zeros and ones. For example, the letter A can be represented as: pryr::bits(&#39;A&#39;) ## [1] &quot;01000001&quot; Encoding(&#39;A&#39;) ## [1] &quot;unknown&quot; pryr is good for understanding the depths of R. In this case, its helpful for us in understanding character encodings. In this case, the letter is represented as one single byte block with eight bits, which is typical of the ASCII encoding. In R, ASCII is also known as ‘unknown’ - not because it is truly unknown, but because ASCII is the same across all computer systems, no matter if you’re using Unix, Windows, or Mac. What if we did the accented é in Québec City? pryr::bits(&#39;é&#39;) ## [1] &quot;11000011 10101001&quot; Encoding(&#39;é&#39;) ## [1] &quot;UTF-8&quot; Notice there are two bytes! This is known as a multi-byte encoding, and here it is represented as UTF-8 (or Unicode 8-bit). In multi-byte encodings, multiple bytes (could be more than two!) could be used to represent a single character on a screen. 9.5.4 Cleaning Text One method of cleaning text information, especially if perfect replication isn’t necessary, is to strip out any and all characters with accents/diacritics. This method appears to be the most effective, and uses the stringi library and the stri_trans_general() function. clean_e &lt;- stringi::stri_trans_general(&#39;é&#39;, &#39;Latin-ASCII&#39;) clean_e ## [1] &quot;e&quot; Encoding(clean_e) ## [1] &quot;unknown&quot; pryr::bits(clean_e) ## [1] &quot;01100101&quot; So if we apply this cleaning conversion across the cities in our dataset, we can then find words without worrying about accents. canada %&gt;% mutate(capital = stringi::stri_trans_general(capital, &#39;Latin-ASCII&#39;)) %&gt;% dplyr::filter(grepl(&#39;Quebec&#39;, capital, ignore.case = TRUE)) ## province capital ## 1 Quebec Quebec City 9.5.5 Searching Multiple Fields Using grepl(), we can detect text phrases in one column. More often than not, we are searching not only for multiple words, but across any text columns as well. First off, lets assemble a set of test words to search in our GL dataset, separating them with the ‘or’ symbol: searchterms &lt;- c(&#39;quebec&#39;, &#39;facilitation&#39;, &#39;as per&#39;) searchterms &lt;- paste(searchterms, collapse = &quot;|&quot;) We’re going to use dplyr syntax to search multiple columns, and explain how its working. To search across text, we need to create a little helper function first. Lets make a function that will tell us if any row has more than 0, which is another way of saying lets evaluate this row as TRUE if any number of columns found is TRUE. rowAny &lt;- function(x) rowSums(x) &gt; 0 Lets say you had three columns, and you were looking for a word in each one. After executing grepl() on each one, you got this result, indicating TRUE where the word was found. example &lt;- data.frame(colA = c(TRUE, FALSE, FALSE), colB = c(FALSE, FALSE, FALSE), colC = c(TRUE, FALSE, TRUE)) example ## colA colB colC ## 1 TRUE FALSE TRUE ## 2 FALSE FALSE FALSE ## 3 FALSE FALSE TRUE Then rowAny() would evaluate as: example$rowAnyResult &lt;- rowAny(example) example ## colA colB colC rowAnyResult ## 1 TRUE FALSE TRUE TRUE ## 2 FALSE FALSE FALSE FALSE ## 3 FALSE FALSE TRUE TRUE This outcome will then be used as the filter to return the matching rows. across() is a relatively young function that came with dplyr 1.0.0. It basically allows you to run a function ‘across’ multiple columns. across() has two arguments: the first argument, .cols, is where you want your function to apply to. You could say everything() for all columns, or only run the function on where the column name contains() a certain word, or where(is.numeric)) specifically for numeric columns. the second argument, .fns, is where you specify the function itself. Here, we specify grepl, but make one change: instead of listing a specific column, we instead use .x to indicate which column we want to analyze. In our case, we want to run the function grepl (with arguments) ‘across’ everything() gl %&gt;% filter(rowAny( across(everything(), ~ grepl(searchterms, .x, ignore.case = TRUE)) )) ## # A tibble: 4 x 11 ## je_num amount gl_date vendor_id account invoice_date description paid_date ## &lt;int&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;date&gt; ## 1 743 166. 2019-04-02 77 exp_me… NA Quebec Cit… NA ## 2 743 -166. 2019-04-02 NA liab_c… NA Quebec Cit… NA ## 3 755 9715. 2019-03-25 2211 exp_co… 2019-03-10 Facilitati… 2019-03-25 ## 4 755 -9715. 2019-03-25 NA liab_a… 2019-03-10 Facilitati… 2019-03-25 ## # … with 3 more variables: day_of_week &lt;ord&gt;, t_weekend &lt;lgl&gt;, ## # end_of_month &lt;date&gt; What if we wanted to make individual columns for each test, and sum them up like we do for our standard testing approach? That can be done as well. First, lets make new columns for each of our keywords (and make sure it works) keyword_matches &lt;- gl %&gt;% mutate(t_quebec = rowAny(across(everything(), ~ grepl(&#39;quebec&#39;, .x, ignore.case = TRUE))), t_facilitation = rowAny(across(everything(), ~ grepl(&#39;facilitation&#39;, .x, ignore.case = TRUE))), t_as_per = rowAny(across(everything(), ~ grepl(&#39;as per&#39;, .x, ignore.case = TRUE)))) %&gt;% select(je_num, t_quebec, t_facilitation, t_as_per) %&gt;% filter(t_quebec | t_facilitation | t_as_per) keyword_matches ## # A tibble: 4 x 4 ## je_num t_quebec t_facilitation t_as_per ## &lt;int&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 743 TRUE FALSE FALSE ## 2 743 TRUE FALSE FALSE ## 3 755 FALSE TRUE TRUE ## 4 755 FALSE TRUE TRUE We can also see how many keywords match each row: keyword_matches %&gt;% mutate(t_keywords = rowSums(across(starts_with(&#39;t_&#39;)))) ## # A tibble: 4 x 5 ## je_num t_quebec t_facilitation t_as_per t_keywords ## &lt;int&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 743 TRUE FALSE FALSE 1 ## 2 743 TRUE FALSE FALSE 1 ## 3 755 FALSE TRUE TRUE 2 ## 4 755 FALSE TRUE TRUE 2 9.5.6 Further reading Regular expressions are powerful, yet deeply complicated. Many sources exist to explain regex (1, 2, 3). Many R packages have functions to help you understand and make use of regex as well - check out the stringr and stringi packages for some of these, as well as the help file with ?grep. "],
["report.html", "Chapter 10 Report 10.1 Report Structure 10.2 Export files", " Chapter 10 Report Communicating the results of your testing, from data retireval to conclusions, is essential in having others depend and interpret your results. While traditional methods of documentation include Word memos-to-file, and even Excel documents that are filled to the brim with essays, the audit practice can easily take advantage of a more modern and integrated approach to documentation. Notebooks are a great medium to communicate your results in. Specifically, R Markdown helps you integrate R code alongside auditor commentary, letting you integrate clear explanations and compelling graphics, and publish into multiple mediums - HTML documents, PDFs, presentations like Powerpoint, or even a book if you so desire (like this one)! You can export &amp; attach files that capture your data at a point in time, so others can review your work and data too. 10.1 Report Structure AU-C section 230, Audit Documentation, outlines the general level of documentation expected in an audit working paper. The level of documentation should meet a reperformance standard where an experienced auditor can pick up and be able to replicate your thought process and work flow. From an Audit Data Analytics (ADA) perspective, you should also consider having the following sections in each notebook you produce, inspired by the AICPA’s Guide to Data Analytics(AICPA 2018): The auditor’s name &amp; date Objective - What objective is being met with the procedure. Risks addressed - The risk of material statement is the procedure addressing, at the financial statement level or assertion level. Data source - The sources of the underlying data, and how it was appropriate relative to the procedure performed. How was the data deemed to be reliable (strong Internal Controls over Financial Reporting with IT General Controls). Data extraction - The steps taken to extract the data into the notebook (whether from a database or file). Include an assessment of the data as initially retrieved. Data transformation - The tools, procedures and techniques used to get the data into an analyzable state. Data completeness - The evaluation of ensuring all expected information is included, and has not introduced items not-in-scope or inadvertently left information out. Test procedures - Including the isolation of data and graphical representations. Test results - Any items of note, including deviations from expectations. Session info - The packages and version of R used for the analysis, with sessionInfo(). Within an R Markdown document, you can create sections in your documentation, following the above structure to help you repeat best practices. # Objective # Risks addresses # Data source You can also embed code in the form of a chunk. You would create a chunk with ```{r}, enter in your R code, and end it with ```. So as an example, with our database, you could document the data retrieval procedure with: # Data source The general ledger data source was provided by the ERP service provider, and uploaded to their super secure file storage. ```{r} download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) print(con) # Shows the active database connection ``` ## Control totals We confirmed that the general ledger data had the following characteristics, and matched the control totals as provided by the client: ```{r} gl &lt;- tbl(con, &#39;gl&#39;) %&gt;% collect() nrow(gl) # Number of rows ncol(gl) # Number of columns glimpse(gl) # Preview of column contents ``` When you’re ready to preview your report, all you need to do is ‘knit’ and you’re ready to go. Learn the basics on how to use R Markdown. It is a good idea to start developing R Markdown templates for your team to reuse; we will cover where to R Markdown templates when you are create your own Audit R Package. 10.2 Export files You may wish to keep some data files to aid in file review. Most standard exports for regular consumption would be CSV or Excel files. You can write to a CSV using write.csv(), and the readr package includes a handy write_excel_csv() to prepare files that can be easily read by Excel. library(tidyverse) library(DBI) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) gl &lt;- tbl(con, &#39;gl&#39;) %&gt;% collect() write.csv(gl, &#39;gl.csv&#39;) dbDisconnect(con) Sometimes you may want to retain whole datasets, which can be unwieldy. The arrow package, which is designed for advanced data processing, contains an open source data format called Parquet. The main advantage of Parquet is its ability to write and shrink even the most basic of data structures into a barebone size, making fields portable and quick to read. With the same general ledger data, lets use write_parquet() to export and then compare the sizes to the CSV format: library(arrow) write_parquet(gl, &#39;gl.parquet&#39;) #File comparison csv &lt;- utils:::format.object_size(file.size(&#39;gl.csv&#39;), &quot;auto&quot;) parq &lt;- utils:::format.object_size(file.size(&#39;gl.parquet&#39;), &quot;auto&quot;) print(paste(&#39;File size differences. CSV:&#39;, csv, &#39;- Parquet:&#39;, parq)) ## [1] &quot;File size differences. CSV: 197.1 Kb - Parquet: 39.7 Kb&quot; The size difference is significant, and its not unusual to achieve file size reductions of up to 90%, especially when compared to a CSV output. When you’re ready to read the data back in, you can use read_parquet() to bring it into R. gl2 &lt;- read_parquet(&#39;gl.parquet&#39;) identical(gl, gl2) ## [1] TRUE "],
["applied-audit-analytics.html", "Chapter 11 Applied Audit Analytics", " Chapter 11 Applied Audit Analytics After a team has gotten their hands on performing analytics, there will be consistent wins in the form of delivering audit analytics for individual engagements. Momentum and demand for analytics will build, and the team will be asked to be involved in more audits. There comes a point where an internal audit data analytics team will hit a limit on the team’s effectiveness to deliver work. Some symptoms to watch out for are: Team members using different versions of code, or having their own specialized procedures that aren’t actively shared, resulting in redundancy and best practices not shared. Manually executing the same script, daily, just to ensure the data is locally fresh before it is used. Ad-hoc requests taking a long time to fulfill, due to the special uniqueness and customization needed of each request. Incorporating business actions to findings is always a drawn out affair. The need to send out potential exceptions, wait for response, and integrating responses, hinders any attempt at rapid risk identification. Rules-based testing is reaching a practical limit, with hundreds of if statements that seem to increase response time while being unsure if its contributing to better test results. To effectively scale both capability and responsiveness, it is your job as an innovation leader to balance your teams ability to deliver audit findings, while building the foundation that the team stands on. This requires deliberately thinking about how an individual can break down their work into reusable and functional components, which is a shift from simply getting the job done. With careful consideration of the architecture and tools chosen, the transition to an adept and mature data analytics team is within reach. "],
["auditpackage.html", "Chapter 12 Audit R Package 12.1 Package functions 12.2 Templates", " Chapter 12 Audit R Package In R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others. (J. B. Hadley Wickham 2020) Every time you asked your team to assemble furniture, would you rather have your team learn and makeshift their own flathead screwdriver every single time they solve a problem, or give them a pre-made screwdriver so they can get to assembly? A package is the toolkit of your data analytics team. With it, instead of members learning how to make the tool, they can instead use the customized R package and all its pre-developed audit tools to solve problems. What can be in an R package? Whatever day-to-day challenges that need to be solved! An effective R package that is customized to your team can address many of the following. Data connectivity, including databases and cloud providers Script and R Markdown templates, to help standardize reporting structure Themes, to help promote visualization consistency Documentation of functions Vignettes, which are demonstrations on how to effectively use the package and functions in a real scenario The package that the team develops together must not be a static product, nor does it perform all the work in your team automatically. Whenever a team member encounters a new problem, they will go find a way to develop a solution to solve their task at hand, probably in R Markdown notebook. These learnings and the task it solves stay specific to that notebook, within a project. There are two great resources for learning about packages: R Packages (J. B. Hadley Wickham 2020) My organization’s first R package workshop 12.1 Package functions The evolution of your package is highly dependent on users breaking down their own work and contributing functions, and should be reviewed at the conclusion of each engagement. As your team completes audits within a notebook, you should consider what could you break out so it could be reused elsewhere. One way to think about functions in an R package is ‘what kind of puzzle pieces are needed to make this picture complete?’ where the pieces are the individual functions that give you the full picture. What you would want to do is break down a task you’ve done into a reusable component - that is, generic enough to be called upon any code, but specific enough that it achieves a certain functionality. Lets step through an example of what this could be. Lets say you’ve performed duplicate payment analysis. You need to download your data, clean it, analyze it, and upload the results for follow-up. At a high level, you’ve completed a single notebook: If you crack open each section, you’ll see what you had to perform, which gives you an idea about what functions you may need. For example, you may have downloaded several tables from your accounting software, and a listing of employees from the HR database, amongst the other several procedures you needed to do. If you further dig into the download phase, you’ll see that you had to create two separate database connections to download information from the ERP and also HR database, before it could be used the even download the information. There are some functions that are worth considering to make. The code you can consider turning into a function could be: Database connections to the ERP via Oracle or HR via SQL. Downloading and creating of the vendor master, invoices and payments. Perform everything - Connecting, downloading and joining the information at the same time. Which ones do you end up choosing to implement? This is a design choice, and is known in programming as cohesion and coupling. Cohesion is where a function does one thing, does it incredibly well, and the inputs and outputs of this are passed between functions. Coupling is how dependent the function is on the input data. Ideally you would choose to retain implement functions that are highly cohesive, and have low coupling. This enables you to create independent functions which can act as puzzle pieces towards any problem. Database connections, connect_erp() and connect_hr(), serve as connectivity mechanisms to their respective databases. These would be reasonably dependent on its input data (a username and password), and would be useful functions to implement. They would quite short and sweet too: connect_erp &lt;- function(username, password){ DBI::dbConnect(oracle, host = erp.abc.com, username, password) } The tbl() functions serve well for their purpose, which is to point to a specific table. You can further lock in that knowledge by implementing a tbl_erp_invoices(), tbl_erp_payments() and tbl_hr_emplist(). This is still highly cohesive (does one thing: connects to a data table), but also starts to increase coupling (where it is dependent on the correct database connection provided). The increase in coupling may be reasonable - you may choose accept this design and implement these functions, as it is hard to remember exactly the table names that house the data you are looking for. tbl_erp_invoices &lt;- function(erp){ dplyr::tbl(erp, &#39;ap_1200&#39;) } The last option, connecting and downloading all the data you need, may make a monolithic module that is tailored only for one use case, and it also performs too many things. At first glance, this structure seems to make sense because this is how the notebook was written: all_in_one &lt;- function(username_1, password_1, username_2, password_2){ erp &lt;- DBI::dbConnect(oracle, host = erp.abc.com, username_1, password_1) sql &lt;- DBI::dbConnect(sql, host = hr.abc.com, username_2, password_2) invoices &lt;- dplyr::tbl(erp, &#39;ap_1200&#39;) payments &lt;- dplyr::tbl(erp, &#39;ap_1201&#39;) vendors &lt;- dplyr::tbl(erp, &#39;vm_5000&#39;) emps &lt;- dplyr::tbl(hr, &#39;v_emp_list&#39;) invoices %&gt;% dplyr::semi_join(payments, by = &#39;invoice_id&#39;) } However, now we’re presented with a function that has low cohesion (its doing multiple things) - it is creating the database connection, then downloading and also joining the tables. Not to mention the two sets of credentials that aren’t obviously related to the output: The username and password to the ERP database. The username and password to the HR database. From a function maintenance perspective, it is difficult to maintain: Will you copy and paste the database connections every single time you want to use it elsewhere? What happens when this database changes host locations? What happens when the ERP is upgraded and the table names change? What if you wanted to keep invoices that were both paid and unpaid? While this function technically achieves one highly specific objective, its highly dependent on making sure the company and systems stays constant. With experience, we know the world never stands still. Given the frailness of the design, a function like this should be used sparingly. Ideally, as this is more like a process, the code should be kept in a notebook or as part of an Extract, Transformation and Load process. 12.2 Templates Pre-developed RMarkdown templates help lay out the routine thought processes and deliverables that your team members should go through when performing fieldwork or preparing a report. By having these standardized structures in place, auditors can focus on data exploration and risk related to the specific audit instead of reinvesting the wheel. Within the Reporting Section, we highlight what items are key within an Audit Data Analytics (ADA) notebook. R Markdown templates can be retained within the inst/rmarkdown/templates folder of your package. Specifically, an ADA R Markdown template can be kept at inst/rmarkdown/templates/ada/skeletion/skeleton.Rmd. To create a RMarkdown template, consider how to create templates within R Markdown. --- title: &quot;Audit Data Analytics - &quot; author: &quot;Your name&quot; date: &quot;2021-02-12 20:55:39&quot; output: html_document --- # Objective # Risks addresses # Data source # Data extraction # Data transformation # Data completeness # Test procedures # Test results # Session Info ```{r} sessionInfo() ``` "],
["continuous-monitoring.html", "Chapter 13 Continuous monitoring 13.1 Considerations 13.2 Create a Collection, Analytic and Table 13.3 Define the High Risk test 13.4 Uploading additional results 13.5 Downloading records and responses 13.6 Triggers and Questionnaires", " Chapter 13 Continuous monitoring Continuous Monitoring (CM) and Continuous Auditing (CA, or automated controls testing) relies on the same premise - that a specific entity of interest is being evaluated on a high frequency and regular basis. Both definitions are used inconsistently in the audit industry - I tend to rely on CM for general risk measurement, and CA for controls. Continuous Monitoring is a no brainer - who would say ‘no thanks’ to higher coverage and more tests? Ironically, asking this same question to external auditors can really put them in a bind. Several audit firms do not have explicit guidance on the handling of reliance on internal controls being continuously tested, as current guidance is almost exclusively pointed towards sampling. See the appendix on how to help get started on establishing an agreeable methodology. For this section we will use the company GL database, as well as your Highbond account. If you don’t have one, you can still follow along and abstract where you would incorporate your technology options. library(dplyr) library(tidyr) library(lubridate) library(DBI) library(galvanizer) dir.create(&quot;data&quot;, showWarnings = FALSE) download.file(url = &quot;https://github.com/jonlinca/auditanalytics/raw/master/data/rauditanalytics.sqlite&quot;, destfile = &quot;data/rauditanalytics.sqlite&quot;, mode = &quot;wb&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/rauditanalytics.sqlite&quot;) gl &lt;- tbl(con, &#39;gl&#39;) %&gt;% collect() %&gt;% mutate(gl_date = as_date(gl_date, origin = &#39;1970-01-01&#39;), paid_date = as_date(paid_date, origin = &#39;1970-01-01&#39;), invoice_date = as_date(invoice_date, origin = &#39;1970-01-01&#39;)) %&gt;% select(-gl_date_char) dbDisconnect(con) 13.1 Considerations To effectively implement a CM task, you need to consider the following: Audit application platform - Where you choose to host the completed results and where others interact with your data needs to be secure and accessible to your stakeholders. Sophisticated platforms require audit trails for uploading of information and tracking responses. Close-to-event detection - An exceptions-based CM testing model will need to be ran on a frequent basis, so not only do you need direct access to the data, but a platform to execute the analysis as well. Timely response - Response mechanisms needs to be integrated with your business, where emerging risks and alerts need to be handled effectively to be considered reasonable. Comprehensiveness of evidence - Both the detection mechanism and response both need to be well documented to aid in follow-up inspection by external parties. In the Architecture chapter, we explained the tooling needed to set up a modern audit department. An R workflow is optimal, it will help us automate the analysis of the data, upload new issues and download responses, and monitoring stakeholder responses and the health of the analytic. The serving of these results, however, requires a different skill set and time investment. Standing up your own interactive web platform that allows users to securely respond to your findings, while ensuring security and high availability, requires significant expertise and maintenance. Highbond Results is a Governance, Risk and Compliance platform that enables the CM ecosystem. Highbond Results allows you to create the fully featured Continuous Monitoring ecosystem - once you upload data from tests completed in R, you can take advantage of the automated questionnaires and the ability to record responses securely from stakeholders. Highbond Results also provides security and access controls to the GRC web application, audit trails, and the ability to monitor controls if the responses get stale. To get starting with implementing CM, you will need the following: A data analytic, ingesting data and preparing potential exceptions. You could productionize this in RStudio Connect, enabling you to run a schedule on a frequent basis. A control contact, someone who can inspect exceptions and provide sufficient evidence or response. A Highbond administrator account, which enables you access to the API. A product license to Results. For an individual control, the flow of information may look something like this: 13.2 Create a Collection, Analytic and Table We will create a test to detect high risk journal entries, routing them to the appropriate personnel for response. Detecting and ensuring these journal entries have extra awareness can help give comfort to external parties that journal entries are being reviewed appropriately and in a timely manner. We will need a table upload the data into Highbond Results. You can create a table via following the help instructions on Results app and add a new collection, or you can use the galvanizer package to create the necessary items for you. The galvanizer package is a package for R that ‘wraps’ the Highbond API. An API wrapper is one that translates the endpoints into something more useful for R, helping you understand what kind of requirements are needed to properly interact with the web application. Instead of using a mouse to interact with a website to generate the correct information, you can instead use R programming to achieve the result, gaining speed, full control and consistency. Danger! You’ll be interacting with your live Highbond environment, so be careful and ensure you do not delete or overwrite production data. If possible, use a test or training environment. Before using the API, you will need some core information to access your Highbond instance. They are: Highbond API token Organization number (or instance number) Data Center location To generate a Highbond API token from your Highbond instance by referencing the Highbond help files. Treat your API token like a password! You will require administrative privileges to interact with the API, and thus can perform any action in the system as if you were an administrator. The above information can be used to create a Highbond authentication connection: library(galvanizer) highbond_openapi &lt;- Sys.getenv(&#39;highbond_openapi&#39;) # Replace with your Highbond API token highbond_org &lt;- Sys.getenv(&#39;highbond_org&#39;) # Replace with your organization number highbond_datacenter &lt;- Sys.getenv(&#39;highbond_datacenter&#39;) # Replace with the data center highbond_auth &lt;- setup_highbond(highbond_openapi, highbond_org, highbond_datacenter) You can see if it works by asking the API to pull your currently existing Collections (the folders at the top level of Results): collections &lt;- get_results_collections(highbond_auth) collections ## # A tibble: 9 x 9 ## id type name description default archived collection_type created_at ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1342… coll… galv… &lt;NA&gt; FALSE FALSE RegularProject 2021-01-2… ## 2 1334… coll… galv… &lt;NA&gt; FALSE FALSE RegularProject 2021-01-1… ## 3 1334… coll… galv… &lt;NA&gt; FALSE FALSE RegularProject 2021-01-1… ## 4 1334… coll… galv… &lt;NA&gt; FALSE FALSE RegularProject 2021-01-1… ## 5 1334… coll… galv… &lt;NA&gt; FALSE FALSE RegularProject 2021-01-1… ## 6 1236… coll… galv… &quot;testthat … FALSE FALSE RegularProject 2020-07-0… ## 7 1234… coll… High… &lt;NA&gt; FALSE FALSE ReportingProje… 2020-07-0… ## 8 1234… coll… Refe… &lt;NA&gt; FALSE FALSE ReferenceProje… 2020-07-0… ## 9 1234… coll… Temp… &quot;&lt;i&gt;\\nThis… TRUE FALSE RegularProject 2020-07-0… ## # … with 1 more variable: updated_at &lt;chr&gt; Its important to get a bearing on the id number provided - the number will correspond to the Results collections, analysis or table, depending on the endpoint you use. This is especially important as we will start creating a Collection, an Analysis and also a Table to house our results. First, lets create our top-level folder, the Collection we want to house our analytic in: new_collection &lt;- create_results_collections(highbond_auth, &quot;Audit Analytics Demo&quot;) new_collection ## # A tibble: 1 x 9 ## id type name description default archived collection_type created_at ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1352… coll… Audi… NA FALSE FALSE RegularProject 2021-02-1… ## # … with 1 more variable: updated_at &lt;chr&gt; The id number is important to retain, as its needed to create the Analysis, the sub-folder within the Collection. new_analysis &lt;- create_results_analyses(highbond_auth, new_collection$id[[1]], &quot;Journal entry monitoring&quot;) new_analysis ## # A tibble: 1 x 7 ## id type name description created_at updated_at relationships ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 174053 analy… Journal en… NA 2021-02-13T0… 2021-02-13T… &lt;tibble [1 ×… Lastly, we will want to make a spot to house our results, which will be stored in the Table within the Analysis. When we create the table, we will get a table ID, which will be important as it represents the number that we will reference in all our uploads. new_table &lt;- create_results_tables(highbond_auth, new_analysis$id[[1]], &quot;High value journal entries&quot;) table_id &lt;- new_table$id[[1]] new_table ## # A tibble: 1 x 10 ## id type name description data_table_id script_name table_type created_at ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 5903… tabl… High… NA 1234514 NA data_anal… 2021-02-1… ## # … with 2 more variables: updated_at &lt;chr&gt;, relationships &lt;list&gt; By now, you should see this within your Highbond Results instance. 13.3 Define the High Risk test In the Testing chapters, we ran a test to detect items over a certain threshold. We’re going to use the GL database again to create our test. all_hits &lt;- gl %&gt;% mutate(t_over_te = amount &gt;= 50000) %&gt;% filter(t_over_te) %&gt;% select(je_num, amount, everything()) %&gt;% arrange(gl_date) all_hits ## # A tibble: 11 x 9 ## je_num amount gl_date vendor_id account invoice_date description ## &lt;int&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 102 50135. 2019-01-12 2726 exp_ma… 2019-01-05 Medical gr… ## 2 223 50128. 2019-01-12 2211 exp_ma… 2019-01-05 Packaging … ## 3 379 54127. 2019-05-10 2211 exp_ma… 2019-05-03 Packaging … ## 4 66 51347. 2019-06-07 2726 exp_ma… 2019-05-31 Medical gr… ## 5 555 57258. 2019-06-19 2211 exp_ma… 2019-06-12 Packaging … ## 6 801 54029. 2019-07-03 2211 exp_ma… 2019-06-26 Packaging … ## 7 926 50587. 2019-07-05 2726 exp_ma… 2019-06-28 Medical gr… ## 8 693 88524. 2019-08-13 2726 exp_ma… 2019-08-06 Quality co… ## 9 631 55739. 2019-12-18 2211 exp_ma… 2019-12-11 Packaging … ## 10 140 96355. 2019-12-19 2211 exp_ma… 2019-12-12 Packaging … ## 11 756 57567. 2019-12-30 2726 exp_ma… 2019-12-23 Medical gr… ## # … with 2 more variables: paid_date &lt;date&gt;, t_over_te &lt;lgl&gt; One key difference is that we are going to assume that, in “production” (when the analytic is live and working daily), that we will constantly be uploading records whenever they arise. To help simulate this, we’re only going to upload one row into the Table to start. upload_rows &lt;- all_hits %&gt;% slice(1) upload_rows ## # A tibble: 1 x 9 ## je_num amount gl_date vendor_id account invoice_date description paid_date ## &lt;int&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;date&gt; ## 1 102 50135. 2019-01-12 2726 exp_ma… 2019-01-05 Medical gr… 2019-01-15 ## # … with 1 more variable: t_over_te &lt;lgl&gt; Lets use galvanizer and upload this record into the Highbond Results Table. Using the post_results_records() function, we need to specify the Highbond authentication token, the table_id we want to upload the information into, and the data frame itself via the upload argument. Since it is the inaugural upload, we also need to specify the purge = TRUE argument so we have a clean slate: post_results_records(highbond_auth, table_id = table_id, upload = upload_rows, purge = TRUE) Upon checking the Results Table on the website, we see that the record was added. Once we uploaded the information, we should specify the primary key in the Table’s settings. The primary key’s purpose in Highbond Results is to ensure that this record stays persistent. When you upload another batch of records, if a ‘primary key’ is defined, then Highbond will merge the information in the new upload, but not automatically repeat any triggers that have been already acted on (example: if you already sent out a request for a response via a questionnaire, it will not occur again). In our scenario, we will want to ask the control owner about a specific journal entry that is high risk. The unique identifier for every journal entry is the je_num, so its probably useful to also make this the primary key. To assign this field as the primary key, within the list of Tables in Highbond, select the ellipses (three dots) corresponding to the Table, and select Settings. Within there, specify the je_num as the primary key. 13.4 Uploading additional results As we intend for this to be a continiously monitored control, we should make sure we can upload additional rows, simulating new results over time. First, lets get the next batch of records: upload_rows &lt;- all_hits %&gt;% slice(2:3) upload_rows ## # A tibble: 2 x 9 ## je_num amount gl_date vendor_id account invoice_date description paid_date ## &lt;int&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;date&gt; ## 1 223 50128. 2019-01-12 2211 exp_ma… 2019-01-05 Packaging … 2019-02-11 ## 2 379 54127. 2019-05-10 2211 exp_ma… 2019-05-03 Packaging … 2019-06-09 ## # … with 1 more variable: t_over_te &lt;lgl&gt; We can use the same post_results_records() function to upload new records, but we’ll make one change: remove the purge argument: post_results_records(highbond_auth, table_id = table_id, upload = upload_rows) 13.5 Downloading records and responses Once information has been uploaded, you can also retrieve the information locally for further analysis, using the `get_results_records(): cm_control &lt;- get_results_records(highbond_auth, table_id = table_id) This data is returned in a nested list, with technical information about the data downloaded and the table columns. You may poke around to see what was returned, or just get the Table records: cm_control$content$data %&gt;% select(je_num, amount, gl_date, vendor_id, account, invoice_date, description, paid_date, t_over_te) ## je_num amount gl_date vendor_id account invoice_date ## 1 102 50134.79 2019-01-12 2726 exp_materials_6000 2019-01-05 ## description paid_date t_over_te ## 1 Medical grade filters 2019-01-15 TRUE ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 2 rows ] As the Result tables are enriched with questionnaire responses from control owners, the information can be downloaded in the same manner - these questionnaires will automatically be created as new columns, prefixed with q_. 13.6 Triggers and Questionnaires Within Results, the Triggers and Questionnaires features are useful in the continuous monitoring framework. You may wish to define the following to complete your architecture: A questionnaire, asking for a response from the control owner. An effectively designed questionnaire can help determine if the control owner has fully evaluated the new and developing situation. An initial trigger for a new record, that is sent to a control owner asking for a response. A follow-up trigger for stale records, that is sent to the audit team member who is monitoring the control for responsiveness. "],
["applied-datamart.html", "Chapter 14 Audit Data Mart 14.1 What to ETL 14.2 ETL Essentials 14.3 ETL Example", " Chapter 14 Audit Data Mart As auditors, you will likely pull from the general pool of data at your company to perform analysis. This information is likely organized in a data warehouse, where data quality is controlled and is oriented towards reporting. Sometimes, this information is simply too large and onerous for rapid reporting. A side effect of a well designed data warehouse is that the data is highly structured (normalized), and performing any trivial analysis requires joining multiple tables together. Data marts have existed to provide a smaller, more defined subset of information, that is developed with a specific need in mind. Generally a data mart is connected to the data warehouse, preparing the data via a process called extraction, transformation and load (ETL). Extraction is the retrieval of information from a flat file or database, transformation is the act of cleaning, joining and summarizing the information so it can be useful, and load is uploading the information into a database. A data mart is generally considered to be relatively inexpensive, as you are only retaining specific information needed to answer most questions. It won’t replace the data warehouse, especially for historical transactions, but it is good for smaller snapshots of information. 14.1 What to ETL Good candidates for content in a data mart is generally information that takes a long time to generate for any individual’s computer, and would be useful to multiple individuals. Specifically: Where multiple databases (or tables) need to be combined before they offer any meaningful value. For example, connecting the expense report system to the company’s travel agency to match flights and hotels may help validate that travel policies are adhered to. Information that needs to be rapidly presented and consumed for an ongoing report. For example, an audit KPI monitoring new risks within a Continuous Monitoring program may be prepared in advance, due to the complexity of calculations needed over a period of time. Data that is considered ‘master’ or ‘reference.’ A listing of account names and numbers may be useful to have on hand as it is relevant to any audit. In contrast, a list of transactions may only be needed for a single audit inquiry. You can use any database that your company provides. It will be important that you have the ability to create tables within this database, delete them, and ensure you can work with someone who can administer the security to prevent non-audit users from querying the database. 14.2 ETL Essentials In the Architecture chapter, we referenced using RStudio Connect to facilitate the process to extract, transform and load (ETL). RStudio Connect relies on R Markdown notebooks to execute analytic processes, which we can design and prepare locally before scheduling. The audit team will likely be maintaining this audit data mart - therefore, some additional considerations need to be thought about: While R Markdown notebooks are self documenting, having well outlined documentation that explains where the data came from, how it was transformed and where it ends up being will be useful to your teammates and future self. Secure your credentials. Do not place them directly in the notebook as plain text. If using RStudio Connect, call passwords via an environment variable. Ensure proper checks are in place. Actively scan for data structure anomalies and catch them before they get uploaded, and ensuring that data that was downloaded is fresh by examining dates. Where possible, avoid overwriting a table completely each time by replacing it with the fresh information that was just extracted. While in some situations this is reasonable, the trade-off between simpler design and the speed of the ETL will become more glaring as the dataset grows. A well designed ETL has three separate and explicitly delineated components: Extraction - Code in this section should be restricted to the query and download of the data from the source system. Ideally, this code should be limited to performing operations on the database or application related to querying, and not creating or cleaning new fields. Transformation - Once the data has been downloaded, the enrichment of the fields can then occur. This includes joining tables, filtering on fields, and creation of new fields. Load - Once the data is clean, it can then be uploaded into the Audit Data Mart. As part of the Load process, a before and after control count should be performed, to ensure the data was uploaded into the database correctly. 14.3 ETL Example Below is a framework of what may be contained in an ETL RMarkdown notebook. Having an example will help you ensure consistency between ETLs, and allow others to create ETLs for their own processes. --- title: &quot;Audit Data Mart - ETL 111 HR&quot; author: &quot;Your name&quot; date: &quot;2021-02-12 20:56:01&quot; output: html_document --- # Credentials ```{r} passwords &lt;- Sys.getenv(&quot;dbpassword&quot;) ``` # Extract ```{r} con &lt;- connect_hr(passwords) employee_list &lt;- get_hr_employees(con) reporting_structure &lt;- get_hr_reporting(con) ``` # Extract check Check on the latest termination date. Usually we have a termination every week. ```{r} days_since_last_term &lt;- Sys.date - max(employee_list$term_date) if (days_since_last_term &gt; 14) {warning(&quot;Danger!&quot;)} ``` # Extract summary ```{r} nrow(employee_list) nrow(reporting_structure) ``` # Transform ```{r} cleaned_staff &lt;- employee_list %&gt;% left_join(reporting_structure, by = &#39;employee_id&#39;) %&gt;% # Gets the ID number of the supervisor of the employee left_join(employee_list, by = &#39;supervisor_id&#39;) # Gets the name of the supervisor ``` # Transform check Check that every employee has a boss ```{r} no_boss &lt;- cleaned_staff %&gt;% filter(is.na(supervisor_id)) if (nrow(no_boss) &gt; 0) {warning(&quot;Danger!&quot;)} ``` # Transform summary ```{r} nrow(cleaned_staff) ``` # Load ```{r} con &lt;- connect_audit_data_mart(passwords) dbWriteTable(con, &#39;staff_table&#39;, cleaned_staff, overwrite = TRUE) ``` # Load check Check that our new table matches the ETL table we just created ```{r} new_count &lt;- dbReadTable(con, &#39;cleaned_staff&#39;) nrow(new_count) if (nrow(new_count) != nrow(cleaned_staff)) {warning(&quot;Danger!&quot;)} ``` # Session Info ```{r} sessionInfo() ``` "],
["machine-learning.html", "Chapter 15 Machine Learning 15.1 Goal: Predict an account class 15.2 Generating features 15.3 Training the model 15.4 Making predictions 15.5 What to explore next", " Chapter 15 Machine Learning One of the utopian visions of an advanced audit analytics shop is the application of predictive modeling to help drive audit effectiveness. Machine learning (and data science generally) is an advanced and complex topic, and while it is rooted in statistical methods, can be summarized as “using a computer to discover patterns in a dataset.” Machine learning employs the use of algorithms, which are statistical methods to optimize one specific goal: error. Specifically, an algorithm will try to minimize error on a training dataset - that is, each and every time the algorithm makes a prediction, it compares its estimated result against the truth. As it performs the comparison and iterates continuously to reduce the error for each individual sample, the algorithm is also ensuring it is generally applicable for an entire training population. Similar to how you don’t need to understand the thermodynamics of a combustion engine to drive a car, you don’t need to necessarily understand how to make an algorithm to use machine learning. However, as an auditor, you still need to understand the tools and algorithm intentions - you will be a lot more fruitful using a screwdriver to insert a screw instead of a hammer. To successfully apply machine learning, an audit practitioner needs to clearly state what needs to be predicted, with what data, and how to evaluate it. The real work is understanding the problem and the data, wrangling it into a proper format, choosing the right algorithm to apply against the data, and interpreting the results. It doesn’t take a lot to train a model (really!). Lets use an example to help illustrate how you could use machine learning in an audit situation. We will use the following packages: library(tidyverse) library(tidymodels) library(tidytext) library(tm) library(rpart) library(rpart.plot) library(gt) # Used for formatting table outputs set.seed(12345) 15.1 Goal: Predict an account class Lets say we were working on an audit to help determine if a transaction will be eventually charged against the correct account class. Specifically, lets see if we can predict if a transaction is a fixed asset, based on the description field itself. First, we’ll get some information from our dataset, which will act as our training data. We collect a bit of information, which includes the description of a GL account, as well as whether or a specific journal entry was booked to a fixed asset account. description &lt;- c(&#39;New computer&#39;, &#39;New computer to replace old unit&#39;, &#39;New computer user training class&#39;, &#39;Computer repair&#39;) is_fixed_asset &lt;- as.factor(c(TRUE, TRUE, FALSE, FALSE)) je_num &lt;- 1:length(description) transactions &lt;- data.frame(je_num, description, is_fixed_asset) transactions ## je_num description is_fixed_asset ## 1 1 New computer TRUE ## 2 2 New computer to replace old unit TRUE ## 3 3 New computer user training class FALSE ## 4 4 Computer repair FALSE We will explore the use of a tree-based basic decision tree, predicting a classification outcome. Different approaches exist to predict outcomes, including decision trees and neural networks. Predictions can either take the form of predicting a class or label via classification, or a number via regression. A decision tree is one of the easiest to interpret, as it logically follows a flow - start a the top and answer a series of ‘yes-no’ questions to determine what the final result should be. You can see there are multiple words that describe each transaction. In our example, “computer” is the running theme here, but they all seem to have different words that describe the transaction itself. We plan on using these words as our ‘yes-no’ branches to help us predict our outcomes. This is a classification problem. 15.2 Generating features Any algorithm requires the generation of features - that is, indicators within the data that the algorithm can use to leverage and predict the outcome. Features typically need to be generated and be made into its own column, before they can leveraged in a prediction. In our example, we have several full sentences. Unfortunately, if we feed this into the model, the whole sentence is actually interpreted as one whole feature, which isn’t terribly useful; the individual words themselves are more important for prediction. To use the individual words, we need to tokenize the sentence - that is, break each sentence into individual words, so they can be used as part of our predictions. The tidytext package is great for this, as it helps us break down each word into its own sentence. First, lets create a lost of tokens - i.e. what was the unique word within each description, and how often did each word occur. words &lt;- transactions %&gt;% unnest_tokens(word, description) %&gt;% count(je_num, word, sort = TRUE) words ## je_num word n ## 1 1 computer 1 ## 2 1 new 1 ## 3 2 computer 1 ## 4 2 new 1 ## 5 2 old 1 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 10 rows ] We can see that each journal entry line is now repeated for each and every unique word. However, we notice some filler words, which don’t add a lot of value in our predictions. There are called ‘stop words’ which are common words in the English language - great for communicating verbally, but computers don’t need such formalities. Here were the stop words detected: transactions %&gt;% unnest_tokens(word, description) %&gt;% filter(word %in% stop_words$word) ## je_num is_fixed_asset word ## 1 1 TRUE new ## 2 2 TRUE new ## 2.2 2 TRUE to ## 2.4 2 TRUE old ## 3 3 FALSE new We could make our own list of stop words - for now, lets drop the default suggestions. words &lt;- transactions %&gt;% unnest_tokens(word, description) %&gt;% filter(!word %in% stop_words$word) %&gt;% # Remove stop words. count(je_num, word, sort = TRUE) words ## je_num word n ## 1 1 computer 1 ## 2 2 computer 1 ## 3 2 replace 1 ## 4 2 unit 1 ## 5 3 class 1 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 5 rows ] With a cleaned layout, we can now better see the valuable words per journal entry. However, its a bit difficult to see what words were shared between each journal entry, so lets lay it out wide and aggregate it so each word is in its own column, aligned with the journal entry. wide_words &lt;- words %&gt;% pivot_wider(id_cols = je_num, names_from = word, values_from = n) wide_words ## # A tibble: 4 x 8 ## je_num computer replace unit class training user repair ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 NA NA NA NA NA NA ## 2 2 1 1 1 NA NA NA NA ## 3 3 1 NA NA 1 1 1 NA ## 4 4 1 NA NA NA NA NA 1 Excellent, lets join this back to our dataset and do the final cleanup to start making predictions. trans_listing &lt;- transactions %&gt;% inner_join(wide_words, by = &#39;je_num&#39;) trans_listing %&gt;% gt() # For making our output prettier html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nspzxzkxbv .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #nspzxzkxbv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nspzxzkxbv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #nspzxzkxbv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #nspzxzkxbv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nspzxzkxbv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nspzxzkxbv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #nspzxzkxbv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #nspzxzkxbv .gt_column_spanner_outer:first-child { padding-left: 0; } #nspzxzkxbv .gt_column_spanner_outer:last-child { padding-right: 0; } #nspzxzkxbv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #nspzxzkxbv .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #nspzxzkxbv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #nspzxzkxbv .gt_from_md > :first-child { margin-top: 0; } #nspzxzkxbv .gt_from_md > :last-child { margin-bottom: 0; } #nspzxzkxbv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nspzxzkxbv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nspzxzkxbv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nspzxzkxbv .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nspzxzkxbv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nspzxzkxbv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nspzxzkxbv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #nspzxzkxbv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nspzxzkxbv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nspzxzkxbv .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #nspzxzkxbv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nspzxzkxbv .gt_sourcenote { font-size: 90%; padding: 4px; } #nspzxzkxbv .gt_left { text-align: left; } #nspzxzkxbv .gt_center { text-align: center; } #nspzxzkxbv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nspzxzkxbv .gt_font_normal { font-weight: normal; } #nspzxzkxbv .gt_font_bold { font-weight: bold; } #nspzxzkxbv .gt_font_italic { font-style: italic; } #nspzxzkxbv .gt_super { font-size: 65%; } #nspzxzkxbv .gt_footnote_marks { font-style: italic; font-size: 65%; } je_num description is_fixed_asset computer replace unit class training user repair 1 New computer TRUE 1 NA NA NA NA NA NA 2 New computer to replace old unit TRUE 1 1 1 NA NA NA NA 3 New computer user training class FALSE 1 NA NA 1 1 1 NA 4 Computer repair FALSE 1 NA NA NA NA NA 1 15.3 Training the model Before we can make predictions, we need to consider what we features we want to use as part of training. Training a model uses the features we’ve generated to discover a pattern and create a model that best fits the training data. Lets go through a few of these columns to determine what to retain and remove: Our je_num is not useful because it is simply a reference, and it would not make sense that a number in a journal entry could dictate the outcome. The original description, in its original form, would not be useful. We do not expect to see some of these whole sentences verbatim repeated again. The individual words that were generated from the description, make sense to keep, as they will be part of our predictive model. We also need to do some final cleanup: The is_fixed_asset will be what we want to predict. In this case, it should be a factor, as we are guessing a state outcome as part of the classification exercise. Many algorithms tend to break when they see NAs. In our dataset, we understand that NA means that the word was not seen, so a zero as a replacement will suffice. training_data &lt;- trans_listing %&gt;% select(-c(je_num, description)) %&gt;% mutate(is_fixed_asset = as.factor(is_fixed_asset)) %&gt;% replace(is.na(.), 0) training_data %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #kfgjqhgfal .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #kfgjqhgfal .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kfgjqhgfal .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #kfgjqhgfal .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #kfgjqhgfal .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kfgjqhgfal .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kfgjqhgfal .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #kfgjqhgfal .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #kfgjqhgfal .gt_column_spanner_outer:first-child { padding-left: 0; } #kfgjqhgfal .gt_column_spanner_outer:last-child { padding-right: 0; } #kfgjqhgfal .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #kfgjqhgfal .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #kfgjqhgfal .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #kfgjqhgfal .gt_from_md > :first-child { margin-top: 0; } #kfgjqhgfal .gt_from_md > :last-child { margin-bottom: 0; } #kfgjqhgfal .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #kfgjqhgfal .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #kfgjqhgfal .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kfgjqhgfal .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #kfgjqhgfal .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kfgjqhgfal .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #kfgjqhgfal .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #kfgjqhgfal .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kfgjqhgfal .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kfgjqhgfal .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #kfgjqhgfal .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kfgjqhgfal .gt_sourcenote { font-size: 90%; padding: 4px; } #kfgjqhgfal .gt_left { text-align: left; } #kfgjqhgfal .gt_center { text-align: center; } #kfgjqhgfal .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #kfgjqhgfal .gt_font_normal { font-weight: normal; } #kfgjqhgfal .gt_font_bold { font-weight: bold; } #kfgjqhgfal .gt_font_italic { font-style: italic; } #kfgjqhgfal .gt_super { font-size: 65%; } #kfgjqhgfal .gt_footnote_marks { font-style: italic; font-size: 65%; } is_fixed_asset computer replace unit class training user repair TRUE 1 0 0 0 0 0 0 TRUE 1 1 1 0 0 0 0 FALSE 1 0 0 1 1 1 0 FALSE 1 0 0 0 0 0 1 There are several different ways to call a model. We will use the tidymodels package, which helps us declare which model we plan on using, the algorithm within the model, and the outcome. dt_model &lt;- decision_tree(min_n = 1) %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) To train a model, we simply take the model we want to use (as determined above using the decision_tree() specification), the outcome is_fixed_asset, and the data source we want to train on. dt_fit &lt;- dt_model %&gt;% fit(is_fixed_asset ~ ., data = training_data) dt_fit ## parsnip model object ## ## Fit time: 7ms ## n= 4 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 4 2 FALSE (0.5000000 0.5000000) ## 2) replace&lt; 0.5 3 1 FALSE (0.6666667 0.3333333) ## 4) class&gt;=0.5 1 0 FALSE (1.0000000 0.0000000) * ## 5) class&lt; 0.5 2 1 FALSE (0.5000000 0.5000000) ## 10) repair&gt;=0.5 1 0 FALSE (1.0000000 0.0000000) * ## 11) repair&lt; 0.5 1 0 TRUE (0.0000000 1.0000000) * ## 3) replace&gt;=0.5 1 0 TRUE (0.0000000 1.0000000) * Without much fanfare, we get a model we can now use. Lets look into a bit more detail as to how the splits take place in our trained model: rpart.plot(dt_fit$fit) Reading a decision tree takes some orienting. The way to read the root (the top level decision) is to say “is the value of replace less than 0.5?” - yes to go left, and no to go right. Another way to interpret the node is “did the word replace occur less than 0.5 times?” If it did happen less than 0.5 times (which is 0 in our dataset), then go left. However, if the word ‘replace’ had a value of more than 0.5, which also means the word occurred at least once, then go right - in this case the predicted outcome for is_fixed_asset will be TRUE. The branches, aka the items below the root, are decided via a gini impurity metric, where the goal is to pose a question that evenly splits the outcomes into 50/50 buckets. The more evenly split the information, the more ‘information gain’ was acquired from the split. 15.4 Making predictions Now that we have a trained model, lets try to make some predictions. Lets say we had three more sentences that were tokenized into: ‘class repair,’ ‘replace repair,’ ‘replace class,’ ‘computer computer computer,’ and simply nothing. If we went through the process again to tokenize, our new dataset (holdout, since it was not used in training) would look like this: holdout_data &lt;- data.frame(matrix(nrow = 1, ncol = ncol(training_data) -1)) colnames(holdout_data) &lt;- names(training_data)[2:ncol(training_data)] holdout_data[1,] &lt;- c(0, 0, 0, 1, 0, 0, 1) holdout_data[2,] &lt;- c(0, 1, 0, 0, 0, 0, 1) holdout_data[3,] &lt;- c(0, 1, 0, 1, 0, 0, 0) holdout_data[4,] &lt;- c(3, 0, 0, 0, 0, 0, 0) holdout_data[5,] &lt;- c(0, 0, 0, 0, 0, 0, 0) holdout_data %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #inmtjwppbt .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #inmtjwppbt .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #inmtjwppbt .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #inmtjwppbt .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #inmtjwppbt .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #inmtjwppbt .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #inmtjwppbt .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #inmtjwppbt .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #inmtjwppbt .gt_column_spanner_outer:first-child { padding-left: 0; } #inmtjwppbt .gt_column_spanner_outer:last-child { padding-right: 0; } #inmtjwppbt .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #inmtjwppbt .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #inmtjwppbt .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #inmtjwppbt .gt_from_md > :first-child { margin-top: 0; } #inmtjwppbt .gt_from_md > :last-child { margin-bottom: 0; } #inmtjwppbt .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #inmtjwppbt .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #inmtjwppbt .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #inmtjwppbt .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #inmtjwppbt .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #inmtjwppbt .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #inmtjwppbt .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #inmtjwppbt .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #inmtjwppbt .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #inmtjwppbt .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #inmtjwppbt .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #inmtjwppbt .gt_sourcenote { font-size: 90%; padding: 4px; } #inmtjwppbt .gt_left { text-align: left; } #inmtjwppbt .gt_center { text-align: center; } #inmtjwppbt .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #inmtjwppbt .gt_font_normal { font-weight: normal; } #inmtjwppbt .gt_font_bold { font-weight: bold; } #inmtjwppbt .gt_font_italic { font-style: italic; } #inmtjwppbt .gt_super { font-size: 65%; } #inmtjwppbt .gt_footnote_marks { font-style: italic; font-size: 65%; } computer replace unit class training user repair 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 And the fun part - lets run our predictions through the model we created: dt_predict &lt;- predict(dt_fit, holdout_data) dt_predict &lt;- bind_cols(holdout_data, dt_predict) dt_predict %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #iydhkfieos .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #iydhkfieos .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #iydhkfieos .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #iydhkfieos .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #iydhkfieos .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #iydhkfieos .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #iydhkfieos .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #iydhkfieos .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #iydhkfieos .gt_column_spanner_outer:first-child { padding-left: 0; } #iydhkfieos .gt_column_spanner_outer:last-child { padding-right: 0; } #iydhkfieos .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #iydhkfieos .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #iydhkfieos .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #iydhkfieos .gt_from_md > :first-child { margin-top: 0; } #iydhkfieos .gt_from_md > :last-child { margin-bottom: 0; } #iydhkfieos .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #iydhkfieos .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #iydhkfieos .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #iydhkfieos .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #iydhkfieos .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #iydhkfieos .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #iydhkfieos .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #iydhkfieos .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #iydhkfieos .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #iydhkfieos .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #iydhkfieos .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #iydhkfieos .gt_sourcenote { font-size: 90%; padding: 4px; } #iydhkfieos .gt_left { text-align: left; } #iydhkfieos .gt_center { text-align: center; } #iydhkfieos .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #iydhkfieos .gt_font_normal { font-weight: normal; } #iydhkfieos .gt_font_bold { font-weight: bold; } #iydhkfieos .gt_font_italic { font-style: italic; } #iydhkfieos .gt_super { font-size: 65%; } #iydhkfieos .gt_footnote_marks { font-style: italic; font-size: 65%; } computer replace unit class training user repair .pred_class 0 0 0 1 0 0 1 FALSE 0 1 0 0 0 0 1 TRUE 0 1 0 1 0 0 0 TRUE 3 0 0 0 0 0 0 TRUE 0 0 0 0 0 0 0 TRUE Some of the predictions could be conferred as such, but whats up with the zero-words example that predicted it was a fixed asset? 15.5 What to explore next In our extremely simplistic example, we overlooked several things that would not be done on a real data set. A larger main dataset needs to be split into a training dataset and a test dataset. The creation of the model should be done on the training data, and its performance should be evaluated against the test dataset. The performance will allow you to evaluate just how good the model is. If this is not done, then your model is simply overfitting, which is the same as memorizing a dataset, and has no ability to generalize against a normal population. For training datasets - sizes matter! An extremely small training set means the model hasn’t seen enough nuances in the data to understand it. Since these seven words never mapped to anything in the decision tree, it always evaluated as TRUE. The quality of a training set also matters greatly, as the training dataset is considered truth. Its very difficult to tell a computer to ignore any mislabelled data, as it has no capacity to know what is mislabelled. The features used in training the model must also be the features for the prediction. For example, if we start seeing the word “Mac” instead of “Computer,” we either need to drop the word “Mac” before running our prediction, or retrain our model with these new keywords. In our case, since we’re using individual words, additional work needs to be done to align the words in the same format as our training dataset. This could be solved by retraining the model as new features develop. From an algorithm perspective, a decision tree makes only one tree as its final outcome. However, there could have been a tree design that put another feature as the root and other trees below it. One way to address this weakness is to use random forest or xgboost, which are other tree-based algorithms that make many trees and make a final model based on a combination of the best tree made (ensemble). Some algorithms offer the ability to tune hyperparameters - that is, the ability to customize an algorithm to help tune and optimize itself. Various algorithms offer different options. For tree-base algorithms, it could be the condition of making each branch. For regression algorithms, it could be the size of a ‘step’ taken when testing a new formula to minimize error; larger steps may avoid local minimums, but may not find optimal minimums due to their lack of sensitivity. Tuning hyperparameters is so powerful, that using just a single algorithm but a well-tuned model can yield significantly better results. There are several other machine learning algorithms out there, as well as regression-based predictions. There is “no free lunch,” and no singular model will work in every situation. The beauty of a code-based approach is that you can try them all in a relatively effective manner. In our example, we used text miniming as part of our feature generation. If you wanted to fully explore text mining, read Text Mining with R by Julia Silge &amp; David Robinson. We only explored the words themselves - there are additional branches of text mining that explore topic modelling, stemming, sentiment, among others approaches. "],
["appendix-ca.html", "Chapter 16 Continuous Auditing methodology 16.1 Support for Continuous Auditing 16.2 The requirements for evaluting ICFR 16.3 Determining population tolerable error rate", " Chapter 16 Continuous Auditing methodology 16.1 Support for Continuous Auditing There is a lot of hype around continuous controls monitoring, and how it will automate or eliminate the need for manual testing or sampling. Regulations have struggled to keep up with the view of continuous testing, despite independent advancements resulting in greater assurances of internal control through these advanced testing mechanisms. This should not discourage your team in applying analytics on a continuous basis. Rather, it gives you a point to open up conversation with external auditors about how the tests of controls offer greater assurance at a higher degree of efficiency. 16.2 The requirements for evaluting ICFR The Security and Exchange Commission (SEC) provides oversight to the Public Company Accounting Oversight Board (PCAOB), and the PCAOB provides many of the Audit Standards that accounting firms interpret. The SEC explicitly avoids detailing guidance and providing examples to test controls, encouraging companies to design controls to mitigate the risk to the reliability of financial reporting. (“Commission Guidance Regarding Management’s Report on Internal Control over Financial Reporting Under Section 13(a) or 15(d) of the Securities Exchange Act of 1934; Final Rule” 2007) As a result, if we take a first principles approach, “The objective of internal control over financial reporting (”ICFR“) is to provide reasonable assurance regarding the reliability of financial reporting and the preparation of financial statements for external purposes in accordance with generally accepted accounting principles (”GAAP“).” (section A) Monitoring activities, including “controls to monitor results of operations and controls to monitor other controls”, also help address ICFR (A.1.b), and “For any individual control, different combinations of the nature, timing, and extent of evaluation procedures may provide sufficient evidence.” (A.2). More specifically, the evidence of the ICFR is allowed to come from on-going monitoring activities (A.2.b), and even to the point where “evidence from on-going monitoring is sufficient and that no further direct testing is required.” (A.2.b) This enables us to critically consider the usage of a strong audit data analytics program in the evaluation of the ICFR. The onus then, is on you to demonstrate high levels of objectivity and competency of the individuals and team for the monitoring program: “Management’s on-going monitoring activities may provide sufficient evidence when the monitoring activities are carried out by individuals with a high degree of objectivity.” (E.2) AS 2201 does allow for the use of entity level controls as conducted by internal audit (.24) (“AS 2201: An Audit of Internal Control over Financial Reporting That Is Integrated with an Audit of Financial Statements” 2007) to lower the risk of other controls (.47), and providing sufficient documentation and evidence of operation around the program (.45) may be enough to contribute to the operating effectiveness and evaluation of a control and risk. 16.3 Determining population tolerable error rate You should define the methodology you will use to Continuously Audit control operations. The reason to have these conversations with your external auditors is because testing 100% of the population will invariably find a deviation. It is inevitable, as any control that involves people (or even systems, to an extent) may trip at some point. Finding one failure in a population of several hundred transactions should not be detrimental - however, to some professionals, truly accepting deviations is a difficult fact to accept. You can help them with this - AS 2201 does allow for individual controls to have deviations, yet be considered effective (.48). One key method to help acknowledge that errors could indeed exist is through clarifying the population tolerable error rate - i.e. the maximum rate of deviations of a prescribed control (.34) (“AS 2315: Audit Sampling” (2015)). This will be based upon the control risk that the audit team decides. In addition, when deviations are found, it does not ultimately result in a misstatement either (.35), as there would have to be sufficient evidence proven that all assertions were not met. Typical sampling asks the auditor to consider control risk, or the risk of incorrect acceptance - i.e. the risk that a control can be deemed effective when it is not. However, when an entire population is considered, this risk effectively reduces to zero, as there is no sampling conducted. Traditionally, sample sizes are calculated when you know the population tolerable error rate, and the risk of incorrect acceptance. Where \\(\\beta\\) is the risk of incorrect acceptance, \\(p_{t}\\) is the population tolerable error rate, and \\(n\\) is the sample size, we can derive the sample size necessary to satisfy audit sampling. (Stewart (2008)) We assume an expectation of no errors in the population with the below: \\[ n = \\frac{ln(\\beta)}{ln(1 - p_{t})} \\] In R, if we were to determine the sample size necessary with a risk of incorrect acceptance of 5% and tolerable error percentage of 5%, our sample size would be: b &lt;- 0.05 pt &lt;- 0.05 log(b) / log(1 - pt) ## [1] 58.40397 If it is difficult to ascertain what the population tolerable error rate may be, it is possible to work backwards from both a sample size and an assumption on the control risk. Derived from Sampling Guide technical notes (Stewart (2008)), the population tolerable error rate can be defined as: \\[ p_{t} = -e^\\frac{ln(\\beta)}{n} + 1 \\] From the previous example, lets say our sample size was 59, and our risk of incorrect acceptance was 10%, we can estimate the population tolerable error rates. n &lt;- 59 b &lt;- 0.05 -exp(1)^(log(b) / n) + 1 ## [1] 0.04950761 With this, your methodology could be derived from existing sampling, and you could set your threshold of control operating effectiveness, when tested on the full population, to accept 5% of samples with errors. "],
["other-practices-to-follow.html", "Chapter 17 Other practices to follow 17.1 Documentation 17.2 Passwords", " Chapter 17 Other practices to follow 17.1 Documentation As you start, you should promote basic habits instilled into you. Documenting your basic thought process in .R files is generally expected, and the ‘why’ a certain process outlined with comments (lines starting with a #). The why is important as it explains to code reviewers (external auditors and your peers) the rationale for your approach, or unusual quirks about the data you are transforming. R Markdown files become valuable as communication mediums for reports, allowing you to embed a mix of code, graphics, and interactive tables. While most of the exploratory work can be done within a basic .R file, having the ability to readily ‘knit’ a document for sharing increases the people you can share your work with. Whether you are using R or R Markdown files, its convenient to have these files as your primary sources of editing as you can use the keyboard shortcuts command-return or control-enter to send a command from the script file to the console. 17.2 Passwords As an Auditor, you should ensure you are securing the passwords you use to access databases or Highbond. There are several methods for securing passwords: You can ask the user to specify the password every single time an analytic is ran, Use the .Renviron file, which allows you to specify environment variables that you call with Sys.getenv(), Use the keyring package, leveraging the operating system’s method for securing credentials, Use a password manager, like 1Password, that is integrated into R via the onepass package. "],
["references.html", "References", " References AICPA. 2018. Guide to Data Analytics. Newark: John Wiley &amp; Sons, Incorporated. “AS 2201: An Audit of Internal Control over Financial Reporting That Is Integrated with an Audit of Financial Statements.” 2007. https://pcaobus.org/Standards/Auditing/Pages/AS2201.aspx. “AS 2315: Audit Sampling.” 2015. https://pcaobus.org/Standards/Auditing/Pages/AS2315.aspx. “Commission Guidance Regarding Management’s Report on Internal Control over Financial Reporting Under Section 13(a) or 15(d) of the Securities Exchange Act of 1934; Final Rule.” 2007. https://www.sec.gov/rules/interp/2007/33-8810fr.pdf. Crawford, Matthew B. 2009. Shop Class as Soulcraft: An Inquiry into the Value of Work. Hadley Wickham, Garrett Grolemund. 2017. R for Data Science. https://r4ds.had.co.nz/index.html. Hadley Wickham, Jennifer Bryan. 2020. R Packages. https://r-pkgs.org. Identifying Anomalies Using the Relative Size Factor Test. 2012. John Wiley &amp; Sons, Ltd. https://doi.org/10.1002/9781118386798.ch11. Identifying Fraud Using Abnormal Duplications Within Subsets. 2012. John Wiley &amp; Sons, Ltd. https://doi.org/10.1002/9781118386798.ch12. Kahneman, Daniel. 2011. Thinking, Fast and Slow. NIST/SEMATECH e-Handbook of Statistical Methods. n.d. https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm. Stewart, Trevor R. 2008. “Technical Notes on the AICPA Audit Guide Audit Sampling.” http://cte.univ-setif.dz/coursenligne/cheriguiomar/Ressources/SamplingGuideTechnicalNotes.pdf. “Tribal Knowledge.” 2020. Wikipedia. Wikimedia Foundation. https://en.wikipedia.org/wiki/Tribal_knowledge. Vitalie Spinu, Hadley Wickham, Garrett Grolemund. 2016. Dates and Times Made Easy with Lubridate. RStudio. https://lubridate.tidyverse.org/reference/lubridate-package.html. “Wikipedia, Name of Quebec City.” 2020. Wikipedia. Wikimedia Foundation. https://en.wikipedia.org/wiki/Name_of_Quebec_City. "]
]
